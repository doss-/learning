==cloud concepts
==VPC
==aws
==ecs-cli


  Main simplified concepts [ELI5]:
   https://www.youtube.com/watch?v=_V3dqC80FHU&list=PLv2a_5pNAko2Jl4Ks7V428ttvy-Fj4NKU&index=3
  
  AWS Essentials:
  https://www.youtube.com/watch?v=eiTYqcsU6VI&index=3&list=PLv2a_5pNAko0Mijc6mnv04xeOut443Wnk

  Scaling Docker in AWS:
  https://www.udemy.com/scaling-docker-on-aws/learn/v4/t/lecture/4691758?start=0
  https://www.youtube.com/watch?v=7CZFpHUPqXw - Great about scaling docker
  https://medium.com/@peatiscoding/docker-compose-ecs-91b033c8fdb6 - same 
  http://containertutorials.com/docker-compose/flask-simple-app.html - Dockerize
	  Flask


==cloud concepts
  
  Main benefits\terms:
 High Availability - once in the cloud the data is available everywhere where
  the internet connection is around, and all the time in whole
 Fault tolerance - cloud hard drive wont fail, data could be easely backupped
  and stuff. Service is alaways available and even during a crash comes back
  shortly or w/o downtime at all using mirrors and backups
 Scalability/Elasticity - computing resources could be shrinked or enlarged 
  easely with support of growing coputing needs or vice versa
 Instance - virtual server, instance of the AMI snapshot of the virtual machine
  which is istantiated and acts as Web Hosting server or such


==VPC (Virtual Private Cloud)
  A VPC is an isolated portion of the AWS cloud.
  Private network where stuff like ec2 instances and RDS(database) is located.
  Like a page on a Facebook where each user can do what ever he wants and
  grant or restrict access to it for other users by friending them etc.
  
====Internet Gateway (IGW)
   Analog of modem in local network
  IGW is the thing that allows communication of whole VPS and its resources
  and services to the outside World(Internet). 
  With default VPC created it also has its own default IGW
   Has 2 states 
    attached to VPC
    detached from VPC
  IGW could be attached to a VPC, only 1 at a time
  VPC could have only 1 IGW attached 
  If VPS have services running IGW can not be detached

====Route Tables (RTs)
  See detailed documentation here:
https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html
   Basically is analog of a router in local network, which is connected to
  a modem(IGW), and to which other devices(services) are connected
   Default VPC already has default 'main' route table
  Rout tables contains set of rules called 'routes' that are used to 
  determine where network traffic is directed.
  Route Destination:
   the CIDR block inside VPC where traffic will be routed from the Gateway
  Route Target:
   the Gateway to be used, to route traffic from into the VPC
  Route Status:
   Active - works ok
   Black Hole - routes traffic from VPC to nowhere
    This could be when IGW is detached from VPC but IGW still routes
   traffic to it
  There could be Main Routing Table.
   Every subnet not explicitly associated with a Routing Table will be
  associsated with Main Routing Table
   Routing Table that has Subnets associated cat not be deleted
   Main Routing Table can not be deleted, but could be replaced with new
  To use IPv4 and IPv6 each of those need to have its own CIDRs like:
   0.0.0.0/0 for IPv4
   ::/0 for IPv6
  Each Route Table has 'local' route for communication within the VPC.
  Seems like this needed to when  even if IGW will go out to Black Hole,
  the 'local' routes keep VPC work in place even offline, as per all the
  services will communicate to each other directly w/o using IGW
   'local' communication will be created for each CIDR in the VPC.
     So if VPC have 2 IPv4 CIDRs and 2 IPv6 CIDRs then each Route Table 
     will also have 'local' routes for those CIDRs.
   'local' routes can not be deleted or modified
    Example:
   VPC has IPv4 CIDR block: 172.31.0.0./16
    Each Route Table will have 'local' route like:
    Destination: 172.31.0.0/16
    Target: local

====Network Access Control Lists (NACLs)
  Provides optional security on Subnet level.
   Works like firewall in the Router - in contrast to Security Groups that are
  like firewall on the particular Compoter inside network - SG should be
  attached to particular EC2 Instance
   Whereas NACLs are assigned to Subnets, Or rather Subnets are assigned 
  to NALCs
   NACL consists of Inbound and Oubound Rules
   Rules have priority (from higher priority 0 to lower 100)
   After rule is triggered evaluations stops.
    Which means that if port 80 is allowed and is rule number 81, other 82-100 
   rules wont evaluate for this traffic.
  Also there is undeletable rules that DENY everything for inbound and outbound.
   That rules are the lowest priority so anything could be allowed.
   This rule is called Catch rule, it catched everything that other rules missed
  But anything that is not allowed explicitly will be Denied.
   For NEW rules Everything is DENIED by default
  SUbnet could only have one NACL attached to it

====Subnets
  Sub section of network.
  So VPC has its own network, and inside that network there could be another 
  section (included in the overall VPC's network), and this will be subnet
   Like 172.31.0.0/16 is VPC
   And 172.31.1.0/24 - is Subnet 
  SUbnets could be Private or Public
    Public subnets have Route Table they attached to with row where Destination
   Points to particular subnet IP ranges and Target is pointed to Internet 
   Gateway
    Private subnets are those subnets that have no row poining to Internet
   Gateway, and only has  the defaul undeletable row that points to 'local'
   as Target

  Public\Private general meaning:
   Application is in Public SUbnet, which is Public because is associated to 
   Routing table that has Internet Gateway attached to it.
   Database is in Private Subnet, which is private because is associated to 
   Routing table that has no IGW attached to it.
   Application and Database could communicate to each other because all part of
   VPC Network, and has 'local' communication set in every RT.
   Application is available from outside world
   Database is not available from outside world, only through the Application
   API itself.
   Additionaly if default Routing table has no IGW attached, every newly
   created Subnet will have no internet access by default, thus be secure.



====Availability Zones
  Geographically separate data centers inside the Region.
  Subnets are created inside AZ
  Every Subnet cannot cover more that 1 AZ, means that particular subnet
   can not spanover more than one AZ
  EC2 instances are spawned in AZ, to which instance's subnet belongs to.
   Good practice are to have mirorred infrastructure on another Availability
  Zone, to increase Fault Tolerance and High Availability.

==S3 (Simple Storage Service)
  Storage - like a harddrive, or DropBox(which is actually s3 lol)
   It is split into Buckets, like different repos in gitHub

 Main components are:
  S3 Buckets
     Bucket names have to be totally uniq across all AWS in the world
     Name should be from 3 to 63 chars, [a-z0-9] and - are accepted
     Must not be formatted as an IP address
  S3 Folders
     Any subfolder in a Bucket
  S3 Objects
     File, basically, any type

====Storage class
  Represents 'classification' assigned to each Object in S3

   Storage classes are:
  Standard (default)
   Most expensive
   Maximum durable ('eleven nines' > 99.999999999%)
   Very available - 99.99%
  Reduced Redundancy Storage(RRS)
   Less expensive
   Very durable - 99.99%
   Very available - 99.99%
   Used for non critical objects that are reproducible
  Infrequent Access(S3-IA)
   Even more cheaper
   Maximum durable - ('eleven nines')
   Less available - 99.90%
   Used for objects accessed infrequently, but when do it is immidiately 
   available
  Glacier
   Cheapest
   Maximum durable
   Could takes hours to days to take object back when accessed, request need to 
   be filled for that

  Object Durability:
   11 nines - 99.999999999% means that there is a 0.0000000001% chance that
   file could be lost in a year
    Example:
   for 10k files there is chance to lose 1 in every 10 million years

  Object Availability:
   for 99.99% there is 0.01% chance to lose access to a file once a year
    Example:
   for 10k hours of usage, there is 1 houre of file being unavailable
  
  Each storage class has various attributes, which is represented by:
   Storage Cost $
   Object availability
   Object durability
   Frequency of access to the object

  Each object myst be assigned a storage class (Standard is the default)

  Usually storage class could be changed

====Lifecycle
  Lifecycle of the object - is automated transition of object between different
  Storage classes determined by time period passed, which purpose is to reduce
  cost of using the service
  Lifecycle could be applied to whole Bucket, a Folder or a Object(File) 
   Example:
   File is used every day for first month
    It is in Standard Storage class for first 0-29 days
   After that period passes we expect file to be used once a week
    So lifecycle is configured for the file to be moved into a Infrequent
    Access Storage class after 29th day passes file is moved to S3-IA Storage
    class
   After another 60 days we expect to do not use file at all but want to keep
   it for another 1000 days
    So after 90 days in total passes file's Storage class is changed to
    Glacier class and is deleted after 1090 days in total.

   Thus cost for S3 usage is minimized for that file - because importance of the
   file and frequency of need to access it is taken into account and
   corresponding Storage Classes are used.

====Permissions
  Permissions for a user of S3:
   View contents (List)
   Edit contents (Write) - upload\delete
   View permissions (Read bucket permissions) - view the permissions themselves
   Edit permissions (Write bucket permissions) - change the permissions
 
  Files and Folders have only 3 permission
   Read File\Folder (download too probably)
   Read Permissions of File\Folder
   Change Permissions of File\Folder
  Files also could be made Public Available, need to read more about it
 
  NOTE:
   Even if regular user has IAM permission to access S3, this user will not be
   able to access Buckets created by other users.
   Because during Bucket creation only user-creator have permissions by default
   So additional 'Grantees' need to be added to that BUcket so that users will
   be able to List\Edit\Permissions in that particular Bucket\Folder\File

---Versioning
  Stores all versions of an object

  Controlled at Bucket level
  Could be turned ON or OFF
  Once turned ON could only be Suspended - NOT turned OFF
   Suspend - means stop versioning new versions, but old versions will persist

  New versions COULD NOT INHERIT STORAGE CLASS from their anscestors, which 
  means that Standard storage class will be used (which is costly)

==EC2 Elastic Compute Cloud
  simple virtual machine in the Amazon cloud, like VPS - virtual private server
   
   Purchasing options:
  On-Demand:
   Most expensive
   Most flexible - turn it on or off when i need it
   Pay only when i use it (between on and off)
  Reserved:
   Cheaper than On-Demand (probably much cheaper)
   Reserved for 1 or 3 years, it will work all that time regardeless of whether
    i need it or not
   Whole time of 1 or years must be paid
  Spot:
   Cheapest, regulated by user defined 'bids'
   Provisioned when price decreases to or below the bid, and shutdowned when
   price rises above the bid
    If bid is $0.2 per hour, and price is decreased to that level the instance
   will be provisioned, and user will have access to it until price is equal or
   less than bid ($0.2). As soon as price goes up, instance is shot down.
   Time available is unpredictable

   This is the way for AWS to utilize machine time that is not used at
   the moment of the machines that are used for EC2 stuff
   
====AMI (Amazon Machine Instance)
  Image of an machine including OS, software and settings for the particular
  hardware where AMI is spinned up
  
   Components:
  Root Volume Template:
   Operation system
   Software\applications
  Launch Permissions
  Block Device mapping
   which is mapping to EBS which is basically harddrive

  AMI can be created by user and used fo rfurther EC2 instances setup and
  start
   It works pretty same as docker basically:
  Spin up an EC2 instance from some standard AMI
  Set up all the deeded software packages
  Save this as AMI
  Spin up multiple instances from this new custom AMI

  AMI categories:
   Quick Start - some preconfigured AMI with some software
   My AMI, for custom user created AMI
   Community AMI - free to use, basic AMI w/o software(probably)
   AWS Marketplace AMI - only paid AMI customized for some specific usage

----EBS (Elastic Block Storage)
  Basically hard drive for EC2 instances (volumes)
  Independent of life of the EC2 Instance

  Every EC2 instance MUST have 'root' volume, which could be EBS
  or other type of storage defice
  
  IOPS - Input Output Operations (per second)
   unit of measure
  caped for 256 KiB for SSD volumes
   and 1024 KiB for HDD volumes
  The more IOPS points - the better
  The more EBS size is - the more IPOS volume will have

  Could be created manually from AWS console or during EC2 creation
  By default EBS storage volumes are deleted after EC2 instance shut down
   But could be set to persist

  Another EBS volume could be attached to EC2 instance during or after its
  creation

 Snapshots:
  Snapshot of EBS volumes, could be used as backup
  New EBS volume could be created from Snapshot
  Snapshot could not be attached to an instance
  Cheaper than having an EBS volume 

====Security Groups (SG)
  Same to Network Access Control List(NACL) - firewall for particular EC2
  instances when NACL is a firewall for whole Subnet inside VPC
   It regulates traffic in and out of EC2 instance by
  creating rules based on Protocol\IPAddress\Port
  
  One or more SG could be associated to EC2 instance
  Update of the SG will immidiately take effect for all the EC2 instances it
  is assigned to

  Unlike NACL all the rules from Security Groups merged together and applied
  for the traffic, w/o any priority like in NACL

  Inbound\Outbound:
   By Default 
    all Inbound traffic is DENIED (it has no rules basically)
    all Outbound traffic is ALLOWED
  
====IP Addressing
  COuld be private and public

  by Default each EC2 instance have Private IP address
   Private IP address allow instances to communicate with each other as long
  as they are located in the same VPC

   Public IP depends on settings of VPC\Subnet
  Without Public IP instance wont be able to communicate with Internet

  Default VPC and Subnets configured so that new instances will have Public IP
  addresses

  Whole stuff require for Instance to be able to communicate with Internet:
  - Public IP address
	settings taken from Subnet that allow Instance to have Public IP
  - Security group
	should allow inbound\outbound traffic for particular address\port
  - Network Access Control List
	should allow inbound\outbound traffic for particular address\port
  - Rout Table
	should be pointed to Internet Gateway
  - Internet Gateway
	should be attached to VPC

==RDS (Relational Database Service)
  SQL database service provided by Amazon
   Amazon Aurora
   MySQL
   MariaDB
   PostgreSQL
   Oracle
   MSSQL
  
  Database also is instantiated like EC2 instance, having CPU\RAM and other
  hardware characteristics that could vary and have different cost
 
  Could be purchased on different terms, also like EC2 
   On Demand
   Reserved

  Create Instace:
   https://www.youtube.com/watch?v=OE25Sni15vo
  Subnets could be grouped, in RDS Dashboard.
  For this subnets could be even in different availability zones but should
  be in one VPC

  Pick database Engine(different sql db above, like MySql)

  Make sure 'free tier' checkbox is selected

  Pick instance type (db.t2.micro  i.e.)

  Select schema name, root username and password

  Pick VPC and Subnet Group, Availability Zone will be take from SUbnets by
  default
   In case this are private subnets , select NO in Publiccly Accessible
 
  Add port 3306 to some existing Security Group or create new one , which will
  have it opened by default
  

==DynamoDB 
  NoSQL database service provided by Amazon
   MongoDB
   Cassandra DB
   Oracle NoSQL
  
==SSH Tunneling
   Creation of a SSH tunnel into the machine in private Subnet(subnet with Route
  Table that has no specified Intenet Gate Way) using another machine located
  in the same VPC but in public Subnet(subnet with Rout Table that has IGW 
  specified). Thus machine in public subnet works as pass-through to the another
  one inside the private subnet, which is unavailable from internet directly

==SNS (Simple Notification Service)
  Service to send email or text notification based on the Events happened in
  the AWS account.
   There are two groups of clients
   Publishers(Producers) - communicate with Subscibers async, producing and 
  sending messages to a topic which is a logical access point and communication
  channge.
   Subscribers(Consumers) - could be Web Servers, Email Addresses, Amazon SQS
  queries, AWS Lambda functions, consume or receive the message or notification
  over one of the supported protocols(Amazon SQS, HTTP/S, email, SMS, Lambda)
  when they are subscribed to the Topic.
   Subscribers are groupped into a Topic

  Topics
   Grouped Endpoints(Subscribers) which will receive the messages sent by SNS 
  after some kind of a trigger to SNS

  How It Works:
    CloudWatch (another AWS service, all of a sudden) monitors , i.e. ec2 
   instances, if any crashes it triggers a C.W. Alarm
    CW Alarm or CloudWatch alarm, is set to trigger a message sending in
   the SNS
    SNS which in turn triggered by CW Alarm, then sends the message into
   the topic to which particular users\services are subscribed
  

==Aws general Architecture
  Regions include 1+ Availability Zones which are separate datacenters and 
  located some kilometers/tens of away from each other. All the Amazon 
  services work on those servers within the datacenters

==aws
aws:
 Docs:
 https://docs.aws.amazon.com/cli/latest/reference/index.html#cli-aws

 install aws command line interface:
 easiest way is to use pip, which is python package manager:
 
 pip install awscli --upgrade --user
 --upgrade - will upgrade all packages required for awscli
 --user - will install it in user directory so it wont interfee with system libs
 Read more:
 https://docs.aws.amazon.com/cli/latest/userguide/installing.html

aws iam list-users:
  list all configured IAM users for aws command line in configured format
  JSON by default

aws ec2:
  Docs:
  https://docs.aws.amazon.com/cli/latest/reference/ec2/index.html#cli-aws-ec2

aws ec2 create-key-pair:
  creates pem key pair. BUT only private key is given, public is silenlty 
   uploaded to AWS
  So probably it is better to use import-key-pair on another key-pair
    generated beforehand
  Params:
   --output:
    KeyFingerprint - SHA-1 digest of the DER encoded private key
    KeyMaterial - unencrypted PEM encoded RSA private key
    KeyName - name of the key pair
   Docs:
  https://docs.aws.amazon.com/cli/latest/reference/ec2/create-key-pair.html
   Example:
  aws ec2 create-key-pair --key-name aws-me --query 'KeyMaterial' --output text > ~/.ssh/aws-me.pem

aws ec2 describe-key-pairs:
  display info about key pairs
  Params:
   --key-name - name of keys to be displayed
  Example:
   --key-name aws-me - will display KeyName and KeyFingerprint of 'aws-me' key

aws ec2 delete-key-pair:
  delete pair
   --key-name - name of keys to be deleted

aws ec2 create-security-group:
  create security group
   Params:
  --group-name <name> - name of the group
  --description "<descr>" - description of the group in ""

aws ec2 describe-security-groups:
  describe security group, displays more details
   Params:
  --group-id <id> - id of the group to be displayed

aws ec2 authorize-security-group-ingress:
  adds ports for security group to be opened for Inbound (?)
   Params:
  --group-id <id>  - id of the group to apply opened address to
  --protocol - protocol used for the adresses and port
  --port - port to be opened
  --cidr - ip ranges (Classless Inter-Domain Routing)
  --source-group - security group id from which traffic will come
	could be the same group, so seems that will allow only traffic from
	inside the group(ec2 instances or something)

aws ec2 delete-security-groups
  deletes security group
   Params:
  --group-id <id>  - id of the group to be deleted

aws ec2 run-instances
  creates an instance with given params
   Params:
  --image-id <ami-2b3b6041>  - ID of the AMI image from which ec2 instance will
	be created from 
  --count <1> -  q-ty of instances to create
  --instance-type <t2.micro> - type of the instance(cpu\ram etc.)
  --iam-instance-profile <Name=ecsInstanceRole_and_s3>  -  IAM role name to be
	used by this instance (permissions that instance will have to interact 
	with outer world of AWS, in other words where credentials will be 
	supplied)
  --key-name <aws-me>  - name of ssh key pair
  --security-group-ids <sg-06bdb4b77c473c2f6>  - ID of security groups which
	will be applied for the instance
  --user-data <file://copy-ecs-config-to-s3>  - custom config for ec2 instance
	initialization or working process. For instance config for Container
	Agent
   --user-data file example:
  #!/bin/bash
  #this is regular bash script so provide bash interpreter

  #soft could be installed too
  #yum install -y aws-cli
  #copy ecs.config for Container Agent before start the agent
  aws s3 cp s3://dos-deepdive/ecs.config /etc/ecs/ecs.config
  #install Container Agent and its dependencies
  yum install -y ecs-init
  #start Docker Daemon which Container Agent depends on 
  #(sysinit in Amazon linux)
  service docker start
  #start Container agent itslef, 'start' is part of sysinit tools
  start ecs
   run-instances Example:
   Example:
  aws ec2 run-instances --image-id ami-0ff8a91507f77f867 --count 1 --instance-type t2.micro --iam-instance-profile Name=ecsInstanceRole_and_s3 --key-name aws-me --security-group-ids sg-06bdb4b77c473c2f6 --user-data file://copy-ecs-config-to-s3
  !! NOTE: !!
  !! AMI need to have Container agent pre installed in order to use config file
  !! in case AMI is not have Container Agent installed it need to be installed
  !! and started manually in script passed to '--user-data' param

aws ec2 describe-instances
  list all instances and its info, like InstanceID

aws ec2 describe-instance-status
  displays instance status of given instance
   Params:
  --instance-id <id>  - ID of the instance to be checked
   Example:
  aws ec2 describe-instance-status --instance-id i-03ef567752be1ccf2

aws ec2 terminate-instances 
  terminate running instane
   Params:
  --instance-ids <instance_id> - id of the instance to terminate
   instance id example: i-03ef567752be1ccf2

aws ecs:

aws ecs create-cluster
  creates a cluster - returns output with info about cluster and its status
  should be 'ACTIVE'
   Params:
  --cluster-name <name> - name for cluster to be created
  
aws ecs list-clusters
  lists all clusters , probably visible for current IAM user configured see
  ~/.aws/credentials for config details

aws ecs describe-clusters
  gives more detailed(than list-clusters command) info about clusters.
  similar output to when cluster is created
   Params:
  --clusters <cluster_name> - returns description of a given cluster

aws ecs delete-cluster
  delete cluster - returns same output as creation, but with updated status to
  'INACTIVE'
   Params:
  --cluster <name>  - delete given cluster

aws ecs list-container-instances
  lists the container instances(ec2 instances) in the cluster
   Will return error if no ec2 instances available and no Cluster is specified
   Will return emtpy list if ec2 instances are available but no Cluster
	is specified
   Will return list of ec2 instance arns with cluster specified
    Arn is some kind of uniq identifier used by Amazon:
    arn:aws:ecs:us-east-1:474383222596:container-instance/006724f3-941e-4544-9f8e-6d768e706d2a
   Params:
  --cluster <cluster_name> - specify cluster to be listed with ec2 instances
   Example:
  aws ecs list-container-instances --cluster deepdive  - list ec2 instances
	wokring in the cluster 'deepdive'

aws ecs describe-container-instances
  describes(means - in detail) ec2 instances for the given cluster
   Params:
  --cluster <cluster_name> - specify cluster whose instances will be described
  --container-instances <arn>  - arn of particular container instance

aws ecs register-task-definition
  creates new task definition.
  Task definition has a Revision, it is incremented each time new task
  definition is created, later on Family+Revision is the name to use in
  'describe-task-definition'  command
   Params:
  --cli-input-json <file://file_name.json>  - accepts json formatted file
	with configuration info about particular container instance, similar
	to docker-compose.yml
   File Example:
     {
       "containerDefinitions": [
         {
	#Name of the container
           "name": "nginx",
	#Image in DockerHub to be used
           "image": "nginx",
	#Ports to be exposed(ports also need to be opened in SecurityGroup )
           "portMappings": [
             {
               "containerPort": 80,
               "hostPort": 80
             }
           ],
	#Maximum memory for the container, if exceeded container is killed
	# thus preventing memory leaks
	#In case task definition is a service - container will be restarted
           "memory": 50,
	#maximum CPU usage out out 1024, so 102 is 10% of CPU
           "cpu": 102
         }
       ],
	#Family name of task definitions, like a custom group..
       "family": "web"
     }

  !!NOTE: aws provides empty skeleton for json file used to register task
	definitions, use following parameter:
    --generate-cli-skeleton
     Example:
    aws ecs register-task-definition --generate-cli-skeleton  - will output to
	STDOUT empty skeleton of JSON file used during regular registartion
	from cli (using --cli-input-json parameter)

aws ecs list-task-definition-families
  List all task definition group(families) names available

aws ecs list-task-definitions
  lists all task definitions Arns (and Family:Revision)
   (output similar to list-container-instances for the given cluster)
 
aws ecs describe-task-definition
  list full details of the Task Definition
   Params:
  --task-definition <Family:Revision>  - describes full details about given 
	family:revision. Revision could be omitted to use latest one
   Example:
  aws ecs describe-task-definition web:1 - will return detailed output about
	family called 'web' of the revision '1'
  ..... -defition web - will return detailed output about latest revision

aws ecs deregister-task-definition
  delete task definition or its Revision
   Params:
  --task-definition <family:revision> -- deletes given task definition 
   Example:
  aws ecs deregister-task-definition --task-definition web:2 - will delete
	second revision of 'web' task definition

 HELP help Help man MAN manual
   add help in the end of the command to see regular 'man' article
   about given command:
  aws ecs register-task-definition help
  aws ecs help
  aws ec2 help
  aws s3 help
  aws help
 

aws s3:
  command line to directly interact with s3 storage (see s3api for managing s3
   below)
  s3 is region free, and same for all the regions of AWS

aws s3 cp
  copy file from local machine into s3 bucket storage 
  using s3 [transfer] protocol
   Example:
  aws s3 cp ecs.config s3://dos-deepdive/ecs.config - will upload local file
	into the bucket under given path, and output back upload info

aws s3 ls
  list files in the given s3 bucket
   Example:
  aws s3 ls s3://dos-deepdive  - will list last modified, size(Bytes), name
	of files in the bucket


aws s3api:
  command line api for s3(storage space for files)

aws s3api create-bucket
  bucket is something like disk\repo\namespace for files to be stored
   Params:
  --bucket <name>  - name of the bucket to be created(underscores _ not allowed)

==ECS:
 Amazon Elastic Container Service
  basically container management system, which manages its on its own

  Main aspects:
=   Cluster - group of Container instances which act as a single computing 
 	resource
=   Container instance - _ec2 instance_ which was registered to be a part of a 
	specific Cluster. Connects to a cluster using Container agent
	Life Cycle:
	'ACTIVE' and 'connected' - when container agent is connected to cluster
	'ACTIVE' and 'disconnected' - when connection status is 'false'
		when Container Instance is stopped.
		moves back to 'connected' when container agent is able to
		connect to Cluster and able to run the tasks again
	'INACTIVE' - when Container Instance is Deactivated or Terminated
		Such instance is not seen as part of the cluster
=   Container agent - open source application on ec2 instance which ensures that
	ec2 instance can register into Cluster, and runs tasks on the instance
	which is given from a ECS(or cluster..)
	It is included into AMI(amazon machine image from wchi ec2 instance is
	generated), but also could be taken from official amazon git hub repo
	or pulled from docker hub:
	https://github.com/aws/amazon­ecs­agent 
	https://hub.docker.com/r/amazon/amazon­ecs­agent/ 
         How To Install:
	https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-install.html
	 Bootstrap container agent:
	https://docs.aws.amazon.com/AmazonECS/latest/developerguide/bootstrap_container_instance.html#bootstrap_container_agent
	Could be configured through Env vars and Config files, Amazon has
	documentation for all the switches and stuff:
	http://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs­agent­config.html 
	Custom config could be created to be used when new container instances 
	are started
=   Task definition - JSON file that describes how docker images should be run
	Basically ;docker compose file on steroids;
	
	Task Definition documentation:
     https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html
	
=   Scheduler - decides which instance in a cluster is best to run a task or a
	service
=   Service - long running task such as a web application. Based on a Task 
	definition.
	I can specify how many instances of a Service will run, and ECS will
	ensure that it is running
=   Task - end result of running a tak definition, similar to Service but is not
	running continuously, it is one time job
=   Amazon ECR - private registry (Amazon's docker hub)
=   Amaxon ECS CLI - open source tool for managing ECS through console (like aws)

====IAM
====== User
 User that is not Root(which is the billing holder and stuff). 
  Different users can have different priveleges and acess to different modules.
 Access to resources is given to particular users through Policies
  Like change password is only available if 'IAMUserChangePassword' policy is 
  attached to the user.
 Another way is to add user to a Group which will have its own policies.
  Policy and Group could be created manually in addition to already 
  existing ones
====== Policy
 Permission to do something basically.
  Like change password, or see billing information, or use particular service
  for instance s3
====== Group
 Policies could be groupped into a Group for easier management and consistency
 Also users is attached to the group to get those access given by bunch of
 policies
====== Role
 Roles are given for services, to act on behalf of a user
 Roles have Policies attached which grants some access
  Example:
 Role for EC2 could be created, this role will have policies to use S3
  If EC2 Instance would be given that Role - software from that Instance will
 be able to access S3.
  So Roles is similar to Groups which are given Policies and then is given to
 a User or Users, but Roles are given to another services and stuff, not users
  
==IoT iot IOT Internet of Things
representation of device in the cloud, its shadow is always there and keeps
track of state, when real one comes online shadow delivers state
also has rules , cerificates(uploaded on device) policies and stuff

Guide link:
https://docs.aws.amazon.com/iot/latest/developerguide/iot-gs.html

===Device
device need to be registered in the registry
Device is key-value paired list of device properties
it could have Type
With type device could have up to 50 pairs

===Certificate
Certificate X.509 is used for secure connection
it need to be first created then uploaded on the real device
also together with certificate priv/pub key pair is created for same secure
reason
Certificates need to be Activated.

Certificate with Attached Policy then
 need to be attached to a Thing

===AWS IoT Policies
similar to IAM Policies - those grants unhuman things privileges to do
something for user , on user behalf
 Example:
IoT Policy could grant subscribing or publishing to MQTT topics

Certificate is used for authentification whereas Policy grants device the
privileges to do something after it is authentificated

Policy has Statements that consist of
  Action which is granted by it, like Connecto to IoT
  Resource ARN - which device to apply the policy to
   * - will apply to everyone
    ARN structure:
   arn:aws:iot:your-region:your-aws-account:client/<my-client-id>
    your-region - AWS region, like us-east-1
    your-aws-account - account id like 47438111111...
    my-client-id - id of the IoT device requesting policy
     Example:
    arn:aws:iot:us-east-1:474383222596:client/replaceWithAClientId
  Effect - Allow\Deny - this action for given ARN

  For different Actions ARNs could be different - for Connect it is 'clients'
  for Publish this is topics

Policy could have several Statements

!Policies are attached >>> to Certificates!
  Attaching IoT policy to a certificate gives the device the permissions 
  specified in the policy
 

==ecs-cli
ecs-cli:

 install:
  download binary:
  sudo curl -o /usr/local/bin/ecs-cli https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest 
  check md5 (see linux_cheatsheet in curl: for explanations):
  echo "$(curl -s https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest.md5) /usr/local/bin/ecs-cli" | md5sum -c -
  grant exec permissions:
  sudo chmod +x /usr/local/bin/ecs-cli

docker-compose + aws
https://medium.com/@peatiscoding/docker-compose-ecs-91b033c8fdb6

login to aws guide
https://blog.gruntwork.io/a-comprehensive-guide-to-authenticating-to-aws-on-the-command-line-63656a686799

AWS Regions:
https://docs.aws.amazon.com/general/latest/gr/rande.html

ecs-cli:
 Command line interface to work with Elastic Container Storage(or something), install tool manual:
 https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_installation.html

 Configure the tool:
 https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_CLI_Configuration.html
  Note:
   During profile creation use 'Account Id:' from account settings from web gui as --access-key
 Also use following command:
  configure:
   configure ecs cli, cluster, region, launch type
  https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cmd-ecs-cli-configure.html

  
