
systemd works weird in containers, see here:
https://forums.docker.com/t/any-simple-and-safe-way-to-start-services-on-centos7-systemd/5695/9

===General
Docker is Client-Server app, both could be run on the same system or client 
could be connected to a remove Docker Daemon

Docker Clients and Servers communicate through sockets or RESTful API
(Representational State Transfer - stateless transfer over HTTP of a web page 
containing XML file that describes and includes the desired content)

Docker main components are:
-Daemon
-Clien
-Docker.io Registry

=====Virtualization General
Typically Regular Virtual machine needs Hypervisor - the software that will 
allocate and communicate hardware resources of the server to the Guest machines
Two tipes of Hypervisors:
 Type1 - where there is no OS on the server and its functions takes Hypervisor,
which manages Guests
 Type2 - server has an OS and OS has a Hypervisor and Hypervisor manages 
resources for Guests

Docker shares Host operating system, and ask for resources through it
Also because of sharing it does not need whole Guest OS, only libs that is
required for particular application, or to emulate different OS used in the
Container.
Additionally because it does not need to boot by itself, Container may lack
lots of unused binaries so it is lightweight and fast. 
It could use Linux OS which is prepared for Flash\CD boot and it will be even
more lightwieght and fast


===Installation
install

Centos7
  add docker repo 
   into /etc/yum.repos.d (see linux_cheatsheet for details)
    file named <anything>.repo, containing following:
	[dockerrepo]
	name=Docker Repository
	baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
	enabled=1
	gpgcheck=1
	gpgkey=https://yum.dockerproject.org/gpg
  update yum
   yum update (see linux_cheatsheet for details about update)
  install docker-engine package
   yum install docker-engine
  start service 'docker'
   systemctl enable docker
   systemctl start docker

  make sure it works:
   systemctl status docker
   docker --version
   sudo docker images - docker client installed together with docker server
	is able to connect to the server and fetch list of images

  Regular user:
   Docker uses docker.sock socket to connect to server, this socket is 
   represented as a file(as everything in linux) and is located in:
    /ver/run/docker.sock
   this file is owned by user 'root' and group 'docker', both user and group
   have same permissions 'rw-'.
    So to be able to run Docker commands which will require Server connection
   as a regular user, this user need to be added to 'docker' group

===General
Base image:
  pull from Docker Hub or another private repos not in Docker Hub.
  Base image is the image we  pulled from somewhere and could build upon it our
  own image.

Container Lifecycle:
  Pull - Image is built or pulled from repo
  Run - image is run and container is instantiated
    Network interface for docker server is used to connect to particular 
    container which is given IP by Dockers DHCP

/var/lib/docker - man working dir of Docker
  .../container - has containers. each directory name is the container name
	returned by 'docker ps -a' command
 /var/lib/docker/image/overlay/imagedb/content/sha256 - where 'overlay' is a
	storage driver name. This will contain all the Images, names will be
	match with Image ID returned by 'docker images' command

===IP address
During install docker daemon creates its own Network Interfacte, with ip address
attached, like 172.17.0.1. 
New containers will be assigned addresses from this pull, from ..0.2 to 0.254
=====Change IP address for new containers
 docker -d -b <network_interface> - could change default IP addresses range for 
new containers
 This will start docker daemon with another than default network interface 
attached


man:
  regular 'man' command works with Docker in the following way:
   man docker - will cal manual for Docker
   man docker-run - will cal manual for 'docker run' command
	So to call man need to substitute spaces with dashes, it should work for most of the commands

version:
  command that displays pretty detailed version info about all the docker stuff
  Client, Server, their Go version, API, built date for both etc.

--version
  parameter for 'docker' itself. 
  Displays version of the docker and its build hash

info:
  command that returns lots of information about Docker engine

images:
  displays list of locally available images
  format
   REPOSITORY - name of repo where downloaded, seems like name of image too
   TAG - tag, 'latest' or other, like version number
   IMAGE ID - id of the image
   CREATED - when it was created in the repo(not pulled locally)
   SIZE - size of the image

pull:
  pulls image from a repo
   docker pull [repo/]<image>
  default - latest version
  version could be specified by tag:
   docker pull centos:centos6 - will pull version 6, when 7 is latest one
	'centos6' is a tag, which specifies a version number
   docker pull centos - will pull using default tag 'latest'

search:
  searches for an image in a repo(somewhere configured)
  Search results has Name , Description, also as
   Reputation - STARS, which is some kind of carma?
   OFFICIAL - official repo marked [OK]
   AUTOMATED - dunno, but marked [OK] if yes

  docker search centos - will search for image containing word 'centos' in its
	name(and probably description)

network:
   network drivers are configured via JSON config files, usually there are:
	bridge - main network, all other networks goes through here, DHCP is configured according to it, and range of IP addresses also
		configured here
	host - TODO
	none - TODO
  ls:
   lists all the network interfaces\drivers available for docker daemon

  inspect: 
   inspect <networkID\Name> - lists all the settings of given network, like 
   inspect <container>, including IP ranges(Subnet), Gateway etc.
	i.e. com.docker.network.bridge.name, looks like is what is displayed in
	'ifconfig' for its network interface

  create:
   creates a network, that could be listed by 'network ls' command
	Example:
   docker network create --subnet 10.1.0.0./24 --gateway 10.1.0.1 mybridge01 - 
	will create new network of type 'bridge', probably default one
	all other stuff will be default(as well as driver used). this network 
	will be available for view by 'docker network ls'
	Also it will be seen in 'ifconfig' as a regular network interface, 
	with wierd name(beginning of network ID actuallybeginning of 
	network ID actually)
  --subnet - subnet of a network first available IP will be used for container
  --gateway - gateway address, could be checked from container by 'netstat -rn'
	which is part of 'net-tools' package
  --ip-range - IP for container will be assigned only from this range, should
	be inside 'subnet'
  --driver - driver to use, default is Bridge, could be Overlay
  --label - ...
	Example:
   docker network create --subnet 10.1.0.0/16 --gateway 10.1.0.1 --ip-range=10.1.4.0/24--driver=bridge --label=host4network bridge04
	Creates subnet of addresses of 10.1.0.0 to 10.1.254.254, Class B netwrk
	Gateway is 10.1.0.1 - this is the address used to communicate with host
	Range of IP available from the subnet hovewer is only 254 addresses
	of last octet, and it is Class C network, 10.1.0.0 to 10.1.0.254
	Network Driver to use is 'bridge'
	Label to refer to it is 'host4network', seems that label will mark
	usage of ip-range?
	Network is 'bridge04'
   docker run -it --name nettest1 --net bridge04 centos:latest /bin/bash
	Runs a container using newly created network from above, its IP will be
	assigned from IP-Range, and will be 10.1.4.0, which is first available
	address
    =================STATIC IP ASSIGNATION======================
	Static IP could only be assigned for Custom networks and CAN NOT be
	assigned for default networks(bridge, host, none)
	--ip for docker run command will do the magic

  rm:
   remove network. by ID or Name

	!!!! DO NOT REMOVE DEFAULT NETWORKS !!!!!
    !!!! it will probably lead to whole Docker reinstallation. !!!!

    Example:
	docker network rm mybridge01 - will return network name and delete it

create:
  create a container but not run it immediately
   seeps its syntax is similar to 'docker run', see below

run:
  runs a image thus creating a container
  docker run [params] <image> [command to start with] [params to the command]
   can run by Image name - 'REPOSITORY' field in 'images', by ID 'IMAGE ID' 
   field or by container name. 
   Container name could be manually or automatically assigned, then it will be 
   bound to the container and this exact container could be restarted.
  -i - interactive run
  -t - connect tty to current terminal
   Example: 
    docker run -it centos:latest /bin/bash
  -d - detached, container will run in background
  --name=<name> - creates container with specified and not random name
    could be Lower case but could have Upper case (unlike Images names which
	have to be Lower case). Spaces are not allowed.
  --net <network name>- use specified network (see docker network)
	--net bridge04 - will use custom network 'bridge04' instead of default
	one called 'bridge'
  --ip - assigns Static IP for a container, will only work with Custom network
	assigned using --net param
  --dns=[] - set custom DNS for a container
  -P - auto publishing. of all exposed Container's ports. Assigning random port
	number (range defined by /proc/sys/net/ipv4/ip_local_port_range) for
	every exposed port
	Forward == Publish
  -p=[] - publish specified port, or range of ports. 
	By Default uses TCP, but UDP or SCTP could be specified:
	-p 127.0.0.1:8080:80/udp - bind port 80 of container to UDP port 8080
	of 127.0.0.1 of the Host Machine
	If range is used amount of ports should match each other:
	 -p 1234-1236:1234-1236/tcp - 3 ports on the left(host), 3 ports on the
	right
	 -p 8080:80 - publishes port 8080 on Host to port 80 on Container
	also could bound IP, like
	0.0.0.0:80:172.17.0.2 - 0.0.0.0 means any IP from Host network interface
	is fine.
	Which means traffic for port 80 coming to any Host ip(127.0.0.1 or any 
	other) will be forwarded into container
	 docker run -d -p 80:80 nginx - will run nginx in Detached mode with
	port 80 of container is published as port 80 of the host
     Ports poblish could be done w/o specification of the forwarded port, so
	-p 80 - will expose container's port 80 to a port from range.
	It works pretty same as -P , but needs a port number specified, when -P
	just grams all the exposed container's ports
	-p 127.0.0.2:8081:80 - also specific IP could be mapped to a container'
	port, so in this case i could reach container's port 80 by using IP+port
	but using another IP will not get me into this port
  Volumes can not be made in Dockerfile because Dockerfile meant to be 
	portable
  -v /host/dir:/container/dir - mounts a Volume from Host machine into Docker
	container. It could overwrite Docker directory, like /etc or default
	nginx dir for stuff to display.
   --volume /mydirectory - will create directory 'mydirectory' as a volume
	inside the container. Thus this will be destination
	Using 'docker inspect' source could and it will be:
	/var/docker/volumes/<ID>/_data
  --mount - is the recommended way to use volumes. see '--mount' here:
	https://docs.docker.com/engine/reference/commandline/service_create/#add-bind-mounts-volumes-or-memory-filesystems
	

  --entrypoint - overides entrypoint of the Base Image

stop:
  stops the container
   docker stop <name>

rename:
  renames container names Stopped or Running ones
  docker rename <container> <new name>
  Container ID also could be changed but in the same way as to deletion of 
  container manually - by stopping Docker daemon, navigating to 
  /etc/var/docker/containers and renaming the folder with a specific container.

ps:
  process list from withing docker daemon
  returns all running containers
  docker ps
  -a - returns All containers, including those who stopped already
  -q - returns only hex IDs of the containers
  -f / --filter "key=value" - filters ps output by key-value paris:
    docker ps -a -f "name=name" - will print out all the containers(stopped and
	running) which field NAME contains string 'name'

inspect:
  gives info about local image or container(running or stopped), output is in 
  JSON format.
   It gives much more information if Running Container is inspected, such as
  IP address etc
 
  docker inspect nginx - will show info about local image of 'nginx' in JSON
	such as info about container config - its hostname, user, STD* 
	processing, tags, env variables set for the container
	env vars will be inherited in case of building on this image

top:
  docker top <container>
  Works pretty same as regular top command, returns list of all running 
  processes in the container, but exits out immediately unlike regular
  'top' which will run continuously.

stats:
  docker stats <container>
  Returns info about RUNNING container Continuously(will refresh it in 
  realtime), unlike 'docker top' which will return only current snap-shot
   Example:
  docker stats `docker ps -q` - will display resources usage for all the
	container that are currently Running
  
events:
  Attaches to Docker daemon and displays all the current docker events like
  stopping or starting container etc.
   docker events <params>
  --since '1h' - will display all the events since given time, e.g. 1 hour 
  --filter 'filter'=value - there are several filters:
	container, event, image, label, type, volume, network, daemon
	filters event output by the event type and value:
	--filter event=attach - will display only attach events, in runtime
    several different filters could be applied 

attach:
  docker attach <container>
  attach to containers process, the main process given to it by Dockerfile.
  Exiting after attach will stop the container, because its main process was
  stopped.
  Useful only if some shell (i.e. bash) is executed there. i.e. if this is nginx
  container no command promt will be given.

  SO in case of Nginx, attaching to container will attach to the Process.
  And exiting by ctrl+c or 'exit' will effectively stop Nginx, and thus a 
  container too.
  

exec:
  execute something on a container that is running some other process and
  'attach' wont help.
  Exiting by 'exit' or ctrl+c will not stop the container as per it is still 
  running its main task given to it in Dockerfile.
  exec <new process> <container>
   Example:
  docker exec -it LifeCycle1 /bin/bash - will execute /bin/bash on container
	LifeCycle1 which is running Nginx in detached(-d) mode and can not be
	attached in useful way
	Notice '-it' params which are the same as for Run command
  -u <UID> - use specific user id during exec execution,
	UID 0 - will be ROOT id (see /etc/passwd for UID)
	docker exec -u 0 -it container_name /bin/bash - will connect to 
	'container name' as Root, regardless which user is specified in USER 
	directive in the Dockerfile
   Example:
  docker exec <container_name> /usr/bin/which ps - will return location of
	'ps' util in the container's file system
  docker exec <container_name> /bin/ps aux - will return all the processes
	running in the container
 
start:
  starts previously stopped container(which could be found by 'ps -a')
  start <container>

restart:
  Shuts down and Starts the container.
   IF the process is restartable
 
stop:
  stop <container>
  gracefully stops the container

save:
  saves image to STDOUT, could be redirected into a file
   docker save <image:name> > file.image.tar - will save image into a file
  -o \ --output - Output, also redirects output to a file:
   docker save -o file.image.tar <image:name>
    later it could be read by tar(no matter it is not a tar at all basically)
     tar tvf file.image.tar - will show contents of a file
     gzip file.image.tar - to get it shrinked and save space

load:
  loads an image from a file using STDIN
   docker load < file.image.tar - will load image from a tar file where image
	has been saved previously
  --input - input file with image contents to load
	docker load --input file.image.tar
    Load could work with compressed files too(compressed by gzip at least)
	docker load --input file.image.tar.gz - will load image back in the
	same way as from a regular file

rmi:
  removes image by its name ID or repo + tag
  docker rmi <repo:tag>
  It will untag and delete image
  Images that has container instatiated from it(even stopped) will not be 
  deleted without Force. 
  -f - force, will remove image if it has container instantiated
  Or containers need to be deleted first (see rm command)
   Example:
  docker rmi centos:centos6 - will remove image from repo 'centos 'tagged as 
	'centos6' which is basically its version

rm:
  removes container by its ID or name
   docker rm <name or ID> [name or ID]
  -v - delete any volumes associated with a container
  -f - force, will delete running container
    Example:
  docker rm name1 name2 - will remove containers named name1 and name2 from 
  the system

  ` - back tick(tilda button w/o shift) allows to execute command and put its 
	results out one by one to parent command.
	Actually it will execute another shell with commant between ``. 
	Same as in For loop like 'for i in `ls -la .`'

  xargs is used for piping:
   docker ps -a -q | xargs docker rm - will display all IDs of all the 
   containers and send it to xargs through pope to rm command.
  Same w/o xargs:
   docker rm `docker ps -a -q`
  Another solution:
   docker rm $(docker ps -a -q)

 == Manually remove container from filesystem w/o involving Docker:
  stop Docker daemon:
   sudo systemctl stop docker
  Containers are stored in /var/lib/docker/containers
   Container ID from 'docker ps'  will match name of directory in that directory
  Restart docker daemon, it will re-read the directory and will find out  that
   container has been removed, it wont show in 'docker ps -a' anymore


port:
  displays all the ports of container and their mapping
    docker port <ContainerName\ID>
  output displays which port under which protocol is exposed on Container
  and the way it is published on the Host(IP of the host and Host's port which
  forwards traffic into COntainer's port)

container port:
  does the same as port, but inherited from 'container' command
    docker container port <Container\ID>

commit:
  commits container into the docker hub repo as an image
  commit is done into local repo, same as git
   docker commit -m "commit" -a "author" <container_name> <user/repo:tag>
  -m "comment" - commit, same as for git
  -a "author" - author name or email, something like this
  
build:
  builds an image from a Dockerfile
   docker build -t="user/imagename:tag" .
  -t - tagged image
  . - use current directory and search for Dockerfile
  or instead of . use:
  < /path/to/Dockerfile
  
logs:
  returns logs of the container. Available only with 'json-file' and 'journald'
  logging drivers.
   docker logs [params] <container>
  --follow - will do pretty similar as 'tail -f'
  --timestamps - will add timestamps
  --details - will add more to the logs, such as env vars etc.
  --since - will show only logs since some date:
	1m30s, 3h - Go duration format
	2006-01-02T15:04:05 - RFC 3339 Nano format
	local time zone is used unless Z or +-00:00 provided

history:
  Returns the history of the image, layers and commands made it
   Also IDs of the layers could show that those are parent ids of other images
   located locally, in other case there will be <missing>
   Latest layer ID it the image ID
  --no-trunc - will show full lines without trancating, usefull to check 
	Dockerfile directives used to create each layer

tag:
  tags an image by ID or name:tag. 
   docker tag <user/image:tag> <user/image:new_tag>
   docker tag <imageID> <user/image:tag>
  New copy with the same ID will be created with new <image/name:tag> value
   docker tag <image> image/name:tag - will create new entry of the image
	(link to it), with tiven <image/name:tag>

===Dockerfile

=====Praser directives
Comes on the top of the Dockerfile
After first empty line or comment line(not existing directives treated as 
comment line too) - Parser no more looking for parser directives
Must start from # , Example:
	# directive = value
Lowercase by convention
Could be any case, could have spaces
Every directive could be used only Once

Anything below will be treated ok
Example:
#directive=value
# directive =value
#	directive= value
# directive = value
#	  dIrEcTiVe=value

escape:
  Default escape character is \ (backslash)
  directive to change escape character, usefull in windows, where \ is dir
   separator

  # escape=`   - changes esc char to ` (backtik) which is also esc char in
	Powershell

=====Environment variables:

Environment variables could be used in Docker file as follows:
$env_variable
${env_variable}
${env_variable:-other_value}	- see bash_cheatsheet
${env_variable:+override_value} - see bash_cheatsheet

Escape env variables usein \ (backslash)
$foo - will refer to foo's value
\$foo - will use it literally as '$foo'
\${foo} - same escape

Example:
FROM busybox
ENV foo /bar
WORKDIR ${foo}   # WORKDIR /bar
ADD . $foo       # ADD . /bar
COPY \$foo /quux # COPY $foo /quux

Following instructions support env variables(variables expansion):
ADD
COPY
ENV
EXPOSE
FROM
LABEL
STOPSIGNAL
USER
VOLUME
WORKDIR
ONBUILD (when combined with one of the supported instructions above)

ENV is not updates variable's values during the instruction execution:
ENV abc=hello
ENV abc=bye def=$abc  	#def is 'hello', bcs of same instruction
ENV ghi=$abc		#ghi is 'bye', bcs is not on same instruction

=====Dockerignore .dockerignore
Filters context sent to docker daemon, before CLI sends context it goes
through .dockerignore, and is being filtered out if matched by path
Need to be located in the root of the context dir
New line separated lit of patterns similar to file globs of Unix shells
Root of the context is considered as Workdir and Root dir, so:
/foo/bar
foo/bar
treated in the same way

Example:
# comment	- ignored
*/temp* 	- any first sub directory\file whose name starts with 'temp'
		/somedir/temporary.txt - ignored
		/somedir/temp - ignored
*/*/temp*	- same but two levels below the Root
temp?		- excludes from Root files\dirs whose names 1 char extention
		of temp: /tempa, /tempb, /tempo etc.

Matching is done using Go's filepath.Match rules
leading and trailing whitespaces are removed during preprocess
. and .. are eliminated during preprocess
After preprocessing Empty lines are ignored
filepath.Clean Go's method is used for preprocess

Special wildcards also supported:
**
which matches any number of directories(0 too).
Example:
**/*.go - will exclude all files that end with .go that are found in all dirs
including the root of the build context

! - adds exceptions for ignore. 
Example:
*.md		- excludes all .md file in context root
!README.md	- except README.md file in context root

Placement of ! influences the behavior
Example:
*.md		- excludes all .md file in context root
!README*.md	- except all files start with README with .md
README-secret.md - this particular file is still excluded

If !README*.md will be 3rd and not 2nd line, current 3rd line will have no 
effect

.dockerignore could even exclude .dockerignore and Dockerfile
Those file will still be sent to the daemon 
BUT WILL NOT BE ADDED IN THE IMAGE using ADD and COPY directives

Good Practice:
Ignore everything using *
Add only what is needed using ! for add exceptions
NOTE: this is the same approach as in 'iptables' when closing everything 
except what is needed

Note: for historical reasons . is ignored

=====Dockerfile directives:


FROM
  FROM <image> [AS <name>]
  FROM <image>[:<tab>] [AS <name>]
  FROM <image>[@<digest>] [AS <name>]
  need to be first command, tells docker daemon from where to build.
  could be:
   scratch - means no image need to be searched or downloaded
	then file need to be added which will contain all the
	needed binaries
   image[:tab] - name of the image optionally with tag, which will be 
	downloaded and used as base image, to which all the other commands
	will be applied
  FROM could appear several times during one Dockerfile.
	  Each FROM unstruction clears any state created by previous one
  AS - used to name a build stage, later on this name could be passed to
	subsequent FROM and COPY --from<name|indes> instructions to refer
	the image built in this stage
  tag or digest - are optional , if omitted builder assumes a 'latest' tag
	by default.
	Error is returned if builder can not find a tag value

ARG
  Declares an argument to be used during build
	ARG <name>[=<default value>]
  Docker file could contain one or More ARG instructions:
	ARG user1	- arg w/o value
	ARG buildno=1	- arg w/ default value
	both could be set from command line, see below
  Argument could be initialized by value from command line like:
	docker build --build-arg <varname>=<valuename>
	Dockerfile could contain such varname w/o or w/ value
	or could not, then Warning will be issued
!!
 ARG could be used before FROM directive:
 But to use it INSIDE a build stage(after FROM directive) ARG need to be
	redeclared w/o default value, see below:
  ARG VERSION=latest 			- set up a default value
  FROM busybox:$VERSION			- use the value
  FROM easybox:$VERSION			- use it again
  ARG VERSION				- set it to be visible inside a build
					  stage
  RUN echo $VERSION > image_version	- save it to a file 'image_version'

!!
 ENV overrides ARG if both with Same name are defined. 
  Same manner as in shell scripts when local scope overrides passed as params
  vars or inherited from environment
  -e - overrides ENV value in docker file:
    docker run -e varInDockerFile="new_value" image:tag
 Example:
  ARG VAR
  ENV VAR ${VAR:-local_value}
  RUN echo $VAR
   docker build --build-arg VAR=outer_value
  with --build art RUN will return 'outer_value', without - 'local_value'
 ENV is persisted in the Image, so w/o --build-arg default 'local_value' will 
 persist in the image.

 NOT ALL DOCKERFILE INSTRUCTIONS SUPPORT VARIABLE EXPANSION($VAR > 'value')
 see '=====Environment variables'

 Predefined ARG values, that could be used w/o using ARG to declare: 
    HTTP_PROXY
    http_proxy
    HTTPS_PROXY
    https_proxy
    FTP_PROXY
    ftp_proxy
    NO_PROXY
    no_proxy
  This could be passed directly from command line, w/o declaration in Dockerfile
   Example:
  --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com 
   Output of those variables is supressed in 'docker history' and build cache
   for security reasons

  Specifying those variables in Dockerfile will override the behavior and
  output will be visible in 'docker history' and saved in build cache

ONBUILD
  sets trigger to fire if current image will be used as base(FROM) image of
  inheriting container. Trigger is another instruction.
   ONBUILD [INSTRUCTION]
  
  Triggers list is saved in Image metadata during build.
  Triggers list could be viewed by 'docker inspect <image>' under OnBuild
  Triggers list checked in inheriting Dockerfile during Build, during FROM
   instruction execution. It checks parent image's metadata and executes
   all instructions in order they were added.
  Triggred instructions are executed immidiately after FROM instruction
  Grand-children do not inherit this Triggers list(Base>child>grand-child)
  
  Useful when before doing something during build e.g inheriting image need some
  prerequisites (like copy source code somewhere before build it).

  Example:
  ONBUILD ADD . /app/src
  ONBUILD RUN /usr/local/bin/python-build --dir /app/src
   This will execute ADD and RUN instructions in inheriting Dockerfile right 
   after its FROM instruction

 NOTE:
  Any instructions allowed except:
   ONBUILD - something strange here it seems
   FROM - may not trigger it
   MAINTAINER - may not trigger it

STOPSIGNAL
  kill command, accepts numbers and names of the signals
   numbers need to match kernel's syscall table
  Command sends signals to container to exit

  Example:
   STOPSIGNAL 9
   STOPSIGNAL SIGKILL

MAINTAINER - deprecated, use LABEL
  name or email or anything relating of who maintans the image
   Example:
  LABEL maintainer="SvenDowideit@home.org.au"

LABEL
  Adds metadata to an image. <key>=<value> pair based.
   use quotes to escape spaces, and backslashes to escape new lines(as usual)
  LABEL <key>=<value> <key>=<value> <key>=<value> ...

  Examples:
   LABEL "com.example.vendor"="ACME Incorporated"
   LABEL com.example.label-with-value="foo"
   LABEL version="1.0"
   LABEL description="This text illustrates \
   that label-values can span multiple lines."
 
  in 'docker inspect' Labels will be available in its separate group
   Example:
    "Labels": {
    "com.example.vendor": "ACME Incorporated"



RUN
	Has two forms, Sell form and Exec form
  RUN <command> - shell form
   RUN useradd -ms /bin/bash testuser
  RUN ["executable", "param1", "param2"] - exec form
	Exec form is parsed as JSON array, thus double quotes " is must, and
	not the single quotes '
   In exec mode env vars expansion is made by the Shell not Docker
   Bachskashes need to be escaped in exec form as to not being valid JSON chars
   Runs in default shell:
	Linux: /bin/sh -c
	Windows: cmd /S /C
    Default shell could be changed using SHELL direcive
   To run different shell use:
	RUN ["/bin/bash", "-c", "echo hello"]
  runs a command after 'RUN ', like regular cli command 'useradd'

  Use \ in the 'shell' mode to create a multi line command:
	RUN /bin/bash -c 'source $HOME/.bashrc; \
	echo $HOME'

  Command will be executed on the current image and saved as a layer
  Can be executed several times during Dockerfile
 
  Layering RUN commands is bad practice and conforms core concepts of Docker
  where commits are cheamp and containers can be created from any point in an
  image's history, like in source control


CMD
  Executes command, but it does not become a layer
  Main purpose of a CMD instruction is to provide defaults for an executiong 
  container
   Has three forms:
    CMD ["executable", "param1", "param2"] - exec form, preferred
    CMD ["param1", "param2"] - default params for ENTRYPOINT
    CMD command param1 param2 - shell form
	Exec form executes command With Out the shell
	variable expansion is not done in exec mode: 
	CMD [ "echo", "$HOME" ] - will return '$HOME'
	if need to be done - call shell directly:
	CMD [ "sh", "-c", "echo $HOME" ] - the shell will expand variables
	
   Default shell is:
	/bin/sh -c
!  Only one CMD could be in Docker file, if several such instructions are given
	only latest one will take effect
  In case executable is omitted (2nd form) ENTRYPOINT must be provided
	If ENTRYPOINT and CMD with params are used, both need to be done in
	JSON format(exec format)
  Can be inherited from parent image in FROM directive
  Can be overridden in child docker file using CMD [] directive
  CMD "echo" "this is a test message" - will execute Echo command with a custom
   message. Also container will immediately stop after doing that, if
   no other directives are given for it to run continuously

  If same executable need to be run everytime on start of the container
   ENTRYPOINT need to be used, together with CMD providing the params

ENTRYPOINT
  Executes a command EVERY time, no matter what command is given during 
  instantiation like 'docker run .... /bin/bash'
    Has two forms:
   ENTRYPOINT ["executable", "param1", "param2"] - exec form, preferred
   ENTRYPOING command param1 param2 - shell form

  Can be inherited from parent image in FROM directive
  Can be overridden in child docker file (probably)
   or in command line using --entrypoint of docker run command

  ETNRYPOINT echo "echoed message" - will be displayed all the time no matter 
   what is applied to run command(such as /bin/bash)
   ENTRYPOINT apachectl "-DFOREGROUND"

  ENTRYPOINT is used to run container as executable
	docker run -i -t --rm -p 80:80 nginx
   command line args to 'docker run <image>'  will be appended after all 
    elements of exec form of ENTRYPOINT. 
    Will override all elements specified in CMD (see 2nd form of CMD command)
   So new arguments could be passed instead of default ones stated in CMD
    Example:
	docker run <image> -d - will path -d argument to the entry point.
   ENTRYPOINT could be overriden by:
	docker run --entrypoing 

   Shell form of ENTRYPOINT prevents any CMD or 'docker run' command line args 
   from being used, but starts not as PID 1, so will not receive Unix signals 
   (SIGTERM) so	docker stop <container> will not work for this container

   Only last ENTRYPOINT will work in Dockerfile.

 Good practice:
  use exec form of Entrypoint to set fairly stable default parameters
  and CMD for those which are more likely to change:
   ENTRYPOINT ["top", "-b"]
   CMD ["-c"]

  If shell form is used, to be able to gracefully stop a container 'exec'
   need to be added to ENTRYPOINT directive:

  ENTRYPOINT exec top -b      - in this case top will be PID1 and docker stop
				will terminate container correctly.
  In other case PID1 will be 'sh', because 'shell' works through '/bin/sh -c'
  and container will be SIGKILL'ed after a timeout when used 'docker stop'
  
=====ENTRYPOINT and CMD iteract
Both CMD and ENTRYPOINT define what command gets executed when running a 
container. There are few rules:
1. Dockerfile should specify at least one of CMD or ENTRYPOINT
2. ENTRYPOINT should be defined when using the container as executable
3. CMD should be used to provide default arguments for ENTRYPOINT
 3.1 or to execute some ad-hoc command in a container.
	Probably the one which could or could not be needed every time
	container is instantiated\started. When there is a high possibility
	of running a container with some Other command
4. CMD will be overriden when running container with alternative arguments
	Example:
	 docker run -itd image:tag --alternative-param - this will override
	params given to CMD with '--alternative-param'

Shell mode:
 ENTRYPOINT exe param
  /bin/sh -c exe param
 CMD exe param
  /bin/sh -c exe param
Exec mode:
 ENTRYPOINT ["exe", "param"]
  exe param
 CMD ["exe", "param"]
  exe param

 ENTRYPOINT/CMD combinations
1. If no ENTRYPOINT or CMD given - Error produced
2. CMD used if No ENTRYPOINT given
3. Shell ENTRYPOINT will ignore all the CMD at all
4. Exec ENTRYPOINT will append to it everything from CMD in the same order
 4.1 If Exec ENTRYPOINT and Shell CMD are combined, ENTRYPOINT will be 
	appended by CMD with '/bin/sh -c' line
	 Example:
	ENTRYPOINT ["exec_entry", "p1_entry"]
	CMD exec_cmd p1_cmd
	 Will produce:
	exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd

VOLUME
  creates volume during container instantiation. several volumes could be 
	created at once:
   VOLUME ["/var/log", "/var/db"]  - JSON array
   VOLUME /var/log /var/db - plain string

  by default volumes created in instantiated container, and on the host in 
  docker directory in 'volumes' subdirectory:
   /var/lib/docker/volumes/a2d05af6cb32bd1f8e827d5a3943500e0ffe51210dbf0e72a15e35b357f99b85/_data - will contain volume's contents

 Notes:
  1. If any build step change the data within the volume after it has been 
	declared, those changes will be discarded
  2. JSON , use double quotes " instead of single quotes '
  3. Declare host directory at container runtime.
	VOLUME instruction does not specify a host-dir. It does not support it
	Host dir mountpoint need to be specified at container:
	- creation
	- restart
    3.1 but i do not know how to do that
    3.2 need to check Volumes usage here:
	https://docs.docker.com/storage/volumes/

USER
  USER <username>[:<group>]
  USER <UID>[:<GID>]
  marks entry point for a container to be under specific user.
  in this case user first need to be created using RUN command
  All the instructions in Dockerfile after this directive will run under 
  the user that was specified!
  All the instructions are:
   CMD
   RUN
   ENTRYPOINT

WORKDIR
   WORKDIR /path/to/workdir
   WORKDIR relative/path/to/previous/WORKDIR
  Sets a working directory for :
  RUN, CMD, ENTRYPOINT, ADD, COPY instructions
   It is created anyway, even if not used(pointing to /)

  Could be absolute or relative, could be used multiple times in a Dockerfile
  If relative - will be added to current WORKDIR (by default /)

  Could use Env variables , but only explicitly set previously in the
  Dockerfile. Env vars from system, even current session, will not apply,
  same as ENV vars set with docker instruction but later on in the same
  Dockerfile

===Environment variables manupulation:
  the simpliest way is to export something into users .bashrc file:
  RUN echo "export JAVA_HOME=/usr/java/jdk1.8.0/jre" >> /home/user/.bashrc - 
  this will add a line into .bashrc file which is read every time the
  shell is executed. Basically is standard way to add ENV variables for a
  user

ENV
  sets system-wide variables, could be used in two ways:
  COuld be overriden by outer parameter for 'docker run' command
   Usage 1:
  ENV <key> <value>
   Example:
  ENV JAVA_BIN /user/java/jdk1.8.0/jre/bin
   Everything after first space will be treated as a 'value' including spaces,
   and even other env var names. Quote chars are ignored if not escaped
   Example:
  ENV myName John Doe
  ENV myDog Rex The Dog
  ENV myCat fluffy

   Usage 2:
  ENV <key>=<value> ...
  use for multiple env vars assignment in one ENV directive
   Example:
   ENV myName="John Doe" myDog=Rex\ The\ Dog \
    myCat=fluffy

  -e - sets Env variable, this will override the ones existing in container
   Example:
  docker run -e LOCAL="outer" image:tag
	this will re set value of LOCAL variable to 'outer'

ADD
  Copies files\dirs from relative <src> of context to <dest> of the image
   Two forms are supported:
    ADD [--chown=<user>:<group>] <src>... <dest>
    ADD [--chown=<user>:<group>] ["<src>",... "<des>"]
	Second form is used when path contains whitespaces
     NOTE: --chown works only in Linux not Windows
  Multiple <src> are supported, all need to be relative to context root
  Wildcards are supported, matching is done using Go's filepat.Match rules
	Example:
   ADD hom* /mydir/        # adds all files starting with "hom" to /midir in 
				the image file system
   ADD hom?.txt /mydir/    # ? is replaced with any single character, 
				e.g., "home.txt"
   <dest> - absolute path in the image file system
	or
	relative path to WORKDIR
	Example:
   ADD test relativeDir/          # adds "test" to `WORKDIR`/relativeDir/
   ADD test /absoluteDir/         # adds "test" to /absoluteDir/

  Special characters like ] or [ need to be escaped
	Example:
   ADD arr[[]0].txt /mydir/ #copies a file "arr[0].txt" to /mydir

  All files and dirs created with UID and GID as 0, unless --chown specifies
	another user/group. Username and Groupname are supported as well as
	UID and GID integers
	If --chown is used Containers /etc/passwd and /etc/group will be used
	to check ID numbers
	Example:
   ADD --chown=55:mygroup files* /somedir/
   ADD --chown=bin files* /somedir/
   ADD --chown=1 files* /somedir/
   ADD --chown=10:11 files* /somedir/
  NOTE: if /etc/passwd or /etc/group files are absent ADD operation will fail 
	during the Build process
!	Using ingeregs for UID and GID is not require checks of those files

!!!!
   If Dockerfile is sent through STDIN:
	docker build - < Dockerfile
    There will be NO context, so Docker file can contain only URL based ADD
	instructions.
    Or pass an archive with Dockerfile and context:
	docker build - < archive.tar.gz
Authentification:
 Note: If your URL files are protected using authentication, you will need to
 use RUN wget, RUN curl or use another tool from within the container as the 
 ADD instruction does not support authentication.

 ADD Rules:
  - <src> must be Inside of the context of the build
	ADD ../something /something - will NOT work
  - if <src> is a URL and <dest> does NOT end with trailing slash, then
	a file is downloaded from URL and copied to <dest>
  - if <src> is a URL and <dest> DOES end with trailing slash, then
	a filename is inferred from URL and file is downloaded to 
	<dest>/<filename>. Example:
	ADD http://example.com/foobar / - will create /foobar file
  - if <src> is a dir, the entire contents of the dir are copied including 
	file system metadata.
	NOTE: directory is not copied, only its contents
  - if <src> is 'local' tar archive , compressed in recognized format
	(identity, gzip, bzip2 or xz) - then it is unpacked as a dir.
  - if multiple <src> are specified <dest> must be a directory and
	must end with trailing slash - /
  - if <dest> does NOT end with a trailing slash it is considered as FILE
	and contents of <src> are written at <dest>
  - if <dest> is missing it is created along with all dirs in its path

COPY
  behaves pretty similar to ADD but is not unpacking archived things
  Preferred to use over ADD until archive need to be unpacked
	Two forms:
    COPY [--chown=<user>:<group>] <src>... <dest>
    COPY [--chown=<user>:<group>] ["<src>",... "<dest>"]
	second form is for path containing whitespaces

  Also accepts flag '--from=<name|index>' of a build stage like
	FROM .. AS <name>
	in this case contents will not be taken from a context but taken
	from a build step specified.
	If no build stage is found with such name, image will be attempted for
	use

EXPOSE
  exposes particular port, or probably list or range of ports
  This port will be always exposed in the container unless unexposed in some
  manual way
   Ports however are still hidden and unavailable, it is just mark made by
   image creator for image user to open this ports as per they are 
   intended to be opened
  To open ports for real use 'docker run -p' command, or '-P'.
   If ports are not EXPOSEd, -P will not work:
   Although '-P' param will not expose anything because it looks for already 
   exposed ports on the containers, and w/o EXPOSE directive there are none
  Examples:
   EXPOSE 80
   EXPOSE 22 80

HEALTHCHECK

SHELL

===Docher Hub
https://hub.docker.com/add/repository/?namespace=doss

create repo:
  go home, and click 'Create Repository +'

login from CLI:
docker login:
  issues login process into the docker hub
  After successful login file in 
	/home/dos/.docker directory will be created called config.json, 
	which will have auth info
  --username=<user> - saves username for further logins

docker logout:
  issues logout process, which will delete auth info from config.json file

docker push:
  Pushes image to authenticated repository
   Note: need to docker login first
   Note: image need to be called properly:
	cant push random named image into my repo, but image named with
	<my_docker_name>/image:tag - goes nice
   Note: creates new repo if no such were existent:
	doss/deepdive:ex1 - will be pushed ok if user 'doss' is the one i've
	logged in with
	If user has no repo called 'deepdive' yet the repo will be created with
	default settings(public repo)
  docker push <user/image:tag>
	Example:
	docker login --username=doss - will ask for password
		then create auth config.json (see above)
	docker tag my_ramdom_image/test:tag1 doss/deepdive:ex1 - rename image
		to user/repo sufficient format, where names match
	docker push doss/deepdive:ex1 - will push the image into Dockerhub for 
		user 'doss' under which i'm logged in and create public 
		repository 'deepdive' if none existent
		Will ask for credentials if repository exists and it is private

docker-compose:
 up:
  Sets everything up(all services) as described in docker-compose.yml
   Uses current terminal for STDOUT\ERR
  Should be executed in directory with docker-compose.yml file
  Restarts everything back in case of host-machine restart

  -d - detached mode, will return control of terminal (same as docker run -d)

 down:
  turns everything down, which was upd by up command
  Deletes all the infrastructure - containers, networks etc (except images)
   --volumes - also removes volumes created during composing stuff

 stop: 
  stops all the composed environment, but keep it

 start:
  starts all the stoped environment

 run:
  run one-off commands on the service
   Example:
  docker-compose run web env - will return all 'env' variables of service
	called 'web'

docker-compose configuration file:

version: 
  version of docker compose parser, pretty old ones are not 3+
   Example:
  version: '3'

services:
  main field, it seems, the head of hierarchy
  all the 'containers' are listed under it
   Example:
  services:
    container_first:
     ...
    container_second:
     ...


build: 
  Instruction to build a container from given Dockerfile(and image if given)
  Could accept only path for directory where Dockerfile is located
   Example:
  build: .
   Or could have list of <property>=<value> points:
  build: 
   context: ./dir 		#(could be .)
   dockerfile: Dockerfile-alternate  
   args:
     buildno: 1

  could also has image provided, with some behavior...
   Compose named build image will have image(?) name and its tag if provided

 context:
  path to a directory or GIT repository url
  which goes right into 'build:' if only this used - path for directory where 
  Dockerfile and context are located.
   if path is relative - location of docker-compose.yml is used as a starting
  point
   path is also a context for build of Dockerfile pointed

 dockerfile:
  alternate file, i dont know what it means..

 args:
  Add build arguments which are env vars accessible only during build process
  If Dockerfile has ARG field specified, it could be initialized with value 
  using this parameter
   Example:
  ARG buildno 		#in Dockerfile
  --
  build:
   args:
    buildno=1		#will apply '1' to Dockerfile's ARG

  Applies YAML's mapping or list:
  args:
   buildno: 1
    =or=
  args:
   - buildno=1

  If no value is provided to args it will be taken from current environment
  where compose is running:
   Example:
  export buildno=1	#shell where env var is exported
  --
  args:
   buildno		#will take value '1' from shell's environment

  Note: YAML boolean values (true, false, yes, no, on, off) must be enclosed 
    in quotes, so that the parser interprets them as strings. 

 cache-from:
  list of images to use cache from
  probably is useful to increase build speed of composed containers

 labels:
  adds metadata to the resulting image using Docker labels.
  could use array or dictionary
   Example
 build:
  labels:
   com.example.description: "Accounting webapp"
   =or=
   - "com.example.description=Accounting webapp"

 NOTE: use reverse-DNS(where .com at beginning) to prevent labels from
  confliction with those used by other software

command:
  overrides CMD, could also be in two forms
  command: bundle exec thin -p 3000
  =or=
  command: ["bundle", "exec", "thin","-p", "3000"]

container_name:
 Specifies a container name. 
  Can not scale service(container(s)) more than 1 in this case - names
  must be uniq
 Container usually named as '<dir_of_docker-compose>_<service_name>'

env_file:


environment:
  
environment expansion:
  1. If shell has value it will be expanded in docker-compose file 
   1.2 It'll passed into container ONLY if mentioned in 'environment:' 
    Example:
   environment:
    - SHELL_VAR
  2. If .env file in root is present it will be expanded in docker-com file
   2.2 it'll be passed into container ONLY if mentioned in 'env_file:'
    Example:
   env_file: .env
  3. If docker-compose has custom .env file (other name or path) it will NOT
   be expanded in docker-compose file
    Example:
   env_file: ./Custom/path/app.env
    3.2 it will be passed into container anyway
  4. If docker-compose has custom environment var defined it will NOT
   be expanded in docker-compose file
    Example:
   environment:
    - CUSTOM_VAR=compose_file_variable
   4.1 it will be passed into container anyway
  5. If Shell has a variable and docker-compose file also has initialized one:
     during expansion SHell value will be used
     but in Container's shell docker-compose value will be passed
      Example:
     export SHARED_VAR=shell
     --
     environment:
      - SHARED_VAR=compose_file
     command "echo shared_var is $SHARED_VAR"
     --
      docker-compose config - will return:
     environment:
      SHARED_VAR=compose_file - this will be in container's shell
      command: 'echo shared_var is shell'

  !!!!!!!!!!!!!!!!!!
  !!!! BE EXPLICIT
  !!!! do not use same names, or recheck twice
  !!!! explicitly use .env file or other variables 
  !!!!!!!!!!!!!!!!!!
 

config:
  shows docker-compose.yml but with expanded variables
  and 'environment:' section has ALL THE ENV VARS THAT WILL BE PASSED TO CONTAINER\SERVICE
