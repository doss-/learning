Contents
1. [General](#General)  
[Env Vars / Environment variables:](#Env-Vars-/-Environment-variables:)
===User\Groups Management
=====User\Groups Management General info
=====Manipulate Users And Groups
=====Super User
===Processes and Services Management
===Package management
=====Debian\Ubuntu
=======Uninstallation
=======Repository setup
=======List all installed packages:
=====RedHat RHEL\Centos
=======Manage repositories
=======Uninstallation
=======List all installed packages:
===File Permissions / Ownership
===IPTables / Linux kernel Firewall(not d)
===Bash
=====Bash command line shortcuts
=====Bash Shell
=====Bash Scripting
===Nginx
===Networking
====Ports checks


## Example

# General

## check linux distro:  
uname -a - returns info about kernel and stuff  
cat /etc/*-release - return info about distro  
lsb_release - command that returns info about distro  
cat /proc/version - file with linux version info  

## facter:
  tool to get info about the system, hardware, network, user, distro..  
  121 property in total  
  Useful for Ansible, to get different facts like os name.  
   Ubuntu: sudo apt-get install facter  
   CentOS: yum install facter # enable EPEL repo  

## directories structure:
 ```/``` - root  
 ```/bin``` - binaries for system boot, goes with distro  
 ```/boot``` - kernel boot files and RAM disc with drv 
  for boot, and boot program itself  
  ```/boot/grub/grub.conf``` - boot config  
  ```/boot/vmlinuz``` - kernel  
 ```/dev``` - all devices are here  
 ```/etc``` - configs all here  
  ```/ect/crontab``` - cron config  
  ```/etc/fstab``` - mounts config  
  ```/etc/passwd``` - users list  
 ```/home``` - contains all users' dirs  
  ```~/bin``` - bins of particular user, add to PATH to use  
 ```/lib``` - shared by various programs libraries(DLLs), probably for  /bin's
 ```/lost+found``` - if HDD feels bad, some stuff could appear here  
 ```/media``` - user-related mount points for flash, hdd, cd etc
   which are mounted manually
 ```/opt``` - optional installed software and other stuff
   mostly used for commercial software
 ```/proc``` - files to peek at kernel things  
 ```/root``` - root's home dir  
 ```/sbin``` - same as ```/bin```  
 ```/tmp``` - temp used by various programs, could be cleaned at boot  
 ```/usr``` - programs and files used by users  
 ```/usr/bin``` - programs from distro, almost everythign already installed goes here  
 ```/usr/lib``` - shared libraries for /usr/bin programs  
 ```/usr/local``` - empty by default, available for all users on system  
  ```/usr/local/bin``` - locally compiled\created bins should go here  
  ```/usr/local/sbin``` - locally created system bins for administration  
 ```/usr/sbin``` - System bins, come with distro, i suppose  
 ```/usr/share``` - shared stuff(configs, files, images) except shared libs  
 ```/usr/share/doc``` - mans for installed soft go here
 ```/var``` - variables, most changed dir, contains DBs, buffers, emails  
 ```/var/log``` - all logs go here
 

## man:
  https://www.howtogeek.com/663440/how-to-use-linuxs-man-command-hidden-secrets-and-basics/
  man [section] <command>
  man sections:
   1. General commands
   2. System calls
   3. C library functions
   4. Special files (usually devices, those found in /dev) and drivers
   5. File formats and conventions
   6. Games and screensavers
   7. Miscellanea
   8. System administration commands and daemons
  Example:
   man 2 stat - will display manual for hashed 'stat'
	man has stat(1) and stat(2), which are different commands..
  -f <command>  - search for all entries of command and return `whatis`
    Example:
   man -f man
    will return man(1) and man(7) and short whatis of both
  -k <text>   - same as apropos - search through man, return line matched
     if . given - it will match EVERYthing
  
  Count all man pages:
    $ man -k . | wc -l
    > 6659 
      number of manpages on the system

## apropos:
  searches through man by keyword and returs matched in format:
   article (secion) - line matched

## echo:
  accepts input from params or STDIN and returns in expanded to STDOUT
  accepts multyline input, and will return it in the same manner:
   echo "this is firstline
	this is second line
	this is third line"
   Params:
  -n - supress new line after output, like '.Write()' vs '.WriteLine();
  -e - enable interpretation of \ escaped stuff

## whatis:
  returns first line from man of given command
   Example:
  dos:~$ whatis ls
  >ls (1)               - list directory contents

## info:
  same as 'man' for GNU software

## pwd:
  print working directory 
  returns string with current path location
   Example:
  pwd  - will return path where you are now
   Example inside linked path:
    https://unix.stackexchange.com/questions/63580/resolving-symbolic-links-pwd
   $ type -a pwd
   pwd is a shell builtin
   pwd is /bin/pwd
   $ mkdir a
   $ ln -s a b
   $ cd b
   $ pwd           <<< this will display path as if there is no link
   /home/michas/b
   $ /bin/pwd
   /home/michas/a  <<< this will display real path including link redirection

## cd:
  change directory
   Params:
  <path/to/folder> - changes dir to the folder
  -/<none>   - changes to current homedir
  ~username - changes to username homedir

## pushd:
## popd:
changes to dir and saves it in stack
changes(backwards) to last dir from stack, removes dir from stack
Example:
```
#silent move w/o displaying full path in STDOUT
pushd some/directory/path > /dev/null
pushd > /dev/null
```

## tree:
  lists directories as trees, very useful
   https://www.ostechnix.com/view-directory-tree-structure-linux/
  sudo apt install tree
   
   Example:  
  ```tree .terraform/```  
   will show everything inside .terraform directory in tree-view  
  ```tree -h --du .terraform/```  
   will show sizes in human readable, and for directories too

## ls:
  list files in given directory or current one
   Params:
  -l - long format (list basically)
    Columns:
    access rights | hard links qty | owner | owner group | 
    | size | data changed | name
  -a - show all files including hidden 
  -d - show only current directory, if given another directory - shows only it
       if given a wildcard that will match directories only - will return only
       the directories, w/o -d it will return every directory and its contents
	Example:
     ls -lda */ - will return list of directories
	Example:
     ls -la */ - will return contents of each directory within a separate list
  -r - reverse sorting order
  -S - sort by size
  -t - sort by date changed
  -F - adds slash to dirs in list, * to g+r files and stuff
  -p - appens slash to dirs
  -h - human readable sizes (mb, gb etc)
  -i - inode number, address where contents is located, same for all hard links
  -I - ignore; dont list entries matching shell pattern


  Example:
  ls -lSrh - sory by size, in reverse order with human readable sizes

## locale:
  displays all Locale related env variables, like
   LANG - set up during installation, picks install language
   LC_MONETARY - currency? 
   etc.

## time:
  seems like calculates time of how long command takes to execute
  > Many shells have built-in `time`; 
  use `/usr/bin/time` to get better time command

   Syntax:
  time <command\script_name>
   Example:
  ```
  time some_script.sh
  > real 0m14.169s	#i spend 14 seconds (script uses 'read')
  > user 0m0.005s	#this two seems to calculate actual time of
  > sys	 0m0.005s	#+ calculations required, except manual intput
  ```
  Example:
  ```
dos:temp$ /usr/bin/time ls
>0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 2528maxresident)k 
0inputs+0outputs (0major+109minor)pagefaults 0swaps
  ```
  __user__  - seconds spent to run binary(or script) owns code. If short could be 0.00  
  __system__ - seconds spent by Kernel to do the process work (i.e. read the filesystem)  
  __elapsed__ - total number of time for process from start to finish including waiting for resources

## timedatectl:
  check \ set time and date  
  Check current time zone:
```
timedatectl
```
  Check available time zones:
``` 
timedatectl list-timezones  | grep Kiev
```
  Set new time zone:
```
sudo timedatectl set-timezone Europe/Kiev
```


## date:
  returns current date - weekday, day, month, year, time, time-zone
   Could be formatted like:
  date +"%x %r %Z"
   where
   x - short date in American format like '20.02.2019' - dot delimited
   r - time hh:mm:ss (dunno whether its 12 hours or 24 hours format)
   Z - short name of time zone like 'EET' for Eastern European Time

## cal:
  calendar, spans for several lines , looks like ok calendar

## file:
 short info about a file, file type etc.
  Linux does not have extensions for files so this is 
 the sure way to acquire such info

## grep:
  Global Regular Expression Print
  filters output using regular expressions
  grep <params> <regex> <filepath>
  -v - inverts results
   ps aux | grep bash |grep -v grep - will return only real bash w/o grep 
	process listed
   '[b]ash' will also do the trick, dunno why
    why it works: 
     https://askubuntu.com/questions/153419/how-does-this-tricky-bracket-expression-in-grep-work
 TL;DR - ps returns process of grep with a parameter '[f]irefox', and
 grep it its turn searches for 'firefox' this pattert will not match string
 from ps output with displayed grep params that includes braces
 So basically string 'grep [f]irefox'(returned by ps aux) will not be 
 matched by regular expression '[f]irefox' (executed by grep)
 Params:
  -i  - case INsensitive
  <filepath> - is not required if used in pipe
  -c - Count matched lines, return number of lines matched
  [] = search for a character
     i.e.: grep [abzfh] file - will search for appearance of any character 
		between square brackets in the file, standard regex
  -f <filename> - pass contents of filename as a pattern
  -l - search inside file contents and returns names of files found
  -L - like -v but for -l - show only not matched files
  -n - line number for matches
  -h - no file names in output 
  -r - recurse search in a directory
      Example:
    grep -lr cron /etc 
       will return all file names which content has
       'cron' pattern in whole /etc directory
  -E - extended regular expressions, same as 'egrep'
  -F - Fixed grep, same as 'fgrep'
   Search in file:
   Example:
  grep pattern filename.txt
   Recurse Example:
  grep -R pattern *

## fgrep:
  interprets regex special symbols as usual ones, and searches text with it
  searches FAST and simple
   fgrep ^hello file.txt - will find for literal '^hello' and not for hello at
			the beginning of the line in the file file.txt

## egrep:
  allows to use Extended regular expressions
   accepts same Parameters as regular 'grep'
  it allows signs as:
   | ? + .* {} 
   {3} - match 3 times exactly
    Example:
  egrep 'AAA|BBB|CCC'   - will match either AAA BBB or CCC
  egrep '^(AAA|BBB|CCC)'   - will match either AAA BBB or CCC on the beginning
	of the line

## stderr stdin stdout

redirection   
stdout:  
```
echo "text to file" > file.txt
```

stderr:
```
find . -size 33c  2>/dev/null
```

stderr & stdout:
```
find . -size 33c  2&>/dev/null
```

stdin:
```
cat < file.txt
```

## stat: 
  returns status info about given file, if it dir, or file, when it was changed,  its size and stuff
   man 2 stat - for details

## find:
  `find <path> <params> <name mask>  `
   __!!!	ORDER IS IMPORTANT__
   Example:
  find / -type f -name "one.tx*" <-ls>

   Operations:
  - -ls - will output results as in 'ls -l'
  - -delete - delete found file. Better check first with -print, then -delete
  - -print - prints full path to stdout, Default operation
  - -print0 - changes delimiter to null character (0 code in ASCII), useful for 
	filenames with spaces, when need to use another delimiter. 

	Together with 'xargs' with --null, will go fine:  
	 `find -name '*.jpg' -print0 | xargs --null ls -l`  
    _Example_:
   ``` 
    Effective:
   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -print0 | xargs -0 rm
    Ineffective:
   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -name '*' -mtime +1 -exec rm {} \; &
     Note: -name "*" is also redundant - default behavior
    xargs will take a number of items up max command length and then run it. 
     So in the previous example rm would run once for each file. With xargs it 
    would run a couple of time with many files on the rm command at onces.
     The find -print0 and xargs -0 option uses the nul character instead of 
    the default newline character to terminate items. This means it will work with file names with spaces or other white space.
  ```
  - -quit - quits after first match
  - -regex - will search with regex, whole path need to be matched in order
	to find it:
	 _Example_:  
   ```
	find . -regex '.*[^-_./0-9a-zA-Z].*'
	 will match whole path(.* at both sides), where files have spaces
	 (spaces are not in [^....] group - files in the group are acceptable
	 in a filename)
   ```
  - -ok - ask before execution, same as exec, see below
  - -exec command '{}' \; - executes a command for _EVERY_ match, so 100 matches
	is 100 times executed commands, substitutes found value in {}
	with ; as a command end delimiter.  
	`{}` and `;` are special for Bash, so need to be escaped by `''` or `\` 
	 Use `+` instead of `;` in the end to concatenate results and execute 
	command just once  
	 Also instead of `+` `'xargs'` command could be used  
 	 This could be helpful as command line has limit on arguments (xargs
	restarts when arguments limit is reached until everything done)  
    __Example__:  
   `find . -name "test*" -exec stat '{}' \;`  
    will find files and folders and display results of hashed 'stat' for 
    every matched item  
    __Example__:  
   `find . -name "test*" -exec stat '{}' +`  
    will do the same as above, but 'stat' will be executed only once with list
    of found files - stat testmatch1 testmatch2 testmatch3  
     __Example__ mass rename:  
    `find . -depth -name "*" -exec sh -c 'f="{}"; mv -- "$f" "${f%}.txt"' \;`  
     rename files w/o extention into files with `.txt` extention  
     __Example__:  
   `find . -name "test*" | xargs stat`  
    __Example__:  
  
  ```
    Effective remove:
    instead of this:
   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -name '*' -mtime +1 -exec rm {} \;
    use this:
   find /u01/app/oracle/product/db11g/rdbms/audit/ -type f -print0 | xargs -0 rm
     Details:
    xargs will take a number of items up max command length and then run it. 
     So in the previous example rm would run once for each file. With xargs it 
    would run a couple of time with many files on the rm command at onces.
     The find -print0 and xargs -0 option uses the nul character instead of 
    the default newline character to terminate items. This means it will work 
    with file names with spaces or other white space.
  ```
    

   Checks:
  - -type f:
	  - d - dir
	  - f - file
	  - l - sym link
	  - b - block device (like /dev/sda)
	  - c - char device (also in /dev , dunno what it is tho)
  - -name "name*" - name or wildcard in "" - to prevent bash expansion
  - -iname "Name*" - case Insensitive
  - -size nM:
	  - b - block, default,            512 bytes
	  - c - char(byte)			 1 byte
	  - w - words (two bytes)		 2 bytes
	  - k - kilobytes, block         1 024 bytes
	  - M - megagytes, block     1 048 576 bytes
	  - G - gigabytes, block 1 073 741 824 bytes  
    __Example:__
    ```    
      find . -size +10M  - will find with size more than 10 megabytes
      find . -size 10M - will find exactly of 10 megabytes
      find . -size -10M - will find with size less than 10 megabytes
      find . -size +1G -exec mv {} delme/. \; - move everything more than 1Gb into another dir 'delme' 
    ```
  - -cmin n - files\dirs with attrs\contents changed exactly n minutes ago
	supports +\ \- like -size
  - -mmin n - files\dirs whos contents changed n minutes ago
  - -cnewer name: files\dirs with attrs\contents changed later than 
	given file name
  - -newer name: files\dirs with contents changed later than given
	file name
  - -ctime n - same as cmin, but n*24 hours is used
  - -mtime n - same as mmin
  - -empty - will match empty files and dirs
  - -inum n - search for files with inode n, useful to search for hard links
  - -samefile name - works same with -inum
  - -user name - files\dirs owned by given user  
  `find . -size 33c -user bandit7 -group bandit6 -print 2>/dev/null` - size 33bytes `bandit7:bandit6`, stderr to null
  - -nouser - files\dirs owned by noone, like deleted user
  - -group group_name - will search for files\dirs owned by given group
  - -nogroup - same but for groups
  - -perm 777 - search by permission

    __Operands:__
   Operands glues Checks\Operands and Groups of Checks:
  - -and\-a - logical AND - default value
  - -or\-o - OR
  - -not\-! - NOT
  - ( ) - group of Checks. Need to be Escaped, as per is special for bash.
	By default checks goes from left to right, so  
    __ORDER IS IMPORTANT__  
   __Example: __  
  `find . \( -type l -or -type d \) -a \(-name "this" -or -name "that" \)`  
   __Example:__  
  `find . -type f -name "*.bak" -print`  
   all files with name *.bak will be printed  
  `find . -print -type f -name "*.bak"`  
   print everytghint  

    __Params:__
  - -depth - process files first, then directories, Default for -delete
  - -maxdepth levels_number - how deep to dig
  - -mindepth levels_humber - go that deep before perform search
	seems like value 2 will ignore current dir, and 1 will not
  - -mount - do not search in mounted directories
	but still searches if explicitly set to search inside mounted dir
  - -noleaf - do not optimize search assuming its Unix filesystem
	required for DOS\WIndows CD-ROM systems
	But works fine without in on NTFS, probably required for exactly CD?
  


   could search by: 
   -date:
	  https://www.cyberciti.biz/faq/linux-unix-osxfind-files-by-date/
   Ranges example:
	  -m\a\ctime - modification\acces\creatioin? time <+\ \-days> 
	  - m\a\cmin - minutes i.e.:
	  find . -mtime -1 -ls - find files modified less than day ago
	  find . -mtime +1 -ls - all filed modified more than 1 day ago
	  find . -amin 1 -ls - all files accessed exactly 1 minute ago
	  -newerXY ; where XY could be:
	    a – The access time of the file reference
	    B – The birth time of the file reference
	    c – The inode status change time of reference
	    m – The modification time of the file reference
	    t – reference is interpreted directly as a time
	   i.e.:
	   find . -type f -newermt 2017-09-24 -ls
	    will find all files modified on 24/sep/2017
	   
   	by type
	 f - file
	 d - directory
	 l - symlink and others
	by user:
          TODO
	by group:
	 -group <name_of_group>

        by permissions:
  -perm - find by permission like Read\Write\Execute
   it seems to be pretty similar to just '-executable' flag(switch?)
    find /usr -type f -executable  - like this
  details here http://www.tutonics.com/2012/12/find-files-based-on-their-permissions.html
  i.e.:
    find / -perm 644 - will match EXACT permission files, only that have 644
    find / -perm -644 - will match files with at least 644 permission
     -perm -u+rw,g+r,o+r - same as above, will match 654 and dont 634
    find / -perm /644 - will match all files that have at least one of 3 sets
     /222 - match will occur if either the owner, the group, or other have their            "write" bit set.
     -perm /u+w,g+w,o+w - same as above, where u-user, g-group, o-owner
     -perm /a+w - same as above where a-all + or = is same so: /a=w 

  could execute commands on what found:
   find ~ -iname test.txt -exec du -h {} \;
    search in home dir for test.txt, case Insensitive, execute du -h for each
    found file. SEE '-exec' in man
    {} - is where the each find result will be piped.
    find and copy:
    !use with caution!  find . -name "*.pdf" -type f -exec cp {} ./pdfsfolder \;
  !see man for lots of details
  NOTE: with Permission denied erros, and probably others
   clear output of found stuff could be redirected to file. probably STDOUT
   while STDERR will keep appear on terminal window

## xargs:
  takes arguments for given command from stdin and executes given command with
   that arguments.
  If amount of arguments are more than a limit of command line xargs will 
   restart given command with what left of arguments list from stdin until 
   handle everything.
    Params:
  --show-limits - will display various limits on the system - command length,
	argument length, how many take the env vars, buffer size, etc.
  --null - accepts null symbol as delimiter, useful when filenames have
	spaces, and need to use another delimiter

   Example:
  find . -name "test*" | xargs ls -l
   will construct ls -l test1 test2 test3 .... and execute it

## locate:
  find files by name.
   NOTE: not all distros have it preinstalled
  call 'updatedb' before use, to update database used by locate
  locate <filename> - will return path to file if found
   i.e.
  locate kernel | grep /usr - will find any file contains 'kernel' in its name
   and grep /usr so it will show up only those which are in /usr dir(matched by)
    Params:
  --regex - search with regex
    Example:
   locate --regex '(^/|^/usr/)[s]?bin/(dz|gz|zip)'
    will search in /usr/bin or sbin or /bin or sbin for files having dz or gz or
   zip in their names


## update files:
 touch <filename> - will update access time of a file, or create new if none
 echo "" > <filename> - create\rewrite filename with blank line
 echo "text" >> <filename> - append existing file with line 'text'


## update-alternatives:
  > NOTE: `editor`(visudo) is not editable? on Ubuntu ; but `vim.basic` is linked by `vim`, so use `vim.basic`
  manages symbolic links i.e.:
   sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 - python3.5 as first list item
   sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 - python3.6 as second list item
   sudo update-alternatives --config python3 - will print the list and ask
	for number to point to for default usage, 2 will pick python3.6 for
	python3 entry
  See here: 
   http://ubuntuhandbook.org/index.php/2017/07/install-python-3-6-1-in-ubuntu-16-04-lts/
 Update EDITOR:
  sudo update-alternatives --install /usr/bin/editor editor /usr/bin/vim 10  
   where /usr/bin/editor  - the entrypoint link used when 'editor' is called  
         editor  - name  
         /usr/bin/vim  - real path which will be used when /usr/bin/editor is called by 'editor' name  
         10  - priority
  sudo update-alternatives --config editor   # pick vim editor which was  just installed

## shuf:
 shuffles the deck lol
 generages random output for range
 -i num-num - input range
 -n num - number of items from the list to display
  Example:
 shuf -i 20-25
  will output 5 numbers in range in random order
 shuf -i 20-25 -n 1
  will output 1 number from the range

## sort:
  sorts by alphabet by default
   Synopsis:
  Sort [option] file
   __Params:__
  - `-r` - reverse
  - `-R` - random sort
  - `-n` - sort by numbers
  - `-b` - ignore leading blanks i.e. '   a'
  - `-f` - ignore case
  - `-h` - sor human readable format, like 1M 1K 1G
  - `-k` - key i.e. -k 1, or --key=1
  - `-k` 5 (or --key=5) - use column 5

	 Example:
   ```
   ls -l /usr/bin | sort -k 5 -rn
   ```
	will sort files by field 5 (size) by Numberic and Reverse
	--key=1,1 - column from 1 to 1
	--key=2nr  - use second column to sort groups after first key
		    accepts short params after column num: b, n, r
	 Example: sort --key=1,1 --key=2n file.txt
	sorts by column 1(diapason is important), then sorts by second field
	numerically - will resort each just sorted group by second field:
	 abc 1   - w/o --key=2n this will be second line
	 abc 10
	--key=3.7 - column number.symbol number. Use b, just in case
	 Example: sort -k 3.7nbr > t.txt ... dates
	will sort by third column and 7th symbol in it, by year 01.01.2006 in 
	this dd.mm.yyyy format
  -m - merge sorted files i.e. sort -m file1 file2 file3 > merged_file
  -o - output to file i.e. -o=file
  -t - change separator, default is 'space' and 'tab'
	 Example:
	sort -t ':' -k 7 /etc/passwd
	will sort by 7th column using : as field separator

## uniq:
  deletes duplicates from __SORTED__ output  
  Uniq match only neighbour lines, so need to SORT all lines first and then search for uniquenness  
  __Use Case Example:__
  ```
  #Find unique line :
  #SORT OUTPUT FIRST
   sort data.txt | uniq -u
  ```
   __Params: __
  - -d - displays only duplicets (kinda inversion)
  - -c - displays number of matches and line (displays all lines) 
	 Example:
	`sort test.txt | uniq -c`
	will return lines in format '<tab><duplicate count> <duplicate line>'
  - -f n - ignore(forward to) n fields in each line, 
	delimiter is as in 'sort' but CANNOT be changed
  - -i - ignore case
  - -s n - skip n symbols in line
  - -u - unique lines only
 
  !Note: to get comparable output use 'sort' otherwice could display stuff in a
	way whcih will be not comparable. see ls examples below 

   Example:
  ls /bin /usr/bin | sort | uniq | less - will display combined unique files in
	both directories, in sorted order
  ls /bin /usr/bin | sort | uniq -d | less - will display only duplicated files
	in both dirs, in sorted order

## touch: 
creates empty file or updates file's date fields
```
touch my_file.txt
stat my_file.txt
```

## fallocate:
creates file of given size
Use Case Example:
```
# create file for swap of 1Gb size
sudo fallocate -l 1G /swapfile
```

## cat:
  concatenate, return file contents on screen. GNU
  cat <filename> <filename> - will return contents of both to screen
  cat <file1> <file2> >> <file3> - create new\append old file with joined
   contents of file1 and 2
  cat file.* > file - will combine lots of files
  cat > newfile - will create a new file and promt for input 
   Params:
  -A - shows all hidden\meta symbols
	^I - tab
	$ - end of the line
  -n - show line numbers
  -s - squeeze blank lines, leaves only 1 blank if 1+ exists concequently

## less:
  pager, upgraded version of 'more' command
  lists output in separate 'window' does not adding history in the console
  ALSO could be used like 'unzip' to view archives

## split:
  splits file into another files, like with collection variable
  split -l 2 <file name> -  will split the file by 2 Lines into new files named
			    xaa, xab, xac, etc.

## nl: 
  number lines, 
  number lines by pages(number from 1 on second page)
  number only regex matched lines (-b regex)
  add header(\:\:\:) and footer(\:) between body(\:\:)

## fold:
  split lines in two if length is exceeded
   Params:
  -w n - n length of w until move to another line
   Example:
  echo "long line of text and stuff" | fold -w 6
  >long line 
  >of text an
  >d stuff

## fmt:
  format text, like 'fold' but saves emtpy lines, spaces and tabs
   Params:
  -w n - how long line should be (same to fold) , but screws up spaces and tabs
  -c - unscrews up spaces and tabs
   Example:
  fmt -cw 50 text.txt
   will format file with lines of 50 long, saving empty lines and tabs

## printf:
  format string, like String.format(). has no new line \n, gotta add it manually
  % - conversion specificator:
   s - string
     Example:
    printf "variable: %s; and another %s\n" var ano
    >variable: var; and another ano
   d - digit negative or positive (1 or -1)
   f - float, with comma - (1,1 > 1,100000)
   o - octal number (1-8, like 8 bites in 1 byte, or chmod 777)
	Example:
	 "octal 1 is %0 octal 9 is %0)" 1 9 > 1 .... 11
   x - hex number (1-9a-f < lowercase)
   % - escapes % (printf "%%")
   %50s - formated string max 50 chars wide
   

## wc: 
  word count, returns number of lines, chars\bytes and words
   -l - count lines

## head:
  returs top of the file - 10 lines by default
   Params:
  -n <num> - change number of returned lines

## tail:
  returns bottom of the file - 10 lines by default
   Params:
  -n <num> - number of returned lines
  -n +<num> - output starting from line, could cut off header lines
  -f  - follow, will update file if any

  Example:
   ls -l | tail -n +2
    will return only listing fo files, w/o header lines:
   $ ls -l
   > total 1140
   > -rw-rw-r-- 1 dos dos 0 feb 3 01:02 1.txt
   ...
    And:
   $ ls -l | tail -n +2
   > -rw-rw-r-- 1 dos dos 0 feb 3 01:02 1.txt
    WHich is usefull for automation
    Same goes with 'ps'

# Troubleshooting:
 Debian:
  /var/log/syslog
  /var/log/auth.log
 RHEL:
  /var/log/messages
  /var/log/secure
 /var/log/syslog too large
 some single event spammed and keep spamming syslog, and it grows in size
 DO NOT DELETE /var/log/syslog
 Find the cause:
   watch tail -f /var/log/syslog
    or
   tail -100 /var/log/syslog
 instead clean the file:
   sudo cat /dev/null > /var/log/syslog

## Wifi adapter search for errors:
  lshw -C network
  uname -rv
  lspci -knn | grep -EiA2 net
  dmesg|grep -Ei 'wlan|firmw|dhc'
  sudo cat /var/log/syslog | grep -Ei 'net|wpa|dhc'
  sudo rfkill list
  https://www.linuxquestions.org/questions/linux-newbie-8/asus-pce-ac51-wlan-adapter-driver-doesn%27t-install-properly-4175619526/

## Network troubleshooting:
  nmcli device 		# display all devices 
   or: nmcli device show eth0 - will show info about the interface(eth0) including dns, gateway, address, mac etc.
  >DEVICE  TYPE      STATE      CONNECTION  
  >wlo1    wifi      connected  homenetwork 
  >lo      loopback  unmanaged  --     
    # connected in wifi type device means it is working and connected to 
    # wireless router

  Networking off:
  nmcli networking off
  nmcli networking off

  lshw -C network	# display all info related about network devices
   Output:
    *-network               
       description: Wireless interface
       product: Intel Corporation
       vendor: Intel Corporation
       physical id: c
       bus info: pci@0000:00:0c.0
       logical name: wlo1
       version: 03
       serial: a0:a4:c5:3f:67:0e
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress msix bus_master cap_list ethernet physical wireless
       configuration: broadcast=yes driver=iwlwifi driverversion=4.15.0-46-generic firmware=34.0.0 ip=192.168.0.101 latency=0 link=yes multicast=yes wireless=IEEE 802.11
       resources: irq:44 memory:a1114000-a1117fff

   Where: 
  description - if device is set up incorrect will have 'Intel Corporation' or
   something similar to manufacturer, otherwice 'Wireless interface'
  configuration - the configuration of device, also has info about driver for 
   the device, ip address

!NOTE:
  If there are no info about drivers this could mean that kernel does not 
  have proper driver for the device, need to check what exact manufacturer and 
  network controller it is and then look for drivers on the manufacturer 
  website
    ALso make sure that kernel version is newer than network adapter, because
  then kernel could not yet has drivers for the device. As option make sure 
  that using newest kernel available

 lspci			# lists all pci devices on the machine
  make sure something related to Network is there:
   00:0c.0 Network controller: Intel Corporation Device 31dc (rev 03)
  the part with 'Intel Corporation Device 31dc' is the device that need to 
  has correct drivers. 

## Nvidia drivers troubleshooting:
[Drivers uninstall](https://linuxconfig.org/how-to-uninstall-the-nvidia-drivers-on-ubuntu-20-04-focal-fossa-linux)  
```
$ sudo dpkg -P $(dpkg -l | grep nvidia-driver | awk '{print $2}')
$ sudo apt autoremove
```
[Drivers Install](https://www.cyberciti.biz/faq/ubuntu-linux-install-nvidia-driver-latest-proprietary-driver/)  
```
sudo apt install nvidia-driver-390
```

Install\Reinstall problems
if any file gets unexpected lock during Drivers install check who locks it:
```
lsof | grep /var/cache/debconf/config.dat
OR
fuser /var/cache/debconf/config.dat
```
Check for command the locking process tries to execute, I.e. it tries to confirm addition of another Key into UEFI keys database (MOK), that need to be killed and re-executed; then reboot: [Secure boot Fix](https://superuser.com/questions/1493050/update-secureboot-policy-enroll-key-running-on-every-new-startup-eating-reso)

### Secure boot work
[Secure Boot on Ubuntu](https://wiki.ubuntu.com/UEFI/SecureBoot#How_UEFI_Secure_Boot_works_on_Ubuntu)

### Enroll new keys required for signed Drivers to work 
[Enrolling the Keys with the UEFI MOK](https://documentation.commvault.com/commvault/v11/article?p=118661.htm)


# NetworkManager:
  network management daemon  
It uses different plugins for different distros and stuff, to be more system-agnostic
### Configuration:
`/etc/NetworkManager`  
`NetworkManager.conf` - main config file .ini\XDG like  
`[main]` - section to load the Plugins  
Plugins:  
- ifupdown - main network plugin for Debian-like
- ifcfg-rh - Redhat-like network plugin
- ifcfg-suse - SuSE-like network plugins
- keyfile - native configuration plugin support
  - stores System-known connections under `/etc/NetworkManager/system-connections` (chmod 600 - only root can view\change)
    - Also passwords in plain text!

#### Ignore Network Interfaces (Unmanaged by NetworkManager)
Usually you want to ignore at least `lo` interface, since it better be configured earlier  
Unmanaged interfaes are configured earlier and then NetworkManager could ignore them
if controlled by `ifupdown`(Debian\Ubuntu):
```
# /etc/network/interfaces:
# configure interface here

# NetworkManager.conf:
[ifupdown]
managed=false
```
if controlled by `ifcfg-rh`(Red Hat):
```
# /etc/sysconfig/network-scripts
# find required ifcfg-* interface file
# make sure there is no line
# NM_CONTROLLED=yes (delete it or set `no`)

# --OR--
# Add Hardware address:
HWADDR=10:78:af:b1:00:61
```

Or ignore the IF by it's MAC address
with `keyfile` plugin:
```
# NetworkManager.conf:
[keyfile]
unmanaged-device=mac:<MAC>;mac:<another_MAC>

```

### Configuration runners
`/etc/NetworkManager/dispatch.d`  
NetworkManager would use 2 params `$1=iface` and `$2=up|down`
contains scripts to be run on UP or DOWN actions (including vpn-up\vpn-down)  
Systems like Ubuntu could bypass this UP\DOWN params into own scripts:  
`dispatch.d\03-ifupdown` will call `/etc/network/` appropriate dir   
  i.e. `if-up.d` using Ubuntu's `run-parts` which will run all executables in a dir  




### nmcli:
  network manager command line interface
   Synopsys:
  nmcli [PARAMS] OBJECT {COMMAND | help}
   OBJECTs:
  g[eneral]       NetworkManager's general status and operations
  n[etworking]    overall networking control
  r[adio]         NetworkManager radio switches
  c[onnection]    NetworkManager's connections
  ```
  #list all connections active and inactive
  nocli connection show
  ```
  d[evice]        devices managed by NetworkManager
  ```
  #list all network devices - real and virtual
  nmcli device show
  ```
  a[gent]         NetworkManager secret agent or polkit agent
  m[onitor]       monitor NetworkManager changes
   Example:
  nmcli device - will display all network devices

## arp:
Lists kernel's ARP Cache  
__Params:__
- `-n` - do not show DNS names
- `-d` - Delete host: `apr -d <host>`
- `-i` - show ARP cache for `if`: `apr -i interface`

##iw:
interface Wireless - Lists all wireless connections
Could create Multiple (virt)Interfaces from Single interface
Security is controlled by daemon `wpa_supplicant`
location: `/etc/wpa_supplicant.conf`

Check wireless connections for wireless interface:
```
sudo iw dev wlo1 scan | less
# if interface is down, turn it up first
ifconfig wlo1 up
```

Check current connected wireless netowrk:
```
sudo iw dev wlo1 link
```
MAC address is of Access point which you connected to

Connect to unsecured network:
```
iw wlo1 connect network_name
```




## ifconfig:
  displays current network interfaces
  part of the 'net-tools' package
   /etc/network/interfaces - for interfaces(5) changes
  -a - will display MAC also- 

UP interface:
`ifconfig wlo1 up`

### Setup netowrk connection old way:
Need to perform steps:
- Connect device to Computer, ensure there is a driver
  - `ifconfig -a` to display the new Kernel Network Interface
- Set network name or password etc.
- Add Computer IP and Net Mask to the new Network Interface - this way `Physical` and `Network` layers of OSI model can communicate between each other
- Add any additional Routes. Including `Default Gateway`

Add Computer IP and Net Mask:
```
ifconfig <interface> [IP addres] mask [Net Mask]
```
 - interface - name of the IF (i.e. eth0)
 - IP Address - the IP addr of the current IF (network card)
 - Net Mask - subnet mask like 255.255.255.0

 Add Default Gateway:
 ```
 route add default gw <GateWay IP Address>
 ```
 GateWay IPAddress must be on the same network as the Interface - same Subnet mask and IP Address(of the network card) should match a subnet and Gateway I
## lshw:
  list hardware, displays all the hardware detected by kernel and its state
  and properties
   -short  - short list of all devices found and their Classes
   -businfo  - Better: than above, more humanfirendly, same list output
   -class <class> - detailed info about class of devices (-short for classes)
   -C <class> - same as above
    Example:
   lshw -C network - will display all info about network adapters and their
                    state, together with drivers and info related
   lshw -C display - graphics drivers

## hdparm:
  lists HDD info
  -I - get long list of HDD info

## lspci:
  list pci devices - network , audio, usb controllers
   Example:
  lspci -knn | grep -EiA2 net
   search for network controllers, and this is drivers used by them:
    Kernel driver in use: rtl8821ae
    Kernel modules: rtl8821ae

## uname:
  system and kernel information
   -r   - kernel info
   -v   - Distro info
   Example:
  uname -rv

## Boot and Startup:
  Boot when loads:
  * BIOS\UEFI
  * MBR
  * GRUB
  * kernel is loaded into initial ramdisk(initrd or initramfs)
  * systemd started
  Startup:
  * syslogd or rsyslogd is not running yet
  * all logs go into `ring buffer`
  * to read `ring buffer` use `dmesg`


## dmesg:
  https://www.howtogeek.com/449335/how-to-use-the-dmesg-command-on-linux/
  reads logs from Kernel Ring buffer 
   kernel ring - place where logs stored during boot and OS loading
    BIOS\UEFI, MBR, GRUB
   During system run also stores all events like new hardware etc.
  Example:
   dmesg | grep -Ei 'wlan|firmw|dhc'
    check drivers loading for wifi adapter
  sudo remove:
   sudo sysctl -w kernel.dmesg_restrict=0
    ANY USER will be able to use `dmesg` now
  -L   - color output
  --color=always  - always use color (pipe redirection mb?
  -H   - human timestamps + `less` is used for output
         seconds are labeled with date
    Example:
   dmesg -H
    [Apr 5 00:41] [UFW BLOCK] IN=wlp2s0 OUT= MAC=01:00
    [  +9.499881] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e
    [  +2.499111] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e
    [  +0.126267] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e
    [  +1.818683] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e
    [  +2.920082] [UFW BLOCK] IN=wlp2s0 OUT= MAC=a8:5e
  -T   - human readable timestamps:
    Example:
   dmesg -T
    [Sun Apr  5 00:45:25 2020] [UFW BLOCK] IN=wlp2s0 OUT=
    [Sun Apr  5 00:45:29 2020] [UFW BLOCK] IN=wlp2s0 OUT= 
    [Sun Apr  5 00:47:30 2020] [UFW BLOCK] IN=wlp2s0 OUT= 
  --follow  - like `tail -f`
  -l <lvl>   -  Log levels
    emerg: System is unusable.
    alert: Action must be taken immediately.
    crit: Critical conditions.
    err: Error conditions.
    warn: Warning conditions.
    notice: Normal but significant condition.
    info: Informational.
    debug: Debug-level messages.

    Example:
   sudo dmesg -l debug,notice

  -f <facility>  - Facility categories
    kern: Kernel messages.
    user: User-level messages.
    mail: Mail system.
    daemon: System daemons.
    auth: Security/authorization messages.
    syslog: Internal syslogd messages.
    lpr: Line printer subsystem.
    news: Network news subsystem.

    Example:
   dmesg -f syslog,daemon

  -x  - decode. show the facility and level as human-readable prefixes 
   to each line.

## tr:
translates or delete Characters  
gets from STDIN outputs in STDOUT  

__Use Case Example:__
```
# ROT13 Encode and Decode back
echo 'fooman@example.com' | tr 'A-Za-z' 'N-ZA-Mn-za-m' | tr 'N-ZA-Mn-za-m' 'A-Za-z'
```

## sed:
  text stream editor, using RegEx for search\edit in stream(STDIN) line by line,
  in a non-interactive way(all decisions made\given in command params)
  Reads from file or STDIN
  Writes into STDOUT or to file if given 'w <file>' param or redirected STDOUT
  Does not change original file content (-i will edit it)
  Does only 1 change per line by default, 'g' - global, will match more than 
  first occurance
  Also can just write something(matched) from stream into a new file
=   sed '' file.txt - will just print everything, same as 'cat'

 See More: https://www.digitalocean.com/community/tutorials/the-basics-of-using-the-sed-stream-editor-to-manipulate-text-in-linux
 https://www.digitalocean.com/community/tutorials/intermediate-sed-manipulating-streams-of-text-in-a-linux-environment

  Use Case Example:
  ```
  #rename part file
   6 REDACT='0930_22'
   17   redacted=$(echo $i | sed "s/...._../$REDACT/")
   18   echo "mv $i r$redacted"

  >mv IMG_20201001_001836.jpg rIMG_20200930_221836.jpg
  ```


  sed <param> '<param1>/pattern/replacement/<param2>' <file>
  -n - supress default ouput
  '/p' - print , works with 's'
    sed -n 'p' file - will print every line of file
    sed 'p' file - will print every line of file TWICE
  '1p' - print first line (address range)
    Example:
    sed -n '1p' /path/to/file    - will show only first line
  '1,5p' - print from first to fifth line (address range) 
  '1,+4p' - print 4 lines from first(inclusively) (address range)
  '1~3d' - print every other line: first then skip until 3rd line and again
	print 1st line(drop counter)
  '/pattern/,/pattern/' - address range matched by patterns
	'/^START$/,/^END$/d' - will delete everything between lines START and
	END, and do that again for another 'START' (until EOF or 'END' line)
  'd' - delete
  -i - edits IN-place - edits given file
   -i.bak - creates baskup of original Edited IN-place file
  's/pattern/replace/' - Substitute given RegEx pattern with given word
	's_pattern_replace_' - will work if need to use slash inside fields
	  or any other character as far as all 3 are consistent
  's/pattern/(&)/' - & is match result, so it will replace matched text with
	() around this same text
  's/\(matchGroup\)/\1/ - \( is escaped to work as matching group, \1 is the
	referenced matching group
  '/w <new_file>' - Write changed lines into file - param2 group
  '/g' - Global, will not go next line until first match
   '/2' - Instead of 'g' will match only SECOND appearance of pattern
  '/i' - Ignore case
  '/G' - inserts blank line after each line
  '=' - inserts line with a number of line after each line
  '/pattern/s/' - matches and substitutes only in the line matched by pattern
	Pattern could be complex too i.e. '/^$/d' will delete all empty lines
	 where ^ - beginning of line immediately followed by $ - end of line
  '/pattern/!' - matches everything EXCEPT pattern
	'/^$/!d' - will delete everything except blank lines
  
     Example:
=   sed 's/parttime/fulltime/gw promotions.txt' team - will Substitute 
	everything found by pattern 'parttime' to new value 'fulltime' more than
	once a line and will Write lines
	that were changed into 'promotions.txt' in file 'team'
=   sed 'fulltime/w fulltime.txt' team - will write all matched by 'fulltime'
	pattern lines from file 'team' into file 'fulltime.txt'
=   sed -n '/pattern/p' annoying.txt - will print only line containing pattern
=   sed .... ... > /dev/null - will supress STDOUT, actually redirecting it 
	into void
=   sed '0,/parttime/s/parttime/promotion/' team - will replace First occurance
	of 'parttime' to 'promotion' in 'team' file
=   sed 's/<[^>]*>//' team - will substitute matched by expression pattern with
	nothing - // it is empty, so nothing. 
	Regex: <[^>]*> - matches HTML tags
=   sed -n 's/on/forward/2p' file - will print out only lines where replace 
	was done, and replacing only 2nd match in line
=   sed 's/^.*at/(&)/' annoying.txt - puts () around matched text
=   sed 's/and/\&/;s/people/horses' annoying.txt - substitues and to escaped &
	then substitutes people to horses in annoying.txt
=   sed '/^$/d' file - will delete empty lines(beginning of line followed by end
	of line) from a 'file'


## tee:
  reads from STDIN and writes to STDOUT and File
  Overrides existing file by default , to append use -a
   Good for saving in the middle of pipe
  tee <params> file1 file2
  -a - append file
    Example:
  ls dir1 | tee file1 
       will display ls on dir1 and write output to file1
  ls dir2 | tee -a file1 | less 
       display dir2 and append ls output to file1
       then moe along a pipe and 'less' with the same result as in file1
   
   
## nl:
  number lines in text file. only numbering not empty lines


## mkdir:
  creates dir
  mkdir <params> <path>
  -p - creates every unexisting dir along the path, return no errors
   mkdir -p /not_exist1/not_exist2 - will create both not existing dirs 
  mkdir <dir1> <dir2> <dir3> - will create 3 separate directories

wildcards(not a command:
 * - any number of symbols
 ? - any symbol
 [ab1,] - one symbold from a list (same to regex)
 [!ab1,] - none of listed symbols([^] in regex)
 [[:class:]] - one symbol from a class
  :alnum: - digits and letters (\w in regex)
  :alpha: - letters (similar to  Unicode character property class \p{L} in regex)
  :digit: - digits (\d in regex)
  :lower: - lowercase
  :upper: - uppercase

## mktemp:
makes temp file or directory with obfuscated hard-to-guess name  
files and dirs are created in `/tmp`  
- -d - make directory, not a file
- --dry-run - dry run  
__Example__:
```
mktemp -d
```

## cp:
  copy stuff
  cp <params> <source1 src2 src3> <destination>
  cp -rf ../source/* .
   -r - recursive copy
   -f - overwrite everything 
   -i - will ask for overwrite
   -a - archive - keep original access rights and owners
   -u - if overwrite - will copy only newer files
   -v - verbose
   . - current directory with saving all the names and paths from source
  NOTE: to copy hidden files, those star with . , need to escape dot(.) as it is
  regular expression:
  cp /etc/skel/\.* . - will copy all hidden files to current directory
  ............/.* . - will not work
  ............/.*.* . - will copy '..' which is parent directory to current one

## mv: 
  see cp: above , pretty same, except it moves\renames files

## rm:
  remove stuff, does not delete dirs and not empty dirs w/o additinal params
  rm <params> <source>
  rm -r <dirname>
   -r - recursive, also deletes directory at the end
   -d - deletes EMPTY directory
   -f - force
   -i - interactive - will ask before do
   -v - verbose
  Remove symlink:
   rm symlink_to_file
   rm symlink_to_dir
   rm symlink_to_dir/   <-- will not work

## unlink:
  delete symlink
   Example:
  unlink symlink_name

  Find broken link:
   find /path/to/dir -xtype l
  Delete:
   find /path/to/directory -xtype l -delete

## ln:
  create hard link by default
  ln <param> <path> <link name>
  -s - create soft link
   Example:
  ln -s /src/file /dest/symlink
	first param is where the file is 
	second param where to put symlink
   Example, symlink from dir to dir:
  ln -s /full/path/to/dir /full/path/to/symlink
  ln -s ~/ansible/buoy ~/opt/tideworks
   this will create 'tideworks -> /home/dos/ansible/buoy/'
   where 'tideworks' is a link which points to another directory ansible/buoy
  https://www.shellhacks.com/symlink-create-symbolic-link-linux/
  
  Hard link: every file has 1 , create another one in different place
   it will has size of its name, and will point to place on HDD for original file
   File can not be deleted until all Hard links deleted.
   Note: ls -i will return memory block of a place where link points
    so file and is hard links will all point in the same place, thus having 
    same number next to it.
   ONly can be created in the same filesystem (disk drive)

  Soft link: upgraded hard link, could be created anywhere, orig file can be
   deleted > link will become broken

## type: 
  returns how given command is interpreted
   Example:
  dos:~$ type ls
  >ls is aliased to `ls --color=auto'
   Example:
  dos:~$ type cd
  >cd is a shell builtin
   Example:
  dos:~$ type apt
  >apt is /usr/bin/apt


## curl: vs wget:
  If I wanted to download content from a website and have the tree-structure 
  of the website searched recursively for that content, I’d use wget.

  If I wanted to interact with a remote server or API, and possibly download 
  some files or web pages, I’d use curl. Especially if the protocol was one 
  of the many not supported by wget.


## wget:
  non interactive network downloader
   Params:
  -q - quiet, turns off wget output
  -i file / --input-file=file - reads URLs from a file
   Example:
  wget -q https://packages.microsoft.com/config/ubuntu/16.04/packages-microsoft-prod.deb - will download .deb file in current directory w/o any output

## curl:
  HTTP talker  
   Work:  
  `curl --trace-ascii trace_file http://www.wikipedia.org`  
   will dump Received\Sent Headers and Data  
   > In HTTP - Headers sent first, then empty line (2 byte size) then Data(HTML Document for instance)  
     curl sends Headers message, 2 byte empty message and then Data messages  
     HTTPS Server sends Headers messages, 2 byte empty message and then Data messages  
  web requests issuer - HTTP[S] requests( see --version protocol list).
   CAREFUL: binary download to stdout in terminal could exploit injections
   https://www.howtogeek.com/447033/how-to-use-curl-to-download-files-from-the-linux-command-line/
  Or could download web pages and files and stuff
   -o - output, redirects output to a file, usefull when downloading files
    sudo curl -o /usr/local/bin/ecs-cli https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest
     will download ecs-cli binary file from amazon storage into /usr/... path
    > - will also workd:
      Example:
     curl https://www.bbc.com  > bbc.html
     When redirected, `curl` detects it and display download bar, columns:
     % Total: The total amount to be retrieved.
     % Received: The percentage and actual values of the data retrieved so far.
     % Xferd: The percent and actual sent, if data is being uploaded.
     Average Speed Dload: The average download speed.
     Average Speed Upload: The average upload speed.
     Time Total: The estimated total duration of the transfer.
     Time Spent: The elapsed time so far for this transfer.
     Time Left: The estimated time left for the transfer to complete
     Current Speed: The current transfer speed for this transfer.

   -#  - display progress bar instead of full statistics

   -C    - continue from offset
      -  - continue from last downloaded offset: simply continue download
     Example:
     curl -C - --output file.name http://example.com/some_partially_dwnld.file
      Note line:
    `** Resuming transfer from byte position 168435712`

   -I - Headers!
     Example:
    curl -I https://www.bbc.com

    download list of files:
    * save list urls into file; new line delimited
    * use xargs to read a file; each line as an argument
    * pass to curl:
     xargs -n 1 curl -O < urls-to-download.txt
      -n 1  - treat each line in file as a separate parameter

  -O - save file with same name from remote server

   -s - silent, no progress or error output. BUT still displays url contents
    echo "$(curl -s https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest.md5) /usr/local/bin/ecs-cli" | md5sum -c -
     will curl display md5 from amazon storage, w/o any other output, then
      will display path to local ecs-cli, which will go into pipe to md5 sum
      which will compare given md5 and md5 from file under given path, and will
      return OK if everything if fine
  Example: Generate crumb(token) and use it as a header to POST request
 to the server restfull call
    crumb=$(curl -u "builder:1234" -s 'http://jenkins.local:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)')
    curl -u "builder:1234" -H "$crumb" -X POST http://jenkins.local:8080/job/ENV/build?delay=0sec
  -H - header of a request(contains Token)
  -u - "user:password"; Works with FTP too
  -X - Request type (GET or POST, PUT, DELETE etc) 
   default: GET
  


## du:
  disk usage
  du <params> <file/dir name>
   Does not accept input from pipe, seems like
    use 'xargs'
    
```
  Where to look for used space:
  du -hs /var/log/journal/
  du -hs ~/deja-dup
```
  Cleanup `systemd` journals:
read more: https://ma.ttias.be/clear-systemd-journal/
```
journalctl --vacuum-time=10d
```

  du -hs dirname
   -h - human readable
	Example:
       ls -d */ | xargs du -hs  - will display size of all subdirs of of 
				current dir
	Or:
       ls -d */ | xargs du -hs | sort -hr | head -20 - top 20 biggest dirs
	Or:
       du -sh */ | sort -hr  - du also accepts wildcards, so no need
				to use ls for supplying list to du
   -s - summary for every directory w/o listings its contents
   hidden directories:
  du -hs .[^.]*

## diff:
  checks differences two files or whole directories
   could recursevly dig into dirs for diffs, and generate patch files with
  differences, to be applied by 'patch' command
   Params:
  -c - context diff format (easier to read than standard POSIX diff)
  -u - unified diff format (less text, even more easier)
   Example:
  diff packages.list packages.list2
    packages.list:
     pkg1
     pkg2
    packages.list2:
     pkg2
     pkg3
   POSIX compliant output:
    1d0
    < pkg1
    2a2
    > pkg3
    read output:
   < - only first file has pkg1, it was on the left in the params list
   > - only second file has it, and it was on the right

  -c output:
   *** pkg1.lst	2019-02-02 02:22:25.312819822 +0200
   --- pkg2.lst	2019-02-02 02:22:36.984692484 +0200
   ***************
   *** 1,2 ****
   - pkg1
     pkg2
   --- 1,2 ----
     pkg2
   + pkg3

  -u output:
   --- pkg1.lst	2019-02-02 02:22:25.312819822 +0200
   +++ pkg2.lst	2019-02-02 02:22:36.984692484 +0200
   @@ -1,2 +1,2 @@
   -pkg1
    pkg2
   +pkg3

   Symbols:
   (none) - line match
   -      - line removed from first file
   +      - line added into second file
   !      - line changed (only -c mode)

## patch:
  'diff' generated patch apply
   generated patch file already has file names, so no need to say anything
  just apply:
   patch < patchfile.txt
  Just kidding - use VCS

## bash history:
  ! - invokes the Bash history mechanism
    !echo - will display and execute latest echo from current shell history
	    if such exists, displays error if nothing found(event not found)
   inverts exit code of commands if with space:
    ! true; echo $? # 1 - means exit code 0 was changed to 1
   also inverts pipe exit code
    ls | bogus_command; echo $? # exit code: 127
    ! ls | bogus_command; echo $? # exit code: 0

## unzip:
  utility to unzip files. Unzips in the current folder by default
   Params:
  -d - specify folder where to extract
  -l - lists archive contents, 'less' tool does the same

## zip: 
  creates zip archive, could also create encrypted archives(with password)
  --encrypt - will promt for password during arch creation:
   Example:
  zip --encrypt archive.zip files
   Less secure example:
  zip --password (password) file.zip files
  
  NOTE: zip encyption is very weak, use additional or other encription like gpg

## Encryption:
public private keys ssl , stuff
 https://www.devco.net/archives/2006/02/13/public_-_private_key_encryption_using_openssl.php
## Sign with private key:
 Example:
 https://raymii.org/s/tutorials/Sign_and_verify_text_files_to_public_keys_via_the_OpenSSL_Command_Line.html

Create CRS - certificate signing request (csr)
 https://www.sslshopper.com/what-is-a-csr-certificate-signing-request.html
 openssl req -new -newkey rsa:2048 -nodes -out servername.csr -keyout servername.key 

General info about Keys, Key Pairs, Key Storages and stuff:
 https://info.townsendsecurity.com/definitive-guide-to-encryption-key-management-fundamentals#How-Encryption-Key-Systems-Work

## gnupg: gpg:
  Gnu PG could encrypt documents, basically all files
   See docs for encription here:
  https://www.gnupg.org/gph/en/manual/x110.html

  basic example w/o using Public-key cryptography:
   gpg --output <encrypted_file.gpg> --symmetryc <file_to_encrypt> - this will 
	ask for passphrase and its done
 Params:
  `--gen-key`  - generate a key
  `--list-keys`  - list all keys

## rsync:
  Remote Syncs files\dirs locally or remote-local\local-remote
   Synopsis:
  rsync [-params] /source/path [/another/source] /receiver/path
   Source-Receiver could be:
  - local file or dir
  - remote file or dir [user@]hostname:/path/to/file
  - remote server rsync, URI - rsync://[user@]hostname[:port]/path/to/file
   Source or Receiver MUST be local, BOTH Remotes NOT WORK
    Params:
  -a - archive, recursive and saves files attributes
  -A - preserve ACLs
  -v - verbose
  --delete - remove files that are in Receiver but not in SOurce
  --rsh=<shell> - remote shell, i.e. ssh - will transfer files via network
	in secure way (secure ssh tunnel)
   Example:
  rsync -av --delete --rsh=ssh /etc /home /usr/local remotesys:/backup
    Will archive(recurse and save file attrs) with verbose output , deleting
    files absent in source dirs and present in remote:/backup, copy different 
    from source dirs files via ssh tunnel to remotesys network location under
    path /backup
   Example:
  rsync -aAHvh bitbucket-home.tar.gz root@o-pe-bitbucket.twlab.int:/opt/atlassian/bitbucket/

## ar:
  archive?
   seems like can unarchive deb packages(other may be too?)
  Example:
  https://dev.to/jake/using-libcurl3-and-libcurl4-on-ubuntu-1804-bionic-184g
   Download libcurl3 package, extract into tar, untar it, copy following
   symlink in source(inside untarred dir) into usr/lib:
  $ mkdir ~/libcurl3 && cd ~/libcurl3
  $ apt-get download -o=dir::cache=~/libcurl3 libcurl3
  $ ar x libcurl3* data.tar.xz
  $ tar xf data.tar.xz
  $ cp -L ~/libcurl3/usr/lib/x86_64-linux-gnu/libcurl.so.4 /usr/lib/libcurl.so.3
  $ cd && rm -rf ~/libcurl3

xz:
  similar to `gzip` and have same arguments  
   

## tar:
  tape archive - collects all files in archive, with savin all permissions
   and user's ownership
  tar <params> <archive> <another_params> <source>

  Create Arhieve:
  ```
  tar -cvf name.tar directory
  ```
  List archieve or compressed archieve contents:
  ```
  tar tvf name.tar
  or
  tar ztvf name.tar.gz
  ```
  Compress Archieve:
  ```
  gzip name.tar
  or
  tar czvf name.tar.gz
  ```
  Decompress and Unarchieve:
  ```
  gunzip name.tar.gz
  tar -xvf name.tar
  or
  zcat name.tar.gz | tar -xvf -
  or
  tar -zxvf name.tar.gz
  ```
  Unarchieve specific file:
  ```
  ```
   -c - create
   -v - verify\verbose
   -f - files
   -t - view archive , or something like that
   -r - append regular file to the end of an archive i.e:
     tar -rvf uncompressed.tar mybkup/mytest.txt
   -z - zip. tells tar that working with zips and not archives; 
        used for zipping, unzipping, viewing zip,
          if 'czvf' - creates .gz archive, compress same as 'gzip'
          if 'tzvf' - lists zip contents
          if 'zxvf' - extracts zip contents
     Example archive+compress:
    tar -zcvf tmp.tar.gz tmp
     Example unarchive+uncompress:
    tar -zxvf tmp.tar.gz
   -j - zup but with bzip2 instead of gzip
   -p - preserve permissions (ignored `umask` setting). used by Default when run under Supersuser
   another params:
    !! could be anywhere in the command, before or after other params
    --exclude=filename - excludes filename/dir/type from adding to archive
      i.e. tar -czvf arch.tar.gz --exclude=file.txt source_dir/
      !! use relative path of tar archive itself! not system file path !!
      !! use path like 'folder/folder/file' w/ or w/o quotas, optional
      !! DO NOT use path like './folder/folder/file' - this will fail silently
  params could be used w/o dash, like 'tar tvf archive.tar'
  Unzip:
    tar zxvf <archive.tar.gz> <path>
   -z - unzips contents
   -x - eXtract it
   -v - verbose
   -f - files
   <path> - cold be empty for current location
  Could pass view output to grep to search for particular file

## gzip:
  compress files. it looks for a file it can compress by default even w/o
   specifying its name
  gzip <filename>
  gzil archive.tar
   will substitute 'archive.tar' by 'archive.tar.gz'
  Could be called by 'tar' command using 'z' key, will works the same
   Params:
  -c - compressed output to STDOUT(console), original file is left untouched
  -d - decompress file, same as 'gunzip'
  -f - force compresses even if .gz with same name already exists
  -h - help
  -l - list files in archive, with compress ratio and orig sizes
  -t - test compressed file for integrity
  -v - verbose
  -1 - set compress ratio 1 to 9, where ()
	1 - fast and little uncompressed and (also --fast)
	9 - slow and super compressed (also --best)
	6 - default value

## gunzip:
 g unzip, unzips .gz files
  -c - cat, same as in gzip - writes to STDOUT leaves original files unchanged

## zcat: 
 shipped with gzip, is like 'cat' but with z
  could be used instead of:
 gunzip -c file.gz | less

## zless: 
 shipped with gzip, is like 'less' but with z
  could be used instead of:
 zcat file.gz | less

## zgrep:
 grep for search in .gz compressed files

## lsblk:
  NOTE: also all Block devices would be in `/sys/block` directory  
  NOTE2: also all devices (and Block too) __with drivers__ are in `/proc/devices`  
  file starting with same name (and device DI in `/proc/devices`) will be in `/dev` directory  

  List Block Devices
  __list all devices mounted and not__
   Example:
   ```
    #show only 'sda' and 'sdb'
    lsblk -po NAME,FSTYPE,SIZE,MOUNTPOINT /dev/sda /dev/sdb
    
   ``` 
   Params:
  - -f/--fs - list fylesystems
  - -d/--nodeps  - show only top device(w/o partitions)  
   Example:
    ```
    lsblk -d /dev/sda
    ```
  - -o Col1,Col2  - Only columns Col1 and Col2
  - -O   - all columns
  - -p   - full path of Block device
  - -e/--exclude - exclude block device number  
    Example:
    ```
    excludes Loop(7) and 11th block devices (see /proc/devices)
    lsblk -e7,11 -po NAME,FSTYPE,SIZE,MOUNTPOINT
    ```    
   __Columns:__  
  - `MAJ:MIN` - block device number : number  
    -  `/proc/devices` - list of ALL block device numbers
  
## lsscsi:
  lists `scsi` devices (all disks basically)

## block device types:
 /dev/fd* - floppy drives
 /dev/hd* - hard drives
 /dev/lp* - printers
 /dev/sd* - SCSI, including PATA/SATA in modern kernels, flash and usb, digital
	    cameras, players and stuff. 
 /dev/sr* - cd/dvd

## block device naming:
 old motherboards had 2 IDE channgels, with cable for 2 devices (master\slave)
 devices are named by alphabet
 partitions are named by numbers
 first channel master - a - sda
  first partition on sda - 1 - sda1
  second partition on sda - 2 - sda2
 first channel slave - b - sdb
 second channel master - c - sdc
 second channel slave - d - sdd

## tune2fs:
  adjust filesystem params
   Params:
  - l <filesystem> - list file system info
   Example:
  tune2fs -l $(df / | grep '/' | awk '{print $1}') | grep 'created'
   will return Filesystem created date, means when system was installed
  because looking at name of '/' root filesystem

## df:
  display all MOUNTED devices(EXCEPT SWAP) with size\free\used\paths etc
    Stands for: Disk Free
  -h - human readable sizes
  __SWAP DISKS ARE NOT MOUNTED - NOT SHOWN IN DF OUT__
    Example:
   df -h / - will show info about root partition - where / is mount point
  -T  - adds colump with FS Type
  -x  - exclude given FS Type
    Example:
    df -x tmpfs -x squashfs -h
     not show tmps and /dev/loop* (snap) filesystems

##mount find usb disk hdd drive:

## format usb disk:
 https://www.wikihow.com/Format-a-USB-Flash-Drive-in-Ubuntu
 https://www.cyberciti.biz/faq/linux-disk-format/

1. locate the disk and found its name:  
  tail -f /var/log/syslog  
 > look for something like this:  
 May 26 15:57:00 dospc kernel: [ 9385.461071]  sdc: sdc1 sdc2

2. #lsblk  

> unmount everything that auto-mounted from that drive

  sudo umount /dev/sdb1

3. (OPTIONALLY)  override everything , will take HOURS:

sudo dd if=/dev/zero of=/dev/sdc bs=4k status=progress && sync

3.1 Faster option, which will override MBR and partition table:

dd if=/dev/urandom of=/dev/sdc bs=1M count=2

4. fdisk /dev/sdc

 option: o

 will create empty partition table

 option: n

will create new partition, then use Defaults

 option: w

will write changes, could take some time. wait for it

5. lsblk

will return all block devices with updated sdc device, check size

6. format into FAT32

sudo mkfs.vfat /dev/sdc1

7. eject disk when done 

sudo eject /dev/sdc

---------------

how to get block device name, partition, mount it and unmount after:
  tail -f /var/log/messages 	- open messages and follow. deprecated
  tail -f /var/log/syslog - this is more standard log file.
  #mount disk, recheck messages for something like:
  "sdb: sdb1"
  "sd 3:0:0:0 [sdb] attached SCI removable disk"
  fdisk /dev/sdb
   p 				- see all partitions
  mkdir ~/mounted_stuff
  mount /dev/sdb1 ~/mounted_stuff
  umount /dev/sdb

## exportfs:
  maintain table of exported NFS file systems
  /etc/exports  - file containing all directories to be exported
    Example:
   /u01/app/mainsail       i-tpt-dv-db111(rw,no_root_squash)

   Params:
  -a  - export all unexported directories
    Example:
   exportfs -a
    will export all unexported directores, and those could be
    mounted on another machine

## mount:
  mount drive (disk, usb, floppy etc)
  __Troubleshooting:__
  - `tail -f /var/log/syslog` or `/var/log/messages`
  - use `-v` or `-vvvv` params with `mount` command

  USE CASES:  
  ssh into Network machine with mountable share  
  `vim /etc/exports`  
  > add line like `/u01/app/mainsail       i-tpt-dv-db111(rw,no_root_squash)`  

  `exportfs -ar`       
  > reload /etc/exports   <-- otherwise get 'mount.nfs: mount(2): Permission denied' error  
  Where:
  > - `-r` is reexport
  > - `-a` is export or UNexport `all` directories in `/etc/exports`

  `vim /etc/fstab`    
  > add mount like `i-tpt-dv-ms:/u01/app/mainsail        /u01/app/mainsail       nfs     rw,bg,hard,nointr,rsize=32768,wsize=32768,tcp,actimeo=0,vers=3,timeo=600    0 0`  
  `mount -a -vvvv`    
  > mount all lines added to /etc/fstab, doing it verbose

  simply add automount of HDD from old windows:  
  `LABEL=Juli      /otherHDD/Juli  ntfs rw,suid,exec,auto,user,async`  
  labels or UUID could be taken from `blkid` command (if block device) dev names are also accepted(`/dev/sda`), but could mess around if block devices are detected in different order 

  UUID:
  ```
  sudo lsblk -f
  #or
  blkid /dev/mmcblk0p3
  ```

  Remount root as read-write:  
  `mount -o rw,remount /`    

  Params:  
  - -o - options like `rw,remount` and so on
    - rw - seems to be read\write  
    - remount - pretty clear at first glance  
  - -v - verbose  
  - -vvvv  - very verbose     
  - / - what to remount
  - -a  - mounts ALL from /etc/fstab

  __NOTE: `none` mountpoint is in fact ignored in `fstab` if FileSystem type is `swap`__

  Example:
  ```
  #view all devices:
   lsblk -f  
  #mount device
   mount /dev/sda5 /path/to/mount/point 
  #unmount (Notice N letter absent in commnad name)
   umount /dev/sda5  
  ```

  Example:
  ```
  #mount ISO image
   mount -t iso9660 -o loop image.iso /path/to/mountpoint
  ```

  `/etc/fstab` - config for filesystems
   read by `fsck`, `mount`, `unmount` at boot
  lines from this file read during `mount -a`.  
  > check `man fstab` for details.

  NOTE:
  `swap` partitions are not mounted since don't have mounpoint in `/etc/fstab` - it is `none`   



## ntfs-config:
  Enable/disable write support for any NTFS devices

## fdisk:
  manipulates disk partitions tables
   Usage:
  fdisk <block device name>
   Commands:
  - m - manual/help
  - p - show all partitions
  - n - New partition (`Primary` as main Physical partitions, and `Extended` as logical from primary ones)
    - p - Primary partition - only 4 could exist, then one of 4 could be devided in Extended partition
    - e - Extended partition - sub-partition of Primary partition.
  - l - list all known filesystems (use one in mkfs )  
   Example:  
  ```
  fdisk /dev/sda
  > m 	- see what could do
  > p 	- see all partitions of the device 
  ```

## mkfs:
  create new filesystem
   Params:
  -t fylesystem type
   Example:
  mkfs -t ext3 /dev/sdb1

## lvm:
 Logical Volume Manager
  [How to install](https://help.ubuntu.com/community/UbuntuDesktopLVM)
  [Ubuntu docs](https://wiki.ubuntu.com/Lvm)
  [How to extend Logical volume](https://www.youtube.com/watch?v=hugEkh50Ynk)
  BEST  - [How to extend, manage, Snapshots etc.](https://www.howtogeek.com/howto/40702/how-to-manage-and-use-lvm-logical-volume-management-in-ubuntu/)

### Add new HDD to PC:
[TEXT: fdisk /dev/sdX](https://www.crucial.com.au/blog/2009/11/18/how-to-create-a-new-partition-on-a-linux-server/)

### Add new HDD to VM (virtualbox)
Find the device:
```
# List all devices like sda, sdb, sdc etc.
fdisk -l /dev/sd*
```
Create new partition:
- `fdisk /dev/sdc`  - Open particular Disk found by udevd
- `n` - create New partition
- `p` - make it Primary partition
- `1` - make it first partition (sdc1)
- `Enter` - for default First cylinder
- `Enter` - for default Last cylinder (so whole /dev/sdc would be 1 partition)

Change partition type:
Set type to be LVM, so lvm could utilize the disk
- `t` - change Type of partition
- `8e` - Linux LVM type
- `p` - verify Partition changes
- `w` - WRITE to disk, this will be Permanent. Fdisk will exit into shell afterwards

Notify the OS about the changes:
`partprobe`

Create LVM Physical volume:
- `pvcreate /dev/sdc1` - create new physical volume from newly formatted disk partition
- `pvs` - list existing PVs and to which Volume Group they belong

Extend Volume Group:
- `vgextend centos /dev/sdc1` - extend VG with name `centos` using newly added PV
- `vgs` - display volume groups, sizes and Free space (to check `extents` run `vgdisplay`)

Extend Logical Volume:
- `vgdisplay` - note number from `Free  PE / Size` - which is the number of `extents` 
- `lvdisplay` - note LV Path of the volume to extend from extended VG 
- `lvextend -l+2047 /dev/centos/root` - 2047 here is `extents` followed by LV path

Format disk volume:  
- `df -h` - note  Mountpoint path or Filesystem (mounted disk like /dev/sda1 if no LVM used)
for `ext4` use:
- `resize2fs /dev/mapper/centos-root` - where last argument is path to Mountpoint 
for `xfs` use:
- `xfs_growfs /dev/mapper/centos-root` 
  - use `-n` param to Dry Run (barely readable..)
- `df -h` - to recheck


## fsck:
  filesystem check and repair (lost+found dir will have repaired files)
   Example:
  fsck /dev/sdb1

## genisoimage:
  creates ISO images
   Parameters:
  -o <file.iso> - output to
  -R - for long file names and access rights for files in POSIX style
  -J - similar long names but for Windows
   Example:
  genisoimage -o cd-rom.iso -R -J ~/cd-rom-files
   where cd-rom-files is a directory with files to be added into ISO image
  

## scp:
  secure copy using ssh protocol
   could copy to or copy from remote location.
  copy to remote:
  ```
  scp <params> <file_path> <receiving_username>@<address>:<receiving_file_path>
  ```
  copy from remote:
  ```
  scp <params> <remote_username>@<address(ip\name)>:<remote_local_path> <path>
  ```
  params:
  -v - verbose  

## sshd(openssh):
  daemon config location:  
  `/etc/ssh/sshd_config`  
   - sftp enable switch is located there, could be commented out and sshd restart to disable it  
   - LogLevel DEBUG3  
     - Enables detailed logs useful when got ssh logon issues  
  
### Options important:
 __PermitRootLogin:__   
  - “yes”,
  - “prohibit-password”
  - “without-password”
  - “forced-commands-only”
  - “no”
  
  RSA keys for ssh authentication generation process:
  https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys-on-centos7  
  Passwordless:  
  to add security ssh login with Password could be disabled, to login
  only with keypair:  
  https://linuxize.com/post/how-to-setup-passwordless-ssh-login/#disabling-ssh-password-authentication
  https://help.ubuntu.com/community/SSH/OpenSSH/Keys
   Log files of sshd:  
  RHEL:  
   `/var/log/secure`
    Example:
  ```
   tail -f -n 500 /var/log/secure | grep 'sshd'
    If getting errors like:
   'ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318'
    the Root login is disabled
   PermitRootLogin yes
  ```
  Other:
  ```
    /var/log/auth.log
   tail -f -n 500 /var/log/auth.log | grep 'sshd'
  ```

## ssh:
  remote connect via ssh protocol  
  can also copy files  

  sshD Generate SDA or DSA Key pair upon install\first start  
  public key goes to client's machine `known_hosts`  
  private is used to encrypt messaging
  User's keys are also used to authenticate remote logon with ssh client
   
  -v - display debug info level1
  -vv  - debug level2
  -vvv - degut level3

  Can also copy files:
  ```
  tar zcvf - dir | ssh username@host zxvf -
  ``` 
  
  __COnfig:__
  - sshd: /etc/ssh/sshd_config  
  - ssh client: /etc/ssh/ssh_config 

__Troubleshooting:__
   if -vvv  shows everything ok:
     debug1: Server accepts key: pkalg ssh-rsa blen 535
     debug2: input_userauth_pk_ok: SHA1 fp 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48
     debug3: sign_and_send_pubkey: RSA 28:bc:1b:64:94:3a:6b:ec:b4:13:45:ca:79:80:be:7c:75:cc:82:48
     debug1: read PEM private key done: type RSA
     debug3: Wrote 1156 bytes for a total of 3101
   Open Log files of sshd:
    tail -f -n 500 /var/log/secure | grep 'sshd'
   if errors like this appear:
    ROOT LOGIN REFUSED FROM 10.253.0.28 port 34318
   open  /etc/ssh/ssh_config and set
  PermitRootLogin yes

## ssh-keygen:
  generates keypair private\public
    Example:
   ssh-keygen -t rsa -b 4096 -C "email" -f my_key
    Example PUBLIC from Private:
   ssh-keygen -y -f private_key > private_key.pub
  -t rsa - key type
  -b 4096 - generates key pair into given path
  -C "text" - comment for key
  -f filename of the key, instead of default 'id_rsa'
   Example:
  ssh-keygen -t rsa -b 4096 -C "email" -f my_key
      - will create new keypair, w/o -f it will generate
	in the path ~/.ssh/id_rsa and its id_rsa.pub part
  -R - remove from ~/.ssh/known_hosts
    Example:
   There are only 1 RSA key is allowed for the IP, in case RSA key is changed, then 
   old one need to be removed, otherwise it will not allow ssh acces:
    ssh-keygen -f "/home/dos/.ssh/known_hosts" -R 172.17.0.2

  -l - list, not create anything.
  -E - Specifies the hash algorithm used when displaying key fingerprints.  
       Valid options are: “md5” and “sha256”.
    Example:
 Get fingerprint:
  ssh-keygen -lf ~/.ssh/keyname[.pub]
 ### GitHub: Get fingerprint in hashed\hex format:
 ```
  ssh-keygen -E md5 -lf ~/.ssh/git_key[.pub]  
 ```

 #### GitHub troubleshoot:  
 verbose connect to github with `git` user  
 `ssh -vT git@github.com`  
 Expected output:  
 ```
debug1: Connecting to github.com [140.82.121.4] port 22.
debug1: Connection established.
debug1: identity file /home/dos/.ssh/id_rsa type 0
...
debug1: Offering public key: RSA SHA256:JZ.......On8 /home/dos/.ssh/id_rsa
debug1: Authentications that can continue: publickey
debug1: Trying private key: /home/dos/.ssh/id_dsa
debug1: Trying private key: /home/dos/.ssh/id_ecdsa
debug1: Trying private key: /home/dos/.ssh/id_ed25519
debug1: No more authentication methods to try.
```
`id_rsa` is not the key GitHub knows.
Check which keys `ssh-agent` knows - it is auth tool used.
```
#start the agent
 eval "$(ssh-agent -s)"
#list md5 of all known keys
 ssh-add -l -E md5
```
> only returned keys are used for auth  
Add key if not listed:  
```
#add key
 ssh-add ~/.ssh/other_key_rsa
#recheck
 ssh-add -l -E md5
```

## ssh-copy-id:
  copies public keys securely to remote machine for paswordless login
  keys are concatenated into ~/.ssh/authorized_keys file
  it will copy all public keys available in ~/.ssh/
   Example:
  ssh-copy-id dmitry2@192.168.42.74
   if there issues with a default 22 port , another port could be changed:
  ssh-copy-id "<username>@<host> -p <port_nr>"
  
  
  if ssh-copy-id is not available here is analoug:
   cat ~/.ssh/id_rsa.pub | ssh remote_username@server_ip_address "mkdir -p ~/.ssh && chmod 700 ~/.ssh && cat >> ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys"
   

## ssh-keygen:
  Known hosts are stored in a file /home/<username>/.ssh/knonwn_hosts.
    Generate public key from private key:
  ssh-keygen -y -f ~/.ssh/id_rsa > ~/.ssh/id_rsa.pub
  
  

## sftp:
  secure file transfer protocol, allows to list dir contents
  built-in into OpenSSH server, which workks as sshd service(daemon) together
  with scp
  sftp <remote_username>@<remote_ipaddress_or_name>
 sftp has its own set of directory related commands:
 pwd - print work dir, returns current remote_dir
 lpwd - current local_dir
 cd - change dir, changes remote_dir
 lcd - local change dir, changes local_dir from sftp via OpenSSH daemon
 get - copy file to current local_dir
  get <remote_filename> <new_local_name>
 ls also works, ALIASES from bashrc do not work

## f5fpc:
 F5 Linux CLI (command line interface ) Edge Client Installer
 work machine: D:\devops\BigIP
 home machine: somewhere in 'vpn'
 Install:
 1. chmod +x Install.sh
 2. bash Install.sh
 Aliases:
 alias vpn-go="sudo f5fpc -s -t seapop.tideworks.com -u 'usa\twuser' -x"
 alias vpn-info="sudo f5fpc -i"
 alias vpn-stop="sudo f5fpc -o"


## shutdown:
 shutdown or reboot or halt etc the computer
  by default shutdowns in 1 minute, also could be scheduled:
   Example:
  shutdown "21:45" - will shutdown at 21:45
  shutdown "+25" - will shutdown in 25 minutes from 'now'
  shutdown - will shutdown as 'shutdown +1'
  shutdown now - shutdown now

## at:
 need to be installed, daemon  
 schedules to do something _at_ some time  
 Example:
 ` 
 There are three ways to use it:  
  ### Example 1:  
execute what echo will return	at time of now + 1 minute - echo will return stuff in ''  
``` 
echo "shutdown" | at now + 1 min
```
  ### Example 2:  
redirect STDIN from cmd.txt , will read	from there, and shutdown 
```
 echo "shutdown" > cmd.txt
 at now + 1 min < cmd.txt 
```
  ### Example 3:  
wait for input in STDIN
```
 at 22:30
 at> shutdown
 ```
 `CTRL+D` to exit, and wait given time before execute shutdown

  Time could be like:  
 `[[CC]YY]MMDDhhmm[.ss]`  
  Example: 
 ```
 at -t 201403142134.12 < script.sh
 ```
 ```
 # September 30 2015 at 22:30
 at 22:30 30.09.15
 ```
 
## crontab:
  list cron schedules;
  System crontab location `/etc/crontab`
  usage:	

  `crontab [-u user] file`  
	`crontab [ -u user ] [ -i ] { -e | -l | -r }`  

- `-u` list all rules for a user:  
`crontab -u <username> -l`  
- `-e` edit current user crontab file
- `-r` remove current crontab
- `-i` as `-r` but with confirmation
- `-u username file.txt` install crontab file from file.txt
 
### Crontab file syntax
- System crontab `/etc/crontab` and `/etc/crontab.d` files can set UID to execute cron job  
- Cron job STDOUT and STDERR could be redirected to file or null or email  

Example:  
run every day at 6:42 as a `superuser`; `stdout` and `stderr` to `null`:
```
42 6 * * * root /usr/local/bin/cleansystem > /dev/null 2>&1
```

see [cron](#cron) for detailed syntax
    

## cron:
 task scheduler
 ```

 *     *     *     *     *  command >> stdouterr.txt (or /dev/null)
 -     -     -     -     -
 |     |     |     |     |
 |     |     |     |     +----- day of week (0 - 6) (Sunday=0)
 |     |     |     +------- month (1 - 12)
 |     |     +--------- day of month (1 - 31)
 |     +----------- hour (0 - 23)
 +------------- min (0 - 59)
 ```


 So, for example, this will run ls every day at 14:04:
 
 `04 14 * * * ls`
 
 To set up a cronjob for a specific date:
 
  - Create a new crontab by running crontab -e. This will bring up a window of your favorite text editor. 
  - Add this line to the file that just opened. This particular example will run at 14:34 on the 15th of March every 5th year if that day is a Friday (so, OK, it might run more than once):  
 `34 14 15 5  /path/to/command`
  - Save the file and exit the editor.
 
## bc:
seems to be a Basic Calculator  
weird  
Use Case Example:
```
#Calculate binary of 192
$> bc
> obase=2; 192
11000000
```

## awk:
 stuff to worh with text, programming language kind of
  Example: 
 awk 'BEGIN {printf "title"} {print "line"} END {printf "end"}' textfile.txt
  this will print 'title'
  then 'line' for every line in textfile
  and print 'end' at the end

 check out tutorial here:
 https://www.tutorialspoint.com/awk/awk_basic_syntax.htm

 workflow is Read, Execute, Repeat with tear up and tear down executed once:
 BEGIN
  Read line
  Execute user code
  Repeat if not EOF
 END

 Begin block executes only once, Tear Up, syntax:
  BEGIN {awk-commands}
 Optional
 
 Body block, which is actual stuff to do for every line matched by pattern:
  /pattern/ {awk-commands}
 Pattern is optional

 End block executes only once, Tear Down, syntax:
  END {awk-commands}
 Optional

 Also gets input from a file:
  -f <file_path>
    Example:
  command.awk:
   BEGIN {printf "start\n"}
   {print}
   END {printf "end\n"}

  awk -f command.awk textfile.txt   - this will read from command.awk and print
   'start' at 1st line
   then print every line
   then print 'end' and the latest line
 
 
===general


===Env Vars / Environment variables:
## declare:
  built-in command to assign or display variables and functions
   Syntax:
  declare [-aAfFgilnrtux] [-p] [name[=value] ...]
  Params:
  Using `+' instead of `-' turns off the given attribute.
   Example:
  declare - will show all variables and functions
  declare | grep "^a=" - will return variable 'a' with its value 
   
  -p - prints only variables w/o functions (displayhs onlyh NAME)
  -a - to make NAMEs indexed arrays(only Bash, not Sh)
   Example:
   
  -i - to make NAMEs integer
   Example:
   declare -i z=asdf    < echo $z - 0
   declare -i z=12	< echo $z - 12
  -l - to convert NAMEs to lower case on assignment
   Example:
   declare -l b=TEST
   echo $b 		>test  - will save it in loweracase
  -u - same but for UPPRECASE
  -g - create global variables in functions:
  When used in a function, `declare' makes NAMEs local, as with the `local'
    command.  The `-g' option suppresses this behavior.

  -x - export variables
   Example:
   declare -x FOO  - same as - export FOO
  -r - readonly variables
  -n - make NAME a reference to the variable named by its value
   Example: pointer-to-pointer
   b=foo		# variable
   declare -n b2=b	# pointer to the variable
   echo $b2
   >foo
   declare -t b3=b2	# pointer to pointer
   echo $b3
   >foo

## printenv: env:
  very similar, prints environment variables to STDOUT
  printenv VAR - prints value of specified variable to STDOUT

  Useful Env variables:
  EDITOR - default text editor
  SHELL - name of the shell
  HOME - home path
  LANG - chars and sort order for language
  OLD_PWD - prev work dir
  PWD - current working dir
  PAGER - program for page view ( i.e. /usr/bin/less)
  PS1 - current prompt string value
	Example:   dos:~$: / root:~#:
	Prompt string could be drastically changed
	even including redraw of clocks in the cmd on every command entered
        Example: \[\033[s\033[0;0H\033[0;41m\033[K\033[1;33m\t\033[0m\033[u\]<>$
    Example:
   export PS1='$(whoami)@$(hostname):$(pwd) '$
  TERM - type of the terminal
  TZ - time zone, usually *nix has Coordinated Universal Time , and it
	is corrected with current time zone upon display
  USER - username
  HISTCONTROL - with =ignoredups - will ignore duplicates in bash history
  HISTSIZE - changes default 500 value of history line-length

## set:
  prints ALL variables - Shell vars, local vars, shell functions
  Builtin so use 'help set'


 Read more of set and printenv\env here:
 https://www.digitalocean.com/community/tutorials/how-to-read-and-set-environmental-and-shell-variables-on-a-linux-vps
 Read more of printenv\env history here:
 https://unix.stackexchange.com/questions/123473/what-is-the-difference-between-env-and-printenv

How to set\export env variables in bash:
  export - share variable for child processes started from current shell  
   Example:
  TEST=1 - will create var for current shell, will not be visible in processes
	started from the shell
  export TEST=1 - will create var for cur shell, and share it to processes
	started from the shell
  echo 'export TEST=1' >> ~/.bashrc - will create var at every non-login start 
	of the shell, if .profile reads .bashrc - login shell also get it
	 Usually for vars there are different file, which bashrc will read
	see/update local .bashrc to do so.
 	 Just exported into bashrc vars will not be visible until shell restart
	or source load:
	 source ~/.bashrc - will add newly exported vars into current session

Read export\env differences here:
 http://hackjutsu.com/2016/08/04/Difference%20between%20set,%20export%20and%20env%20in%20bash/

  Set variable ONLY for current shell:
  varname="my value"
  Set variable for current shell and ALL PROCESSES started from current shell:
  export varname="my value"
  For below LogOut is required:
  Set var PERMANENTLY for current user
  ~/.bashrc - add 'export varname="my val"' here
  Set var permanently and SYSTEM WIDE (all users/all processes)
  /etc/environment - add line 'VARNAME="my value"', caps bcs of naming convents
		     'export' is not userd here parser does not know it
  LogOut is required for permanent changes.
  
  Unset:
  unset ENV_VAR
  echo $ENV_VAR - returns nothing , bcs it is unset

  ALSO has options:
  set -o - will display all the options
   one of the options is 'noclobber'
   set -o noclobber - prevents '>' redirection from overriding files
   set +o noclobber - removes noclobber back


## execute: source: dot (.):
  read more:
  https://superuser.com/questions/176783/what-is-the-difference-between-executing-a-bash-script-vs-sourcing-it/176788#176788

  execute script if it has x permission
  !! CREATES NEW SHELL !!
   ./script - script in current directory(./ dots lash notaion)
   script - execute script if it is in PATH var

  source script even if it does not have x permission
  documentation:
  http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#dot
  !! DOES NOT CREATE A NEW SHELL !!
  Executes commands from a file in the CURRENT environment. 
  In case it is for login-shell, it will update whole command line, probably
   not sure how it will work in case of 'su -'

   source myscript - myscript is param for source so no matter myscript is in
 		     path or not.
		     Still need to be valid shell script(#!/interpreter/pat)  
   . myscript - 'source' is alias for . in bash, syntax from POSIX 

  Source saves stuff changed in script(i.e. Env var updates) to the current 
  shell session where as Execute creates new session where changes are saved
  and kills it after everything is done - basic behaviour of everything


## Login process
 Read More:
 http://mywiki.wooledge.org/DotFiles

No GUI login into text console. Local login shell:
1. getty(8) starts and produces 'login:' prompt
2. getty reads login and passes it to login(1)
3. login reads password db and decides whether to ask for password
4. after password provided PAM could be loaded - /etc/pam.d/login
   this could load additional settings for environment or session etc.
5. login 'execs' login shell(probably from /etc/passwd) with -
   -/bin/bash - means "this is login shell, not a regular shell"
6. since it's login shell it reads /etc/profile and /etc/profile.d
   /etc/profile should have code to read profile.d
   then ~/.bash_profile, 
   then if no - .bash_login, 
   then if no - .profile
     .profile is standard Bourne/POSIX/Korn shell config file
7. bash stops look for config files and displays prompt

 Example of login process:
Let's take a moment to review. A system administrator has set up a Debian system (which is Linux-based and uses PAM) and has a locale setting of LANG=en_US in /etc/environment. A local user named pierre prefers to use the fr_CA locale instead, so he puts export LANG=fr_CA in his .bash_profile. He also puts source ~/.bashrc in that same file, and then puts set +o histexpand in his .bashrc. Now he logs in to the Debian system by sitting at the console. login(1) (via PAM) reads /etc/environment and puts LANG=en_US in the environment. Then login "execs" bash, which reads /etc/profile and .bash_profile. The export command in .bash_profile causes the environment variable LANG to be changed from en_US to fr_CA. Finally, the source command causes bash to read .bashrc, so that the set +o histexpand command is executed for this shell. After all this, pierre gets his prompt and is ready to type commands interactively. 

.bashrc is not read for login shells, it could to be read manually in .profile
In case this is .profile, before read it, need to ensure that Bash is used:
  if  [ -n $BASH ] && [ -r ~/.bashrc ]; then
    . ~/.bashrc
  fi

### Remote Login Shell:
alsmost same as previous, but except 'getty' and 'login' - 'sshd' handles
greetings(login prompt by 'getty') and password auth('login').
PAM also linked with 'sshd', but will read /etc/pam.d/ssh (not .../login).
 Once sshd has run through the PAM steps (if applicable to your system), it 
"execs" bash as a login shell, which causes it to read /etc/profile and then 
one of .bash_profile or .bash_login or .profile.

 NOTE:
during ssh login, some of local Env Vars could be sent to remote sshd
like LANG and LC_* variables want to be preserved by the remote login.
Unfortunately, the configuration files on the server may override them.


## User\Groups Management

### User\Groups Management General info
group related info is stored in
  /etc/group   - file, use 'vigr' for edit
user related info stored in 
  /etc/passwd  - file. in case of edit use 'vipw' like 'visudo' with sudoers
default homedir files are stored in
  /etc/skel/   - directory
		 those files will be copied to user home when it is created
  ..../.bashrc - executed by bash for non-login shells - every time bash
		 is started interactively(from command line, which is also bash)
see more: https://unix.stackexchange.com/questions/129143/what-is-the-purpose-of-bashrc-and-how-does-it-work
  ..../.profile - executed by command interpreter for login shells
		  is not read if '.bash_login' or '.bash_profile' exists
		Usually also reads /etc/bash.bashrc
  ~/.profile - for login-shells also reads ~/.bashrc
  ..../.bash_logout - executed by bash when login shell ended


unlike files in /etc/skel changes to this files will affect even existing users
changes from here will be applied with every re-login to every
  user on thesystem:
/etc/bash.bashrc -  will affect ~/.bashrc 
/etc/profile -  will affect ~/.profile

/etc/login.defs - useradd/userdel/usermod config
/etc/adduser.conf - adduser config(Ubuntu)
/etc/shadow - secure account info - passwords
/etc/gshadow - secure group account info - passwords

/etc/passwd:
root:x:0:0:root:/root:/bin/bash
root - username
x - placeholder for a password, modern systems use /etc/shadow file for pass
0 - is the user ID(UID) for this user
0 - is the group ID(GID) for this user
root - comment about this user
/root - home directory for this user
/bin/bash - default shell for this user(after he logs in presumably)
	    Possible values:
  /bin/nologin - deny login at all
  /bin/false - deny login but still can be logged in using 'su' command from 
		another account


/etc/group:
wheel:x:10:centos,user
wheel - group name
x - place holder for a password, modern systems use /etc/shadow filr for pass
10 - group id(GID) for the group
centos,user - users in the group, and probably user wheel too
   user along with who group was created id not listed in the group..
   probably to make sure group do or does not contains it , could try check
   groups of that user like 'groups wheel' - it will return 'no such user' if
   there is no user with same name as a group

NOTE: there are Primary group for a user and SUpplementary groups, 
  Primary is the first group of a user, others are suppplementary:
    $ groups johnny
    > johnny : test1 john newgroup1 newgroup2
  to add extra supplementary groups use 'usermod -a -G group1,group2'. 
  To change Primary group use  'usermod -g john johnny'
    > johnny : john newgroup1 newgroup2
  NOTE THAT 'test1' group has been removed at all, and not moved to 
   supplementary groups
  NOTE to change to test1 w/o deleting it user 'newgrp' command:
    $ newgrp newgroup1
    > newgroup1 john newgroup2
  Files created by user are owned by PRIMARY group of the user

# Manipulate Users And Groups
## useradd:
  old since any *nix creation. Almost everything need to be done manually bcs of
  compatibility - different *nix platforms handles users differently, their ~
  directories could be different
  useradd -<params> <username>
  -d <~_path> - use if home name\path differs from default location, i.e.:
		 
	useradd -d /home/accounts/john testuser
  useradd -c "John from Accounts" -m -s /bin/bash john
   -c - adds a COMMENT in the /etc/passwd for this acc
   -m - MAKEs the home directory for this user, like /home/john
   -s - assigns the SHELL for the user
   john - actual user name
    user 'john' is the member of group 'john' and this is his PRIMARY group
  also:
   -u <UID> - user ID , first free picked (from range in config) id no specified
   -g <GID\Name> - assign to already existing group. i.e. -g accounts
   -G <GID\Name> - additional group. i.e. -G employees
   -e <YYY-MM-DD> - EXPIRATION date of the account
   -k </path> - sKELETON directory if differs from /etc/skel
   -p <hashed_pwd> - encrypted password for acc, or use 'passwd' command later
   
NOTE: after user is created the password need to be added:
passwd testuser

## Manual User Creation
user# sudo su -		- become root
root# vipw		- edit /etc/passwd
 add new line with user info like:
 username:x:UID:GID:comment:/home/username:/bin/bash
root# vigr		- edit /etc/group
 add new line with user group info like:
 usergroup:x:GID:
root# cd /home && mkdir username - create user's home folder
root# cp -rf /etc/skel/\.* /home/username/. - copy everything from skel
root# chown -R username:usergroup username/ - change recurse ownership of dir
root# passwd username - create password for the user
=====manual user creation end

## id:
  returns info of current user - id, main group id, other groups, etc.

## groups:
  list groups of a user
  groups [username]
    w/o params gives current user groups
    with username - gives groups of that user

## getent:
  getent group <$(whoami)>
  lists all groups on system and users of those groups(similar to /etc/group)

## usermod:
  modify existing users
  usermod [<params>] [groups] <user>
   -l - change LOGIN of the user. i.e.:
      usermod -l johnny john - change username john to johnny
   -d - DIRECTORY path for New home directory of the user
   -m - MOVE-home dir to new DIRECTORY path
      usermod -m -d /home/johnny johnny - creates new dir for the user and moves
	everything from old dir to new dir
   -g - change primary GROUP
   -G - add to other GROUPs delimited , and no spaces adds, more groups for user
	will overwrite other supplementary groups except primary
   -a - APPEND groups, used with -G, to append existing supplementary groups:
   Example:
      usermod -a -G newgroup1,newgroup2 johnny - will add two groups to the
	end of the list of a groups user johnny have
   -L - LOCK user
	after lock there will be exclamation mark in /etc/shadow file before
	user passwod:
	johnny:!$6$DQMYnvhr$xKMYZSorH2wePlAunWDBKYWYSK8bmnyKMbr9IAuMoykPl7....
   -U - UNlock user, and remove exclamation mark '!' before user passwd hash
  More to read at:
   https://www.tecmint.com/usermod-command-examples/

## userdel:
  deletes user in old way
  userdel <username>
  -r - deletes home directory too

## adduser:
  adding user
  shell around perl written useradd command, which is hard to use
   adduser <username>
  CentOS7, does not have adduser, it links it to useradd:
   lrwxrwxrwx. 1 root root 7 гру 14  2016 /usr/sbin/adduser -> useradd

  /etc/adduser.conf - config for this command, has default ids, default shell
			default home dir(/home) and so on
    detailed how adduser works on Ubuntu:
    https://askubuntu.com/questions/659953/what-is-ubuntus-automatic-uid-generation-behavior

## groupadd:
  adds group
  info about groups is stored in /etc/group file, see General above for details 
   once group is created use 'usermod' to add users to it
  group [options] <group>
  -g - manually set GROUP id

## chgrp: 
  change group, changes group of file or directory
   Synopsys:
  chgrp [param] <group> /path/to/file
   Params:
  -R - recursive
  -c - like verbose but only when change is made
  -v - verbose

## groupmod:
  modify existing groups
  -g - change GROUP id
    groupmod -g 300 manager - change group 'manager' to have new GID of 300
  -n - change NAME of a group
    groupmod -n managers manager - change group 'manager' to 'managers'

  
## passwd:  
  ![Strength](img/password_strength.jpg)
  change password for current user or for given user  
  `passwd [username]`  
  located in 
  - `/usr/bin/passwd` - ubuntu  
  - `/bin/passwd` - centos/red hat  

  has `setuid` bit which allows unprivileged users to use 'root' owned files.  
  I.e - `passwd`, `sudo`, `su`  

### check for setuid:
```
ls -la `which passwd`
-rwsr-xr-x 1 root root 59640 Mar 22  2019 /usr/bin/passwd*
```
The `s` implies that the executable bit (`x`) is set, otherwise you would see a capital `S`. This happens when the `setuid` or `setgid` bits are set, but the executable bit is not, showing the user an inconsistency: the `setuid` and `setgit` bits have no effect if the executable bit is not set.   
The `setuid` bit has no effect on directories. 
_Setuid_: see [setuid](#setuid:-setgid:) for details


## gpasswd:
  add\remove users from a group, set admins for the group, set password for
  a group
  gpasswd <params> <group>
  -a <user>  - ADD user to a group
  -M <user1,user2> - add MULTIPLE users, commadelimited w/o spaces
     gpasswd -M john,jane manager
  -d <user> - DELETE user from a group
     sudo gpasswd -d johnny newgroup1 - remove 'johnny' from 'newgroup1'
  -A <user> - add ADMIN user for a group, dunno what it is about.. TODO

## newgrp:
  newgrp <groupname>
  Changes primary group of the user w/o deleting any groups
   could freely move between assigned groups 
  And asks for password(probably group password) if trying to set unassigned
  group as primary one

## groupdel:
  delete group
  groupdel [params] <groupname>

## whoami:
 returns username of currently logged user

# Super User
/etc/sudoers - file where all the privileges set. has user accounts with 
		privileges, as well as some groups, 
		for Ubuntu: 
			admin - almost root
			sudo - as root
		for CentOS7: 
			wheel - as root
NOTE: this file must be edited through 'visudo' command, this will ensure safe
  changes including lock file on edit and so on

## root access:
  should be disabled for remote login in sshd for security reasons

## su:
  su - [<username>] - creates new session for a user, i.e.:
     su - user - log in under 'user' user
  log in as super user(root), need to provide root password i.e.: 
  su -
   - dash is used to call login shell and reset most of env vars, basically safe
     against env related exploits and overriden standard commands. see here:
  https://unix.stackexchange.com/questions/7013/why-do-we-use-su-and-not-just-su
  
  !!!!
  in case root pasword is lost here is how to recover it from recovery mode
  https://askubuntu.com/questions/24006/how-do-i-reset-a-lost-administrative-password
  UBUNTU by default installs with random root password so noone knows it,
   need to use SUDO 
  
  Another way to recover a root password is to add a user to a privileged group
   like 'sudo' or 'wheel' - this way user, using program 'sudo' can become root
   w/o entering the forgotten(random for Ubuntu) password:
  user# groups - make sure user is in sudo group
  user# sudo su -  - become root w/o entering password root's password
  root# passwd  - enter new password

sudo
  TODO
=====super user end



# Processes and Services Management

## Memory:
Kernel , in order to help CPU's Memory Management Unit(MMU), breaks memory into _pages_.  
Pages real addresses are stored in the _Page Table_.  
MMU translates virtual memory addresses into real ones based on Page Table.  

---
Process does not need all the pages to be loaded in order to run. So not all pages are loaded for a process, only the required ones:  
_On-Demand paging_ - is allocation of new pages for a process when it requires ones (additional code r
equired etc)

---
Demand paging steps:
- program's code is loaded into memory pages
- working-memory pages could be allocated by the Kernel
- when next instruction to run is not in the loaded memory, Kernel pauses execution, loads that code into newly allocated pages and continues execution
- when more working-memory pages are required Kernel will allocate(or free older ones - see OOM Killer) and assigns them to process

### Page Faults
If process accesses memory page and it is not ready process would trigger _page fault_. The kernel will take over control of CPU from the process in order to get page ready.

Could be viewed by `time`, `ps` or `top` commands:
Example:
```
$ /usr/bin/time cat > /dev/null
0.00user 0.00system 0:03.15elapsed 0%CPU (0avgtext+0avgdata 2076maxresident)k
72inputs+0outputs (1major+75minor)pagefaults 0swaps
```
`cat` is loaded first time and got 1 major fault  
Example:
```
$ top
f            # to get into __FIELDS MANAGEMENT__
<Up>/<Down>  # to find `nMin`, `nMaj` fields for Page Faults
Right        # to select
<Up>/<Down>  # to move the field next to other `*` selected fields
<Left>       # to unselect field after it was moved
<Space>      # to mark field for display
q            # to exit FIELDS MANAGEMENT
PID USER      PR    VIRT    RES    SHR S  %CPU %MEM     TIME+ nMaj nMin COMMAND 
684 systemd+  20   71856   6448   5132 S   0.0  0.2   0:03.63    5  852 systemd-resolve         
685 systemd+  20  146136   3192   2672 S   0.0  0.1   0:00.07    3  575 systemd-timesyn  
```
Example:
```
$ ps -o pid,min_flt,maj_flt 684
  PID  MINFL  MAJFL
  684    852      5
```
Min and Maj page faults for process with PID 684

#### Minor Page Fault
Occur when:
- new memory was allocated and MMU does not yet know of the new _page_ allocated  
- MMU does not have more space to store existing in memory pages addresses

Kernel in such cases would interfere(getting _page fault_ event) and tell about new page to MMU 

#### Major Page Fault
Occur when required memory page is not in main Memory(physical one). Kernel need to load it from disk or other slow-storage first.  
This could happen when program is run first time so many code not in the memory yet.  
The worst case is when low on memory - thus before loading new mages from the disk Kernel will store unused existing ones into _swap_ which is also on the slow storage(i.e. disk).

## Load Averages
average number of processes in ready-to-run (including running) state in a given time - thus using CPU
on System with 1 Core Load Average of 1 will mean that at given time 1 process was running all the time  
on System with Multiple Cores the Load Average of 1 would mean that only 1 Core was running process constantly, 2 - two Cores  
Load Average of 0.01 would mean that in Given time only 1% of 1 Core of CPU was used

### Load Average is High:
- Many processes are running simultaneously, this could be 'by design' so no worries
- Web Servers could spawn lots of short lived processes, so Load Average could not be correctly calculated
- Low Memory - kernel could thrash or relocate memory To and From the Disk. 
  Process could be in ready-to-run state - contribute to Load Average, but physically could not be run because of Memory is unavailable

## free:
  shows System's Memory status.  
  Memory info could be found in `/proc/meminfo` too  
  if `Swap` row is 0B - there is no Swap enabled (see `swapon`)
### Params:
- -h - human readable

## Create Swap File:
https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04  
```
# Check swap enabled:
sudo swapon --show
# Double check for 0B in Swap row:
free -h
# Check disks (i.e. /) has enough space (4G for swap enough for HDD for RAM fallback)
df -h
# Create swap file and verify size
sudo fallocate -l 1G /swapfile
ls -lh /swapfile
# Set readonly for Root user (security reasons)
sudo chmod 600 /swapfile
# Make the file to be actual swap
sudo mkswap /swapfile
# Enable the swap into the file
sudo swapon /swapfile
# Check swap changed
sudo swapon --show
# Permanent changes:
sudo cp /etc/fstab /etc/fstab.bak
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

### Swappiness
percentage at which Kernel would use Swap over RAM  
0 - 100  
values closer to 0 - Kernel won't swap unless absolutely neccessary  
values closer to 100 - Kernel will try swap more to keep more RAM free  
Default: __60__  
Use Case Example:
```
# Check swappiness
cat /proc/sys/vm/swappiness

# Change temporarily until reboot
sudo sysctl vm.swappiness=10

# Change permanently
echo "vm.swappiness=10" >> /etc/sysctl.conf
```

### Cache Pressure
percentage at which Kernel would swap __inodes__ and __dentry__  
It is Costly to look for and very frequently needed to look - good to cache.  
0 - 100+   
at 100 - the kernel will attempt to
reclaim dentries and inodes at a "fair" rate with respect to pagecache and
swapcache reclaim  
at 0 the kernel will
never reclaim dentries and inodes due to memory pressure and this can easily
lead to out-of-memory conditions  
```
# Check
cat /proc/sys/vm/vfs_cache_pressure

# Change
sudo sysctl vm.vfs_cache_pressure=50

# Change permanently
echo "vm.vfs_cache_pressure=50" >> /etc/sysctl.conf
```

## swapon:
turns swap on.  
Swap could be either on separate special partition or just a file.  
Use Case Example:
```
$ sudo swapon --show
NAME      TYPE       SIZE USED PRIO
/dev/sda9 partition 15.9G 780K   -2

```
__Params:__
- --show - shows whether swap is on, and where. __No output__ means there is no swap enabled.


## vmstat:
Virtual Memory STATistics  
shows statistis for the system about Resources usage

First line is average for whole `uptime` of the system. 
Second line and below (shows up each 2 seconds) display stats for the time period from param(2 seconds in the example)  

__Params:__  
- -S k\K\m\M - kilobytes or megabytes (lowercase rounded to 1000)
- -a - active\inactive memory - displayed instead of buff\cache memory
- -d - disk statistics  
Example:
```
$ vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  2 219232 461404 159344 988640    2   14   178   120 1066 1420 23  9 66  1  0
 0  0 219232 460500 159404 989280    0    0   150  1120 4411 11152 14  9 68  8  0
 1  0 219232 460720 159404 989144    0    0     0     0  358 1058  2  1 97  0  0
 ```
 Sections:
  - procs - processes
    - r - runnable processes (running or waiting for run time)
    - b - blocked - processes in uninterruptible sleep
  - memory - RAM
    - swpd - amount of virtual memory used
    - free - amount of idle memory
    - buff - amount of memory used as buffers
    - cache - amount of memory used as cache
    - inact - amount of inactive memory (`-a` option)
    - active - amount of active memory (`-a` option)
  - swap - memory pages wapped to and from the disk
    - si - amount of memory swapped IN from Disk ..per second?
    - so - amount of memory swapped OUT to Disk ..per second?
  - io - input and output
    - bi - blocks IN from block device (Disk) ..per second?
    - bo - blocks OUT to block device (Disk) ..per second?
  - system - how many times kernel does something
    - in - number of interupts per second, including the clock(what clock?)
    - cs - number of context switches per second
  - cpu - cpu statistics (%ntage of total CPU time)
    - us - time spent on user code(non-kernel) - user time including nice time
    - sy - time spent to run kernel code - system time
    - id - idle time 
    - wa - time waiting for I\O
    - st - time stolen from virtual machine - wut  
  Device output:  
  ```
  vmstat -d | ( head -2; grep 'mmcblk0'; )
  ```
  both commands after pipe will be executed in subshell working with same output of `vmstat`, so headers and matched stuff is displayed
 
## iostat:
  available in `sysstat` package for Ubuntu  
  __Shows average for whole UPTIME time__ first time, next pages will be for the Interval duration  
  Shows detailed but not overwhelming I\O statistics (see `vmstat -d` for overwhelming one)  
  __Params__:
  - -d - show disks only (w/o CPU average)
  - <n> - interval in Seconds - LATEST PARAM
  - -p ALL - displays all partitions (including partitions)  
  Note: root device(such as `sda`) also could have I/O time, i.e. used to read the `partition table`  
  Example:  
  `iostat -d 2` - dont show CPU avrg, update every 2 sec  
  `iostat -d -p mmcblk0` - show disks and partitions only for mmcblk0 block device   
  __Fields__:
  - tps - Data Transfers per second
  - kB_read/s - avrg num of kiloBytes read per second
  - kB_wrtn/s - avg num of kB written per second
  - kB_read - total number of kB read
  - kB_wrtn - total number of kB written

## iotop:
  top processes using I\O, uses ThreadID instead of PID
  main Thread ID is the PID, other threads will increment the TID  
  > _Requires SUDO or Root_

  Use Case 1 Example:
  ```
  # Show for a User, use PID, show accumulated I/O
  
  iotop --user=dmitry -P -a
  ```
  Use Case 2 Example:
  ```
  # Filter by User, show PID, show timestamp, be quiet, update every 5 seconds, only show ones using IO, accumulate IO usage

  iotop --user=dmitry -P -t -qqq -d5 -o -a
  ```

  __COLUMNS:__
  - TID - thread ID
  - PRIO - priority, has 3 `scheduling classes`:
    - `be/n` - Best Effort/priority - most processes are of this scheduling class - Kernel will try to evenly proiritize it with others `be`.
    - `rt/n` - Real Time - Kernel will schedule I\O first for this class
    - `idle` - Idle processes are served when nothing other waits, has no priority
    - Priority is higher with lesser `n` i.e. `be/4` is scheduled _after_ `be/3`
  - USER - username of process owner  

  __Params:__
  - -o/--only - only show processes doing IO
  - -u USER/--user=USER - processes for the user
  - -P/--processes - show Processes(PID) instead of Threads(TID)
  - -a/--accumulated - accumulated IO
  - -b/--batch - non-interactive, good for Logging
  - -t/--time - adds timestamps, implies `--batch`
  - -q/--quiet - decreases headers and summary (-qq\-qqq), implies `--batch`
  - -d SEC/--delay SEC - custom delay(default is 1)
  
## ionice:
  change priority (i.e. `be/4`) of a process or thread in context of I/O resources usage
  > not usually used, probably let it be as is (same as of `nice` thing for processes and CPU)

## pidstat:
  shows good statistics about Processes

  Use Case Example:
  ```
  # Show info for PID 5769 every second
  pidstat -p 5769 1
  ```
  Use Case Example:
  ```
  # Show human readable Memory usage for current user for top 5(8-3) processes 
  pidstat -U $(whoami) --human -r | (head -3; sort -h -r -k 8) | head -8
  ```

  __Modes:__
  Could operate in different modes:
  - Memory: `-r`
    - `VSZ` - virtual memory (swap or mem+swap ?)
    - `RSS` - residental set size - RAM used
  - Disk: `-d`
    - `kB_ccwr/s` - cancelled, by task(process), writting to disk
  - CPU: `-u` - default mode
    - `%user` - user space processes
    - `%system` - kernel processes
    - `%guest` - used by Guest(virtual machine on the Host)
  - __and bunch of others, check `man pidstat`__

  __Params:__
  - `-C comm` - show only matched by 'comm' string, supports regex
  - `--human` - human-readable sizes
  - `-p PID,PID | SELF | ALL` - ALL is default - shows everything
  - `-r` - memory pages statistics - maj\min faults, overall utilization
  - `-T TASK | CHILD | ALL` - show individual process | it's children too | everything - tasks and children
  - `-t` - show Processes'(tasks) Threads info too
  - `-U USER` - filters processes of the User given, show Username instead of UID


## top: htop:
  gives a list of processes and resources used by the OS
  - p <PID> - List only few processes:
  ```
  top -p 10959 -p 10652 
  ```
  Process could has a priority:
   20 - is the LOWEST priority
   -20 - is the HIGHEST priority
  PID number 1 is always 'init' command or 'systemd' in my case on ubuntu 16 and
  centos 7
 

## uptime:
  shows TimeNow, kernel uptime and [Load Averages](#load-averages) of 1, 5 and 15 minutes accordingly
  Example:
  ```
 $ uptime
 19:01:52 up 17 min,  1 user,  load average: 1.49, 1.72, 1.26
  ```
  where:  
  - current time is 19:01:52
  - system is up for 17 minutes
  - 1 user is logged in
  - for last 1 minute 1.49 Processes were running (Occupying 1 Core and 46% of 2nd Core)
  - for last 5 minutes 1.72 processes were running
  - for last 15 minutes 1.26 processes were running

## lscpu:
  info about CPU
  Calculate number or CPU threads:
  `lscpu`
  `Thread(s) per core` * `Core(s) per socket` * `Socket(s)`
  Example:
  ```
  lscpu
  ...
  Thread(s) per core:  1
  Core(s) per socket:  4
  Socket(s):           1
  ...
  ```
  1 * 4 * 1 = 4 CPU Threads are available


## glances:
 https://opensource.com/article/19/11/monitoring-linux-glances?sc_cid=70160000001273HAAQ
if you are a fan of top / htop / atop / iotop....  you might like this..
looks like glances can also setup a "web interface" to view.. as well as monitor things like docker etc

## general processes:
  all processes are spawned from process with ID 1, 'init' in manual or systemd
  in my case, for some reason
  so there is a PID which is Process ID
  and PPID which is Parent Process ID, so every process has parent, except PID 1  it seems

## strace:
  Trace systemcalls made by a command or a process(daemons too)
  Starts after fork() systemcall is done.
   Example:
  ```
  strace cat /dev/null
  ```
  -o <file_name> - logs action of any child process that command(i.e. crummyd daemon) spawns into `file_name.pid` where `pid` is PID of child process  
   Example:  
  ```
  strace -o crummyd_strace -ff crummyd
  ```
  Could shows trace of used files by the given command
   Example:
   ```
  strace -s 2000 hostnamectl |& grep ^open | tail -5
  ```
   will return files directly opened by 'hostnamectl' tool
   still it could iteract with other services which could read from files, and this is not 
   seen here, alas.
  
## ltrace:
  trace calls to Shared Libraries. Output similar to `strace`.  
  Does not track anything on Kernel level - does it matter to me?
  Example:
  ```
  ltrace cat /dev/null
  ```

## watch:
  cool debug tool again:
  execute a program periodically, showing output fullscreen
   Example:
  watch "ps -eaf|grep [h]ostname"
  #execute 'hostnamectl' and new entry will be added 
  > root     11003     1  0 02:51 ?        00:00:00 /lib/systemd/systemd-hostnamed

## pstree:
lists tree of all the processes, squashes same sub-processes
Example:
```
pstree -l 2499
```
 - -p - display process PIDs


## ps:
  by default returns processes run by my current user and current terminal session  
  Use Case 1 Example:
  ```
  #Return 'java' processes and show header of `ps` using subshell (or { head -1; grep 'java'; } also works using Groups)
  ps aux | (head -1; grep 'java';)
  ```

  ```
  ps [-[-]]<params>   
	   UNIX standards - -<params>  
	   BSD standards - <params>  
	   GNU standards - --<params>  
  ```
params could be mixed, but conflicts could appear  
  a - list all processes that has terminal attached: ps a  
  x - list all processes owned my you(same EUID as ps)  
  ax - list all processes(less columns)  
  u - user oriented format  
  p <pidlist> (-p, --pid)- select process by id (or just PID, == --pid <PID>)  
  -C <cmdlist> - select by command name(COMMAND column)  
  t <ttylist> - select by tty  
  U, -U, -u, --user - selects by user name or EUID(RUID for -U), different selections by user , will output different stuff  
  -j - jobs format  
  j - job control format  
  -H - hierarhy(tree\forest format)  
  l - long format  
  -l long format , -y good with this  
  -f - full format listing  
  f - ASCII art process hierarchy (forest)  
  o, -o, --format - change columns format  
  Example:  
  `ps -o pid,ruser=RealUser -o comm=Command` - wil return 3 columns
  named PID, RealUser and Command, with values of standard colmns  
  `ps m -o pid,tid,command` - show threads , display PID, TID and command

`ps aux` - returns all processes run by all users from all terminals
    if user does not exist 'x' in case of UNIX format it could treat it
    in BSD format
`ps axjf` - formatted method of processes with parents in tree view

Example:
```
ps -eo pid,euser,ruser,comm
```
will show effective user(`euid`) and real user(`ruid`) and command executed

## setuid: setgid:
  executable bit from permissions set:  
  The `setuid` bit has no effect on directories. 
  `rwx` but for `x` it would be `s` in owner permissions  
  `setuid` - SETs User ID
  `setgid` - SETs Group ID
  Runs program with permissions of the owner but not executor.  
  `EUID` - effective user ID - id of Executable file Owner  
  `RUID` - real user ID - id of User who executed the Executable file

  But better to have them all the same (default behavior of `sudo`) - `EUID` usually set to all others `IDs`  (or `RUID` if setuid file owned not by `root` user) 
  To change the behavior add following into `/etc/sudoers` file:
  ```
  Defaults    stay_setuid
  ```
  Setup setuid:
  ```
  root@host [~]# touch myfile
  root@host [~]# chmod u+s myfile 
## capital `S` because no Executable(x) permissions for owner
##-rwSrw-r-- 1 test test 0 Mar 2 17:59 myfile
root@host [~]#  chmod u+x myfile 
## now it is as expected - lowercase `s`
##-rwsrw-r-- 1 test test 0 Mar 2 17:59 myfile
  ```

## pgrep:
  process grep , could find process id (PID) by process name, like
  pgrep bash - will return PID of the bash process running somewhere locally

## kill:
  for terminating the processes
  kill <param> <PID>
  each kills param has its number equialent
  -TERM\-15-  sends a Signal to a process (Term[inate] Signal), in other words
   it asks the application to call its Dispose method, to gracefully stop. i.e.
   kill 1292 or kill -15 1292 or kill -TERM 1292
  -KILL\-9 ask OS's Kernel to shut down the process even if the process(app) 
   does not respond for 
  -HUP - restarts process if possible, does not change PID
  -l - lists all the signals available with their names and numbers(minus SIG
   prefix)
  NOTE: only owner of the process(or root) could kill the process
  %JOB_SPEC - kills job with appropriate number
   Examle:
  kill %1 - will kill the job with JOB_SPEC [1]

 kill all processes:
  kill -9 $(ps aux | grep '/usr/lib/firefox' | grep -v grep | awk '{print $2}')
   $() will be expanded. if list is returned will iterate over every item
	in the list
   grep -v - invert grep results (excluding own grep process from ps aux)
   awk will print second column from the list - PID
	as per grep matched several lines(List de-facto) every line will be 
	rinted. And sent out of $() into kill -9 as argument

## killall:
 kills all instances of the process
  Example:
 killall xlogo - will kill all instances of 'xlogo'

nice: renice:
  changes priority of the process
  nice is for new processes
  
  renice is for already running processes

  renice <priority> <PID> i.e.
  renice 10 1292 - will change priority of process 1292 to 10, and will display
   previous value of the priority 
  
  nice <param> <priority> <binary_path> i.e.:
  nice -n 20 /bin/bash - will start New(-n) process from /bin/bash binary, with
   priority of 20(the lowest one)
   
=====Init

The Tragedy of systemd (Jan 24, 2019; linux.conf.au) - 
https://www.youtube.com/watch?v=o_AIw9bGogo


sysvinit, systemd, upstart
/usr/lib/systemd - systemd service manager working dir (new Rhel\Debian-like)
  https://unix.stackexchange.com/questions/5877/what-are-the-pros-cons-of-upstart-and-systemd  - GOOOOOOOD TO READ. Systemd compared to Upstart
/usr/share/upstart - upstart service manageer working dir(old Debian\Ubuntu)
/etc/init.d - sysvitin service manager working dir (old Centos\Debian\Ubuntu)
Note: Ubuntu could has all three installed currently, unlike centos that has
initd but it wrapped in systemd


service: daemon:
service vs systemctl vs upstart
service is an "high-level" command used for starting and stopping services in different unixes and linuxes. Depending on the "lower-level" service manager, service redirects on different binaries.
For example, on CentOS 7 it redirects to systemctl, while on CentOS 6 it directly calls the relative /etc/init.d script. On the other hand, in older Ubuntu releases it redirects to upstart
service is adequate for basic service management, while directly calling systemctl give greater control options.
 https://serverfault.com/questions/867322/what-is-the-difference-between-service-and-systemctl

  RHEL-like - systemd (moved from SysVinit)
	https://fedoraproject.org/wiki/SysVinit_to_Systemd_Cheatsheet
  Debian-like - upstart service (also adopted systemd)
  for Debian-like and RHEL-like systems its two different ways
   for Ubuntu since 15.xx it seems it is even more different..
   okay its systemd again(which uses systemctl as cli)
  Debian-like:
  status <service> 
  start <service>
  stop <service>
  restart <service>
   where service could be like 'ssh' or 'cron'
  to disable\enable services in upstart services the .override file need to be
  created in the /etc/init directory. For instance to disable cron:
   ensure cron exists:
   /etc/init/cron.conf - should exist
   echo "manual" > /etc/init/cron.override - will create text file with word
    'manual' as its only contents
   now cron will not be loaded on boot
   to enable cron back - simply delete 'cron.override' file from /etc/init dir

  RHEL-like:
  systemctl start <systemd>
  systemctl status <systemd>
  -- restart\stop 
   where systemd could be like 'sshd' or 'crond' where d means daemon
  systemctl disable <systemd> - disable system daemon from starting on boot
   basically it will delete soft link from /etc/systemd/... directory which it 
   seems is monitored on Init and everything there is executed
  systemctl enable <systemd> - enable datemon to start on boot
   basically it creates soft link from real binary location of daemon file into
   /etc/systemd/.... directory where all the links for boot are stored

  Ubuntu since 15.xx
  service <service> status
  -- stop\start\restart
   where service could be like 'ssh' or 'cron'
  service --status-all - will return statuses of all the services
!!  TODO: 
   check how to enable/disable services on boot for this new stuff and check how
  it exactly called now
  it seems it also supports systemctl and disable\enable with same commands,
   but creating .override file in the Debian way

# Systemd:
configure units(services)
 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/sect-managing_services_with_systemd-unit_files
 Example:
 https://www.jetbrains.com/help/teamcity/setting-up-and-running-additional-build-agents.html#Automatic-Agent-Start-under-Linux


## systemctl:
  cli for systemd service controller
  it is called by 'service' command for backwards and cross compatibility reasons

### list-units:
  list of active units on the system  
  Default command of Systemctl

### reload:
### daemon-reload:
  NOTE: Need to be called if Unit configuration has been changed during Runtime
  Reolads either 1 unit if passed to `reload`  
  or all of the units if `daemon-reload` is called (it is paremeterless)  
   Example:  
  ```
  systemctl reload NetworkManager.service  # reloads only Network Manager unit
  systemctl daemon-reload # reloads all the services
  ```

### list-jobs:
  lists jobs (Jobs is different kind of units)

## journalctl:
  displays logs
   Params
  -u - show logs for Unit
   Example:
  ```
  sudo journalctl -u sshd.service
  ```
    OR:
  ```
  journalctl _SYSTEMD_UNIT=NetworkManager.service
  ```

## hostnamectl:
  cli of systemd service controller related to hostname changes and management
  unified way to deal with hostname, list /etc/*-release, uname and stuff
   read more here, like really detailed - good stuff:
  https://unix.stackexchange.com/questions/459610/whats-the-point-of-the-hostnamectl-command
   Example:
  hostnamectl set-hostname 'gfs-server-03'
  
## change hostname:
  read more:  https://www.cyberciti.biz/faq/linux-change-hostname/
   with systemd adopted:
  hostnamectl set-hostname 'gfs-server-03'
   before
  /etc/hostname
   + edit everything with old hostname in /etc/hosts, and add hostname to localhost addr

## supervisor:
  lightweight services controller, available for both RHEL and Debian like
   distros. Could be used in containers instead of systemd or systemctl etc
  Manages processes, could restart and monitor them, require config file to
  operate
   See how to start several processes in Docker using this tool:
 https://kuldeeparya.wordpress.com/tag/how-to-run-ssh-and-apache2-in-docker-container/

  config file:
	[supervisord]
	nodaemon=true
	
	[program:sshd]
	command=/usr/sbin/sshd -D
	
	[program:apache2]
	command=/bin/bash -c “source /etc/apache2/envvars && exec /usr/sbin/apache2 -DFOREGROUND”


# Package management
## Debian\Ubuntu

.deb - packages format

## dpkg:
  dpackage - debian packages manager, fully console, installs only package
  -i - install a package
     only package , no dependencies - generate error with dependencies required
  -l - list of all installed packages(use grep for specific packages)  
    First 3 lines of output descripes first 3 +++ on every line , like `ii` or `rc`
    [Interpret Status](https://linuxprograms.wordpress.com/2010/05/11/status-dpkg-list/):  
    - ii - Desired state: Installed; Status state: Installed
      - which means successfully installed
    - rc - Desired state: Remove; Status state: Cfg-files
      - which means package marked for deletion and only config files are left after uninstall
    Remove packages marked for removal:
    ```
    dpkg --list |grep "^rc" | cut -d " " -f 3 | xargs sudo dpkg --purge
    ```

  -L <package> - list of all the files that were created during package 
		 installation
  -S <filename> - similar to 'yum provides' but only for Installed packages
		it returns list of all packages that contain the string:
		-S less will return also 'serverless' word.. 
	Another option is to use web interface on Ubuntu:
	https://packages.ubuntu.com/

## aptitude:
  frontend of dpkg, GUI in commandline
   categories of packages, local, available etc, then all the packages by 
  categories. also displays info about each package
  Enter key to open\close category.
  g or u - install selected package

## apt-get: apt:
  stands for Advanced Package Tool  
   `apt` is merge of `apt-get` and `apt-cache`, for easier use, has not all functions  but also has some additional functions main apt tool used to install or download packages reads dependencies, and could install all of them  
  `apt-get update` - reads all the repos and updates the local packages cache  
  Update single package with `install` if it is already installed of older version  
  `apt-get install` - installs package(s), list delimited by space
       or `install <package-2.3.5-3ubuntu1>` - installs particular version if  
		    it is compatible with distro and stuff     
       install -f - will install .deb package with its dependencies  
		    __Fix Broken Dependencies:__  
			https://unix.stackexchange.com/questions/159094/how-to-install-a-deb-file-by-dpkg-i-or-by-apt
  apt-get upgrade - made after Update, upgrades all the packages installed to
		    latest updates
          -y - answer Yes for all questons automatically
  .. dist-upgrade - updates to next available supported distro, 14.04 to 14.10 
		    if 14.10 is not supported already then to 15.04 etc
  apt-get autoclean - cleans cache, which means freeing space on hdd

  apt search <pkg> - search for package
  apt list --<params> - lists available packages from repos
	   --installed - lists only installed packages
  apt show <package> - shows info about package(apt-cache's command)

  --dry-run - simulation, will display possible actions but not perform any

  apt-get check - what dependencies may be broken
  apt-get build-dep - exact build dependencies for particular app (it seems it 
		is not necessery to download them all for work though...)

  apt-get download <package> - downloads package(probably to current dir
			       or /etc/apt?) - but only package w/o dependencies
  apt-get changelog - package changelog, like version history

  /etc/apt/ - apt configs
   sources.list - config of the repositories, which are re-read during 'update'
   		  command execution
  /var/cache/apt - cache folder of apt
  ./archives - contain all the archives, could be removed by 'autoclean' command

## apt-cache:
  support tool of apt used to work with apt cache(updated by apt-get cache 
  command)
  apt-cache pkgnames - list of all pkgs APT knows, not all could be downloaded
			installed or installable e.g. virtual pkgs
  ... search <package> - lists all packages that contain package name in its 
			 name or description
  ... show <package> - info about the package, like description in apptitude
  ... stats - info about local cache(packages related stuff) - could be shrinked
  	      by running 'apt-get autoclean' to delete useless stuff
  apt-cache showpkg <package> - shows infou about package
       		and where from package came from
    Example:
   >apt-cache showpkg htop
   >Package: htop
   >Versions: 
   >2.0.1-1ubuntu1 (/var/lib/apt/lists/ua.archive.ubuntu.com_ubuntu_dists_xenial-updates_universe_binary-amd64_Packages)
     it is htop package with version 2.0.1ubuntu1
     taken from ua.archive.ubuntu.com
      where repository is for xenial
      and name of repository is Universe
   apt-cache policy - returns all used repositories, kind of 

## apt-file:
  another extension for apt-get, is in different package, 
   need to be installed first

  apt-file update - updates its own caches, need to be run first
  apt-file find <pattern>  - alias for search, see below:
  apt-file search <pattern> - search packages for file matched by pattern.
		returns list of packages and paths where file was found;
		includes removed packages too
  apt-file list - similar to dpkg -L but package not need to be installed or
		fetched

=======Uninstallation
  apt-get remove <package> - removes package binaries, leaves configs in the 
		             system for future use by other version or another
			     reason
  .. remove --purge <package> - removes package and all the stuff package 
  .. purge <package> ^same^     created during install(SYSTEMWIDE configs, link
				etc)
		it will not remove:
		 -dependence packages , to delete orphanes use
			apt-get autoremove
			  or
			.. --purge autoremove (same - will remove configs also)
		 -NOT SYSTEMWIDE config files - user-specific files
			files in user's home dir, or .config subdirectory of 
			home, those could be hidde (starts with .)
  		 -doesn't reverse changes in already existing user-specific
		  config files
		 -doesn't remove 'gconf' and 'dconf' files or reverse any
		  configuration d\gconf changes
	existing SYSTEMWIDE configs also are not affected by neither purge or
	remove commands, those ones created by user or other packages. but 
	uninstalling package could sometimes affect such files and undone 
	something
  apt-get autoremove - removes all orphaned packages
  .. purge --auto-remove <package> is similar to autoremove
=======uninstallation end

=======Repository setup
 Repository configs are in 
  /etc/apt/sources.list and /etc/apt/sources.list.d/  
  Contents of sources.list file:
   deb http://archive.ubuntu.com/ubuntu/ xenial main restricted universe
  where http... is address of repo
        xenial is name of distro - $(lsb_release -sc)
        main ... is name of repository
   After 'add-apt-repository multiverse' this repo will be concatenated into
   line above:
    deb http... xenial ... multiverse

## add-apt-repository: apt-add-repository:
  Newer versions of Ubuntu support 'add-apt-repository'
   standard repositories could be added w/o specifing address, just repo name: 
    Example:
   sudo apt-add-repository universe
    this will add 'universe' repo to standard ubuntu addresses
  Older versions support this:
   sudo add-apt-repository "deb http://archive.ubuntu.com/ubuntu $(lsb_release -sc) universe"

 NOTE: make sure to 'sudo apt-get update' before use newly added repository
 Read More:
 https://askubuntu.com/questions/148638/how-do-i-enable-the-universe-repository

  Due to regular use of secure stuff also make sure following packages are
  installed(use apt-cahce show <package name> or 'apt list --installed'):
   apt-transport-https - This package enables the usage of 
	'deb https://foo distro main' lines
	in the /etc/apt/sources.list so that all package managers using the
	libapt-pkg library can access metadata and packages available in sources
	accessible over https (Hypertext Transfer Protocol Secure).
	  This transport supports server as well as client authentication
	 with certificates.
   ca-certificates - list of PEM files of certificates approved by Certificate
	Authorities, some common trusted certificates. For instance it has
	certificates for Debian stuff and Mozilla stuff.. 
	TLDR:
	It is a list of default trusted SSL sertificates stored in PEM format.
   curl - tool for transfering data from or to a server via HTTP/S, FTP/S, LDAP
	and lots of other formats, except SSH, but it has SCP..
	Used to download GPG key from docker storage. Which is added then to
	local apt storage
   software-properties-common - adds additional command for APT repositories 
	management. Such as 'add-apt-repository'

  Add gpg key:
	curl -fsSl https://download.docker.com/linux/ubuntu/gpg | sudo apt-key
	add -
	  Curl will download gpg key from docker , then pass it to apt-key (-) 
	and it will be added to local key storage
    make sure it is added, by searching for fingerpring:
	sudo apt-key fingerprint 0EBFCD88 - this will return OK if everything is
	fine
  Add a STABLE repository, it seems it is required:
	it is stored in:
	/etc/apt/sources.list - the file with the sources, where 
	add-apt-repository will add the repo by default
	/etc/apt/sources.list.d - directory where new file with a repo should be
	added, like in CentOS' yum
	add-apt-repository example:
	sudo add-apt-repository \
	   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
	   $(lsb_release -cs) \ - returns distro name
	   stable" - list of repos, test, edge could be added for docker
  PPA: launchpad.net:
        Ubuntu in default repos has only stable versions of packages that was 
        on moment of release. Later on only security updates.
        launchad.net has lots of repos(Personal Package Archives - PPA)
        add PPA repo to for latest versions:
      Ansible:
     $ sudo apt update
     $ sudo apt install software-properties-common
     $ sudo apt-add-repository --yes --update ppa:ansible/ansible
     $ sudo apt install ansible
	Another example:
     $ sudo add-apt-repository ppa:jonathonf/python-3.7
     $ sudo apt-get update
	
   

## update-ca-certificates:
  updates certificates from _somewhere_
   Pamars:
  --fresh - somethinh fresh?

## List certificates:
  awk -v cmd='openssl x509 -noout -subject' '
    /BEGIN/{close(cmd)};{print | cmd}' < /etc/ssl/certs/ca-certificates.crt
   Search for existing root cert:
  \ | grep -i '<root cert common name>'


### List all installed packages:
  dpkg -l - list of packages and their details
  apt list --installed - list of full names
  aptitude - has a category for installed pkgs divided by categories
 Active Repositories
  apt-cache policy


# RedHat RHEL\Centos

  .rpm files
## rpm:
  red hat package manager
  It seems it has some differend modes, where similar keys could exist with 
  different functions, i.e. -i - installation mode, -i in -q(query) mode is info
   the modes are:
   - install/update/freshen (first param -i)
   - uninstall (first param -e)
   - query (first param -q)
   - verify (first param -V probably)
   - set owners/groups
   - show querytags
   - show configurations
  Also it has general options, seems like it works with any mode
  if files or dependencies package need are absent installation of rpm will fail
  -q - query, probably 'what about something, like package requirements'
	single param will display package full name if it is installed or will
	say that it is not installed, i.e.:
!	rpm -q openssh-server - will return fill name if installed, like this
				openssh-server-7.4p1-13.el7_4.x86_64
   -p - package file
   -R - requires, lists package dependencies
  i.e.:
  rpm -qpR <rpm_package or package_name> - will list package requirements(depen
	dencies), if rpm_package name is given(xterm-123.3.rpm) - will check for
	package locally	if general package name is given(xterm) will look into
	remote repos
   -l - list files of the package, similar to 'dpkg -L' i.e.:
	rpm -ql openssh-server - will list all the files and paths of sshd
   -a --last - will list (a)ll the packages installed filtered by latest
   rpm -qa | grep nmap - will list all Installed packages, and grep nmap from 
		this list
   -d - documentation mentionings of a package 
   -f - some related to documentation param
   rpm -qdf <package> - return all documentation files where this package is
		mentioned
  -i - installation mode (fails if package installed of any version)
   -v - verbose (general parameter)
   -h - prints some hashes, looks better with -v
   -U - update package, BUT will install package if there is none installed
	if package is installed will not fail but update it(controversy to -i)
  rpm -Uvh <rpm_package or package_name> - will update or install package, 
	fails if there are missing dependencies
  -e or --erase - erase package, uninstall mode
   -v - short verbose mode, RETURNS NOTHING if no Verbose mode is passed
   -vv - very verbose, dunno why not -v
  -V - verify, there is some verification key for every package and rpm could
	check whether the package indeed has that key
   -a - all, same as for -a in Query mode of rpm tool 
   Keys could be imported from remote repos, or rpm could list all the keys
   that has been already imported
   rpm --import - will import key somehow from somewhere..
   rpm -qa gpg-pubkey* - will list all the public verification keys on system
  
## yum:
  Stands for "Yellowdog Updater, Modified". yellowdog is already unsupported 
  version of Linux for PowerPC, which is also dead now
  yum <params> <package_name>
  Package management system
  yum update 
  yum upgrade - same as update - updates all the pkg repos and upgrades pkgs

  yum list [<pkg>] - displays if installed, version, and @updates mean that it
		   accepts updates. Package could be market to ignore updates
		   which probably means that yum update will not affect it
  ..  list installed - returns list of installed packages
  yum search <pkg> - searches for package, search in name and description
  yum install <pkg> - install package, and its dependencies
	-y - answer Yes for prompts automatically
  ..  install https://...../package.rmp 
        will install package from remote location
        or this installs repolist.. from which that packages could be 
        installed..
        --enablerepo=<reponame>  - enables repo
    Example:
   yum install -y https://centos7.iuscommunity.org/ius-release.rpm && \
    yum install -y php71u-fpm
  yum localinstall <pkg> - installs .rpm package downloaded to local machine
	will ALSO install needed dependencies, unlike 'rpm' tool
  yum info <pkg> - info about a package: descr, url, size, arch, etc
  yum check-update - checks which packages could be updated
     -C - use only local cache, doesn't update cache
  yum grouplist - YUM can group packages, this command displays all the groups
		  those groups will install bunch of packages
  .. groupinstall '<group name>' - will install all the packages from the group
  .. gropuremove '<gn>' - removes all the packages related to the group
  .. groupupdate '<gn>' - updates same stuff

  yum-config-manager --enable remi  - enable repo

  yum repolist - lists all available(enabled) repositories
  ..  .. all - lists all the repos disregard of their staus
  .. --enablerepo=<repoid from repolist> install <pkg> - enable a repo and
		install the package from that repo
    Example:
   yum repolist enabled
    show enabled repos
   yum repolist all
    show all repos (including disabled)
  yum provides <feature or file> - Just use a specific name or a 
  	file-glob-syntax wild‐cards to list the packages available or installed
	that provide that feature or file.
         Note:
        this could not work for regular user, use 'sudo'
  yum clean all - clears cache similar to apt-get autoclean
    Example of usage:
     use it after some bunch of packages has been installed to clean space
  yum history - history of yum - install\remove\upd package and stuff, sudo

### Manage repositories
repos stored in /etc/yum.repos.d, which is a directory
## to add new repository:
 create new file in the /etc/yum.repos.d dir named '<anything>.repo'

## Format of repo file:
[dockerrepo]  - name of the repository section
name=Docker Repository - name of repo to display
baseurl=https://...    - address to where from do pulls of packages
enabled=1	       - repo is enabled inside my system
gpgcheck=1	       - enables gpg check
gpgkey=https://....    - location of the key


=======Uninstallation
  yum remove <pkg> - removes the package, doesn't remove its dependencies
!   yum autoremove <pkg> - also removes package, probably its deps too..
  To remove dependencies there are several ways:
  Option1: [probably should do this as standard]
  yum autoremove - removes orphaned dependencies, similar to 'apt-get utoremove'
  Option2: [probably could try this]
  update /etc/yum.conf file
   set
    clean_requirements_on_remove=1
   it is boolean value, which works on removel\update\obsoletion, goes through 
   each package's dependencies and deletes ones that are no longer required.
   default value False, could be 1,0,True,False,yes,no
  Option3: [probably better try not to use it]
  yum history undo <ID>
  basically in undoes the operation, that was performed during installation
  and thus removing the package and all the dependencies, and probably other
  stuff. 
  the only thing here is that i'm not sure whether it checks that package from 
  dependencies is dependent only by this package that caused them to be 
  installed, or not, so is not - it could end up with broken dependencies
  so how to do:
  yum history - check the ID column next to the command line with installation
  command of the package that need to be uninstalled
  then call 'yum history undo <ID>'


### List all installed packages:
  rpm -qa - query all packages, will return list of all installed packages
  yum list installed - will return list of installed packages


## yum-utils:
  yumdownloader:
  yumdownloader <pkg>
    downloads a .rpm file, into current dir pretty similar to apt-get download
    command

# File Permissions / Ownership

! Linux treats everything - device\file\directory as if it is a FILE

Discertionary Access Control (DAC)(used after Mandatory Access Control MAC from LSM):
 drwxrwxrwx:
d - type of file(file\device\dir\link)
 r - read
   r only gives ability to list contents, but lots of ????? and file names
    can't even read files with 'cat' and stuff, need x permission for this
 w - write
 x - execute
   x only gives ability to 'cat' files, but cant ls into dir
    if file in such dir has only 'r' can even 'cat' it, if file has only x: cant
     cat it, need 'r' on file in dir with only 'x' to cat
   together with read - can ls norm and execute stuff like 'cat' to read file
first 3 - user permissions 
second 3 - user's group permissions
 third 3 - all other users

## chown:
  changes owner and group of a file\directory\link
  chown <params> [user][:group] <file/dir>
		 -reference=R_FILE FILE
  !!!! links could change owner of targeting file only!!!!!!!!!!
  -h \ -H - for changing owner of LINK not referenced location, see MANUAL
  -R - recursively change owners of nested directories
  -v - verbose, could be redirected to a file for future use, just in case
   in case of fucked up links
  could change only owner(<owner>), only group(<:group>), or both(<owner:group>)  could take ownership schema like owner:group from a file:
   chown --from=:user :otheruser file.txt

## chmod:
  change modification
  chmod [<params>] <permissions> <file_path>
  text modifications:
  u - owner
  g - group
  a - all
  r/w/x - read/write/execute
  i.e.
  chmod a+rwx <filename> - give all the permissions of read/write/execute

  _Setuid_: see [setuid](#setuid:-setgid:)


  binary mode:
  4 - read
  2 - write
  1 - execute

  sum of numbers above will indicate permissions
  rwx = 4+2+1 = 7, and so on 

  Parameters:
  -v - verbose for every file processed
  -c - changes (less verbose), only when change is made
  -f - supress most error messages, --silent, --quiet
  --preserve-root - fail to operate recursively on '/'
  -R - recursive

## lsattr:
  lists extra attributes of files on linux file system
  lsattr file1 - display attributes of the file1

## chattr:
  changes extra attributes, lots of them, here some:
  i - immutable (can not be changed)
  a - appendable (can be opened only in append mode)
  u - undeletable
  syntax: 
  + - adds attribute
  - - removes attribute
  = - causes  selected attrs to be only attrs that files have..(not sure)
  i.e. 
  chattr +u file1 - adds undeletable attr to file1

# IPTables / Linux kernel Firewall(not d) 
 read more:
 https://www.unixmen.com/iptables-vs-firewalld/
 https://linuxacademy.com/cp/socialize/index/type/community_post/id/15473

!!!!!!!!!!!!!!!!  
!!  WHen using SSH configure SSH rules first BEFORE changing   
!!    the policies of the chains in the Filter table.   
!!  !!    TO DO NOT LOCK MYSELF OUT OF THE MACHINE  !!  !!   !!   !!  
!!  CentOS 7 applies the rules right after entering the command  
!!  even if Service IS stopped.  
!!!!!!!!!!!!!!!  

__Iptables is a tool to work with Netilter Framework, kind of firewall__
/There are different services on different Linux distributions such as:  
- Ubuntu - ufw(Uncomplicated Firewall) service for Ubuntu, which uses IPtables  
- Centos7 - firewalld - firewall daemon which uses iptables  
- generic? - iptables-services - firewall service that uses iptables, generic it seems

## Packet Flow:
[VIDEO: iptables: Packet Processing](https://www.youtube.com/watch?v=yE82upHCxfU)  
Expose Docker port w/o `-p` using only Iptables chains in tables - PREROUTING\FORWARD chains: 
[VIDEO: How Linux processes your network packet - Elazar Leibovich](https://www.youtube.com/watch?v=3Ij0aZRsw9w&t=2s)  
Packet from Network arrives (__INGRESS__):
- __PREROUTING__ - DO stuff _before_ checking whether to Route the package:
  - __raw__ table - check whether to _bypass_ the firewall
  - __mangle__ table - makes changes to packet (mangles it) [what is mangle](https://serverfault.com/questions/467756/what-is-the-mangle-table-in-iptables)
  - __nat__ table - implement Network Address Translation
- __INPUT__ \ __FORWARD__ - IS this host the destination?
  - __INPUT__: 
    - __mangle__ - again mangle packet headers
    - __filter__ - main Table where actuall Firewalling usually occurs
    - __security__ - Mandatory Access Control(MAC) implemented here(SELinux and stuff)
    - When done packet is passed to `Network Socket`(TCP Port) of the Host application
  - __FORWARD__:
    - __mangle__  - arcane stuff with a packet
    - __filter__ - check conditions on whether to keep forward
    - __security__ - Mandatory Access Control
    - Is packet still forwardable?(is there anything listening for it?)
      - NO: apply some actions (Default: __DROP__)
      - YES: 
        - __POSTROUTING__:
          - __mangle__ - mangle stuff here when packet is Forwarded
          - __nat__ - apply NAT rules such as 'destination' IP\PORT change
          - Packet is sent to DataLink layer and to destination Network Interface
Packet generated by Local process(__EGRESS__ \ __OUTBOUND__):  
- __OUTPUT__ Chain processing:
  - __raw__ table - check whether to bypass FW
  - __mangle__ table 
  - __filter__ table - actual work is here usually
  - __security__ table
  - Is packet still routable?
    - NO: drop the packet
    - YES: 
      - __POSTROUTING__:
        - __mangle__ table
        - __nat__ table
        - The packet is set to Data Link Layer of Outbound interface

__Linux Netfilter Packets Flow:__  
![Linux Netfilter Packets Flow](img/iptables-inbound-outbound-packets-flow.jpg)

## Main three groups or something
### Tables:
 used to group different functionalities and separated in five:
 - filter - default table if no specified. Packet filtering.
   Built-in chains:
    - INPUT
    - OUTPUT
    - FORWARD
 - nat - used for network address translation(NAT)
 - raw - first table to check, configuring exemptions from conn. tracking
 - mangle - some specialized packet alteration...
 - security - Mandatory Access Control(MAC) networking rules
 -t\--table - parameter to specify a table

### Chains:
 Tables contain set of chains.  
 Chains group rules on different points of process.  
 Chains could be Built-in or User defined.  
 -  INPUT - Filter. input packets
 -  OUTPUT - Filter output packets
 -  FORWARD - Filter. going through packets
 - -N\--new-chain - adds user defined chain
 - -X (--delete-chain) - delete chain

### Rules:
 Defined as a set of matches and a target.   
 Are listed in chains and followed by order until a match is found, then packet  
 is handled by the target specified by the rule.  
 - -A (--append) - add new rule
 - -D (--delete) - delete rule along with the chain in which is contained
 - -L (--list) - list rules
 - -S (--list-rules) - same list but in command format, as if the commands
		     that were used to add rule is saved and displayed here


### Matches:
   if match is true the packet will be processed by iptables.  
   Following matches exist(not full list):  
   - -s (--source) - source of the packe(IP, hostname or network IP)
   - -d (--destination) - destination of the packet(IP, hostname or network IP)
   - -p (--protocol) - protocol(tcp/udp/all , all - default if none specified)
   - -i (--in-interface) - interface that receives(left side of ifconfig output)
   - -o (--out-interface) - output interface
   - ! - Not. Match everything that is not in the match.

  Protocols has also its own matches, like 
   --dport - destination port(22 for ssh) is match for TCP protocol
  see full list of protocol related matches:
   iptables -p <protocol> -h - where protocol is tcp, icmp etc i.e.:
    iptables -A INPUT -p tcp -i eth0 --dport 22:25 -j ACCEPT 
      - adds rule to default table (-t filter) , appends Chain INPUT, matches:
	protocol TCP and interface eth0, protocol specific match port with
	range of ports 22 to 25. Rule's target if match is met - ACCEPT. 
	If not next rule will be processed until RETURN or end of rules, then
	Default TARGET(Default policy) will be applied.

### Targets:
  Determines the action to be taken if packet is Matched.  
  - -j (--jump) - specify Target
  There are 4 built-in Targets:  
   - ACCEPT - no checks , just accept packet
   - DROP - refuse packet, do not send a response(simply ignore it)
   - QUEUE - sends the packet to user space
   - RETURN - returns to previous chain, or handle by Chain policy(Default?)

### Default Policies:
  When packet is not matched by any Rule on the Chain - it is handled by the
  target specified in the policy of that chain.  
  Two main approaches:
   - Accept everything by default, and add rules to refuse access
   - Refuse everything by default, and add rules for accept
  - -P (--policy) <chain> <target> - set default policy for a chain with target


## iptables:
  a generic table structure for the definition of rulsets  
  basically ruleset of Linux kernel firewall  
  it is a tool to get daemon(service) install other app:   
   __Centos__ - iptables-services(firewalld)  
   __Ubuntu__ \ __Debian__ - ufw(probably goes with iptables pkg)  
   __RHEL__ - iptables  
   
  
  - -L - list all rules currently installed on the system
  - -A CHAIN_NAME - Append Rule at the end of Chain  
    Example:  
    `iptables -A INPUT -s 127.0.0.1 -j ACCEPT` - accept my own traffic to myself (throught loopback interface)
    `iptables -A INPUT -p icmp -j ACCEPT` - append rule to INPUT Chain to accept ICMP traffic   
    `iptables -A INPUT -p tcp '!' --syn -j ACCEPT` - append rule to INPUT Chain to ACCEPT all TCP protocol traccif which is NOT `SYN` packets (used to establish TCP connection) - so outside can't establish connection with me  
    `iptables -A INPUT -p udp --source-port 53 -s 192.168.1.1 -j ACCEPT` - accept UDP traffic FROM 53 port (DNS) from Name Server 192.168.1.1  
    `iptables -A INPUT -p tcp --source-port 22 -j ACCEPT` - accept TCP traffic TO port 22
  - -I CHAIN_NAME N - Insert rule at the top, or at N line in Chain
  - -D CHAIN_NAME N - Delete rule in Chain at N line
  - -P CHAIN_NAME POLICY - set given `POLICY` to Chain  
    Example:  
    `iptables -P INPUT DROP` - drop everything by default(not matched by rules)

Service uses loads default config file when starts, applying some 
default rules, /etc/sysconfig/iptables for CentOS 7
  to manage a service:
   Debian/RHEL: sudo /etc/init.d/iptables start\stop\status
   Ubuntu: sudo service ufw start\stop\status
   Centos: servicectl start\stop\status iptables

## IP address ranges:
CIDR blocks
 https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation
 https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks

## firewalld:
## firewall-cmd:
  https://www.digitalocean.com/community/tutorials/how-to-set-up-a-firewall-using-firewalld-on-centos-7
 firewall-cmd --get-default-zone
  get default zone
 firewall-cmd --get-active-zones
  get all active zones
 sudo firewall-cmd --list-all
  list all rules for zones

 Open ports:
  1. Open port for a service.
   add service to /etc/services
   add service to FirewallD
   https://linuxhint.com/open-port-80-centos7/
  2. Open prot directly:

 OPEN PORT:
  sudo firewall-cmd --zone=public --add-port=5000/tcp --permanent
 CHECK OPENED:
  sudo firewall-cmd --zone=public --list-ports
 RELOAD AFTERWARDS:
  sudo firewall-cmd --reload

 Example:
  firewall-cmd --zone=public --add-port=5693/tcp      - turns on the rule
  firewall-cmd --zone=public --add-port=5693/tcp --permanent   - saves the rule so it is applied on the boot


# Bash
## Bash command line shortcuts
ctrl+a - moves cursor on the beginning of the line
ctrl+e - moved cursor on the End of the line
ctrl+u - removes everything from cursor position to the beginning of line
ctrl+k - removes everything from cursor position to the edn of the line
ctrl+w - removes previous word(space separated)
ctrl+l - cleares screen except of current command LINE
ctrl+p - last executed command shown in command line, same as UP ARROW key 
ctrl+c - terminate current process + exit\del current line by terminating it
ctrl+z - pause program, adding it to backgroud job. 'bg'\'fg' to continue run
## history:
ctrl+r - bash history by typed command
ctrl+r - again looks one matched line upwards
ctrl+j - edit matched line
ctrl+o - execute current command from history

## Bash Shell
## alias:
 new aliasws workws only for current session
 alias <new name>="<command>" - adds command as alias
 alias <old name> - shows command
 alias - shows all aliases for current session

## sleep:
  just regular sleep, stops current thread for amount of given seconds

  Experiments:
  sleep 10 & - will stop the thread for 10 seconds and move it to background

## background: (not command)
  to start process in a background use & at the end
   Example:
  xlogo & - will start xlogo but cmd will return control to user
   xlogo will run as a Job, with 'Running' state

## jobs:
  built-in command
  shows jobs in background, running, stopped
   More read:
  https://unix.stackexchange.com/questions/116959/there-are-stopped-jobs-on-bash-exit
   Params:
  -l - display PID

  NOte: kill could also accept JOB_SPECs to kill jobs, instead of PIDs

## fg:
  moves job to the foreground
  fg JOB_SPEC
   Example:
  fg 1 - moves job [1] to the foreground
  Ctrl+z will move it back, but 'paused'- use bg to continue in background

## bg:
  moves job to the background
   If program is 'Stopped' like after executiong ctrl+z shortcut, bg <%num>
  will continue its execution as a job, in the background
   Params:
  %job_spec - address numbered job

## script:
  writes a session script - all executed commands and stuff
   with time parameter will create file with timings of each command and could
  play it afterwards using these two files

## bash:
  man bash
  
  -c - accepts a whole bunch of commands like:
  /bin/bash -c "while true; do echo HELLO; sleep 1; done" - which will execute
	echo HELLO endlessly witn interval of 1 second.
	This even could be passed into a container:
   docker run -d ubuntu:xenial /bin/bash -c "...."


  could customize command prompt from default <user>@<hostname>:<path>$ 
  to something like <HH:MM:SS> <user> <whatever>!
   \t - 24 hour time format
   \w - full current path
   \w - current directory only
   \u - username
   \j - amount of running jobs
   \h - hostname
      Hostname is stored in /etc/hostname

## hostname:
  network computer name. command returns current hostname
  value is stored in /etc/hostname
   Synopsis:
  hostname [-b] {hostname|-F file} - set hostname (from file)
  hostname [params] 		   - formatted output
  hostname 			   - display hostname
   Params:
  -I - show all ip addresses for host machine
  -b - set hostname
   Example:
  hostname -b new_hostname
  
  TO CHANGE HOSTNAME:
   change hostname in /etc/hostname
   change every occurance of old hostname to new one  in /etc/hosts
    If nothing is available in /etc/hostname first hostname with 127.0.0.1 
   address will be taken as hostanme

## history:
  lists history, dumps .bash_history file into STDOUT, lines numbered
  -c - Clear history, but only in session, .bash_history remains the same
	only in CentOS it seems, ubuntu does nothing
   Example:
  history | grep /usr/bin - look for only what matched

## which:
  shows location of a command and its alias if exists
   Does not show built-ins
  can search by alias too
  which <command\alias>
   i.e.
  which ll - will return alias line and path for binary

## whereis:
  searches for binary file, sources, man files of the command
  Does not work with aliases!
   i.e.
  whereis pwd - will return path for binary and path for man page
   -b - search only for binaries
  Example:
  Remove IntelliJIdea:
  $whereis idea
  remove all entries returned, and places whihch they are linking
   https://intellij-support.jetbrains.com/hc/en-us/articles/206544519
   https://askubuntu.com/questions/300684/uninstall-intellij-ultimate-edition-version-12/300692#300692

Streams and Redirects. redirection:
 Linux\Unix philosophy is that 'everything is a file' but in fact this are not
  files but block defices stored in /dev or /proc directories, and not files but
  some stuff stored in RAM for instance
 STD - means standard
  Read More:
  https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr
## STDIN:
  I/O Stream
  file handle that process read to get info from user
  NOTE:
/dev/stdin is a symlink to /proc/self/fd/0 -- the first file descriptor that the
  currently running program has open. So, what is pointed to by /dev/stdin will
  change from program to program, because /proc/self/ always points to the 
  'currently running program'. (Whichever program is doing the open call.) 
  /dev/stdin and friends were put there to make setuid shell scripts safer, and
  let you pass the filename /dev/stdin to programs that only work with files, 
  but you want to control more interactively. (Someday this will be a useful 
  trick for you to know. :) 
## STDOUT:
  I/O Stream
  process writes normal info to this file handle
## STDERR:
  I/O Stream
  process writes error info to this file handle

  > - used to redirect STDOUT in cli to someplace else (i.e. file instaed of 
	console)
  2> - redirects STDERR
  >> - redirects STDOUT\ERR but file will not be overwritten, and appended 
	instead
  < - redurects STDIN, it will take data not from command line but from
	something else, i.e. a file
  cat /etc/passwd > /tmp/out     # redirect cat's standard out to /tmp/out
  cat /nonexistant 2> /tmp/err   # redirect cat's standard error to /tmp/error
  cat < /etc/passwd              # redirect cat's standard input to /etc/passwd
    or like this:
  cat < /etc/passwd > /tmp/out 2> /tmp/err

NOTE: redirect Standard Output(and error too) to null will hide it from everyone
  like this:
   ls secret_place >> /dev/null - will drop output into a void!!11
  Errors also could be hidden like this, in case of exposing of some secure or
  sensitive info

## Redirect SDOUT and STDERR into two files in the same time:
  cat goodFile notExistingFile >mystdout 2> mystderr
	mystdout will contain standard output
	mystderr will contain standard error output

## Redirect STDOUT and STDERR into single file:
  cat goodFile notExistingFile >mystdout 2>&1
	mystdout will contain both STDOUT and STDERR
    Or
  cat goodFile notExistingFile &> mystdout

## noclobber:
  is an option set-up by 'set' command
   prevents redirection '>' from overriding files. it will return an error
  


## pipe: |:
  redirects standard output of first program into standard input of another

## expand:
  changes Tabs to Spaces in files from argument or stream from STDIN
   Example:
    cat -A test   >>> ^Ia ^Ia    a^Ia$
    expand test | cat -A  >>>        a       a    a  a$
  will change all tabs in file to spaces.

## unexpand:
  change Spaces to Tabs - vice versa of 'expand'  

## cut:
  Removes section from each line(column in fact) from a File
   Or parses each line in file by delimiter(Default is TAB) and could retreive
   only selected columns, or bytes - this is for -f option
  cut <param> <filename>
  See more: https://www.computerhope.com/unix/ucut.htm
  -d '<delim>' - change delimiter to <delim, ' - to prevent bash expansion of
	meta characters, if * or similar is delimiter. 
	-d ':' - safe way 
	  or
	-d: - will work too, but could fuck up a little
   Example:
  cut -f1 -d: passwd - will get only first field(-f1) from a file passwd
		fields are created by delimiting by colon(-d:)
  -b --bytes=LIST - cuts out bytes
  -c --characters=LIST - cuts out characters(in different encodings 1 character
	could be more than 1 byte, which is 8 bits)
	 Example:
	10/01/2007 - '-c 7-10' will cut out '2007'
  -f --fields=LIST - field to cut. or fields.
		uses delimiter(default TAB) to parse each line by it then
	it could return columns mentioned in the list by numbers from 1
	 Example:
	abc<tab>10.1<tab>10/01/2007 - '-f 3' will cut out '10/01/2007'
  How LIST works:
   !List of Integers
   !Starts from 1
   list of Integers, or range of integers, or multiple ranges, Separated by 
   commas. Selected columns is printed in the same order they were read,
   written to output exactly once(dunno what it means)
  N - the Nth byte, character or field
  N- - range from Nth byte,char,field to end of the line
  N-M - range from N to M as above
  -M - from beginning of the line till M
  -f Option:
  cut -f 1-2,4-5 data.txt - will cut 1st,2nd, 4th, 5th columns from data.txt
   --output-delimiter - changes delimiter in the output i.e.:
    cut -f 1,3 -d ':' --output-delimiter=' ' /etc/passwd - will substitute
	colon delimiter ':' to space ' ' in the OUTPUT of command for easier
	read
    ..... --output-delimiter=$'\t' - will substitute colon ':' for tab '\t'
	character, '$' here is for escapt
  -c Option:
   cut -c 3-12 data.txt - will cut from 3rd to 12th characters in the data.txt
		will not use delimiters at all
  -b Option:
   cut -b 3-12 data.txt - will cut rom 3rd to 12th BYTE, in case of ASCII 
		encoding it will be the same with -c as there 1 char == 1 byte

  Advanced example:
   grep '/bin/bash' /etc/passwd | cut -d ':' -f 1,6 - greps passwd file for 
	users that use /bin/bash as their default shell, and prints out 
	username and user home directory
	So cut accepts STDIN redirection

## paste:
 vice versa of 'cut' - pastes columns in files

## join:
 sql join but for text files
  in case of need to use this, use man\google, i'm to lazy to document
 Sorry.

## bash escapes:
## special symbols:
$'\t' - print tab, which is not printable character, same as /r/n or somehting

## Bash Scripting
## test:
 [ 1 -eq 1 ]    - ints compared with -eq, -lt, -gt, -ge, etc
 [ 'a' = 'a' ]  - strings compared with =, <, >, >=, etc
  tests condition, returns 0 if test is passed(true)

  -f <file> - file exists, aviode ~
  -d <file> - directory exists
  -h <file> or -L - symlinc exists
  -s <file> - exists and has size greater than 0
  -u <file> - exists and set-user-ID bit is set
  -k <file> - exists and its sticky bit set 
  -n <string> - length is not zero
  -r <file> - file exists and with read permission
  -w <file> - file exists and with write permission 
  -x <file> - file exists and with execute permission
  -z <string> - length of string is 0

  Test could be performed from script , better to use braces [ . ]
  Also could be preformed right inside CLI, it will return nothing, so need to
  create manual then\else-like structures:
   test -f '/path/to/file name' && echo "exitcode: &?. True" || echo "exitcode:
 &?. False" - test for file existance, then if exit code 0 will execute second 
	echo with True, else (OR) will execute echo with False.
      &? -  will print exit code of previously executed command, which in this 
	case is 'test' command

  Test even can be performed w/o test itself:
    Example:
  curl -s google.com | egrep -ci '301 moved' > /dev/null && echo "file has moved
" || echo "false" - curl will pass STDOUT to egrep which will try to match 
	string '301 moved', if it succeed exit code will be 0.
	And first Echo (after &&) will be executed
	Else, egrep will return 1, and thus second echo (after ||) will be
	executed
  
## for:
  regular for loop could be done in command line too

  for i in `command`; do echo $i; done
    backtick -  `, does the job, expression between it will be executed in 
	sub shell and its result will be returned, when we have a collection
	we could iterate it with FOR 
  for line in `cat file1`; do echo $line; done
	

# Other programs:

## elinks:
  command line browser, with interface similar to MC or aptitude
  
## mc:
  two-windowed GUI to work with file system


===Nginx
  fast web server, could be used as load balancer between servers(docker 
   containers) - reverse proxy server

  package name: nginx
  /etc/nginx - config(as usual in etc dir)
    ...conf.d - directory for some main configs probably
    ...default.d - directory for default configs
    ...sites-available - directory for available sites, convention directory
	does not exist by default
    ...sites-enabled - same as previous
	Read more about sites-* here:
	https://serverfault.com/questions/527630/what-is-the-different-usages-for-sites-available-vs-the-conf-d-directory-for-ngi

    Basically all the configs are included in main config:
    /etc/nginx/nginx.conf
	it includes all the configs from directories mentioned above
	same settings from different configs are overwritten by later added:
	first.conf(a=1, b=2), second.conf(b=4, c=3) will be treated like this:
	a=1, b=4, c =3
 !!! MAKE SURE to follow syntax !!!
	'upstream' directive can not be added before http{} section 
	or inside http{} section

	So all includes need to be made correctly
	also not all directives could be added into configs that are included
	because it could lead to conflicts and Nginx wont start
	
	IF nginx is not starting check:
	journalctl -xe - for service errors

	IF nginx throws errors or behave in not expected way check error log
	its location in nginx.conf file
	default location: /var/log/nginx/error.log

    Nginx could refuse connection:
	(13: Permission denied)
       this related to SELinux, which also has error log:
	sudo cat /var/log/audit/audit.log | grep nginx | grep denied
     SELinux has bool values that Nginx and Apache and probably others use
       getsebool -a | grep httpd - will return all bools related to httpd 
	and Nginx(it uses them too)
     To allow connections set:
	setsebool httpd_can_network_connect on - or use 1
	-P - will set it permanently
	

    Config setup, for proxy server:
    Option 1:
	create config with following stuff:

#pool for balancing? should be after http{} section
upstream containerapp {
        server localhost:8081; #- not sure whether localhost is fine
        server localhost:8082;
}

#server configuration
server {
        listen *:80; #everything coming to port 80

        server_name localhost;
        index index.html index.htm index.php #match any such file and use it

        access_log /var/log/nginx/localweb.log; #save logs here
        error_log /var/log/nginx/localerr.log;

	#location is root , for some reason
        location / {
		#pass traffic coming through port 80 into the pool
                proxy_pass http://containerapp; 
        }
}

     Option 2:
	Add upstream (with local network IPs) into nginx.config after http{}
	section
	Update server{} like above, but w/o *_log fields
	Server_name also use local network IP
	ALso i commented default 'root' not sure if this necessary

# DNS
Domain Name Server - performs resolution of Hostnames into IP addresses.  
Works on UDP protocol.  
Belongs to Application Layer of TCP\IP stack model.   

## Name Server cache
Routers and other machines can intercept Name Server Requests and send back 'cache'. This is to decrease traffic.

__Daemons:__
- dnsmasq
- nscd
- BIND - standard UNIX Name server which could act as cache

# Networking
https://www.youtube.com/watch?v=PVOuff-p420 network troubleshooting and related videos on the channel
https://www.youtube.com/watch?v=l0QGLMwR-lY Top 10 Linux Job Interview Questions

## Networking relies on configuration files:

### dhclient:
DHCP client 
InterFace configuration:  
`dhclient eth0`  
Files:  
- /var/run/dhclient.pid - file with PID of dhclient
- /var/state/dhclient.lease - file with IP address lease information

### DHCP Server
Linux machine could be DHCP server aswell - assuming it is used as a Default Gateway by the clients    
to enable forwarding (routes see in `route -n`):
```
sysctl -w #net.ipv4.ip_forward
```
To make it permanent save it into config file (not .bashrc lol):
```
# Add this to the main config file /etc/sysctl.conf:
et.ipv4.ip_forward=1

-OR-
# inspect README in here, and add config line to a file in the dir
/etc/sysctl.d/
```


### ifcfg-rh:
Main Network Manager plugin for Redhat-like systems  
/etc/sysconfig/network-scripts/ifcfg-<interfase_name> - this file describes 
	how network interface works, every interface has its own file.
	Settings of the interface are stored in here
	 Example:
	DEVICE="eth0"
	BOOTPROTO="dhcp"
	HWADDR="00:0C:29:3B:44:D3"
	IPV6INIT="yes"
	NM_CONTROLLED="yes"
	ONBOOT="yes"
	TYPE="Ethernet"
	UUID="951b47f7-e9c5-4e64-85ce-44c79732d914"
/etc/sysconfig/network - general network configuration, probably
	 Example:
	NETWORKING=yes
	HOSTNAME=centos7
	GATEWAY=10.0.0.1

### ifupdown:
Main Network Manager plugin for Debian-like systems  

`/etc/resolv.conf` - store default DNS here
Also has list of domains(`search`) to look for when 'incomplete' hostname is used - i.e. 'seasc' from seasc.usa.int would work if 'usa.int' is in resolv.conf  
If `127.0.0.1` is in resolv.conf it means that local machine has Name Server cache set up (see dnsmasq ) 
	 Example:  
  ```
  search usa.int
	nameserver 8.8.8.8
  ```
 see this for details on resolving Networking issue on VMWare:
 https://superuser.com/questions/901672/centos-7-ping-8-8-8-8-connect-network-is-unreachable/901677

glibc is called to make ns lookup, it has config here:
`/etc/nsswitch.conf` - Name Server Switch.  
- `hosts` - entry is for Hostnames resolution
  - `files` is set first - to check /etc/hosts first
  - `dns` is set afterwards to setup dns check (resolv.conf)

## ping:
uses `Internet Control Message Protocol (ICMP)` and `Domain Name Service (DNS)` system.  
`ICMP` i.e. it's required to correct MTU (maximum transmission unit) from default 1500 if network requires less MTU. If `ICMP` is disable MTU adjustment need to be done manually through `iptables` __mangle__ table, [read more](https://serverfault.com/questions/467756/what-is-the-mangle-table-in-iptables):  
```
iptables -t mangle -A FORWARD -p tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 1452
```

ping sends `ICMP` echo packet(56 bytes) to a Host, asking it to return the echo packet. If remote Host gets echo and configured to return echo packets it will send it back to `ping` issuer, packet would be 56+8 bytes by default (`64`)

Example:
```
$ ping 192.168.1.1
PING 192.168.1.1 (192.168.1.1) 56(84) bytes of data.
64 bytes from 192.168.1.1: icmp_seq=1 ttl=64 time=0.340 ms
64 bytes from 192.168.1.1: icmp_seq=2 ttl=64 time=0.373 ms
64 bytes from 192.168.1.1: icmp_seq=3 ttl=64 time=0.375 ms
^C
--- 192.168.1.1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2050ms
rtt min/avg/max/mdev = 0.340/0.362/0.375/0.027 ms
```
`icmp_seq` incremented sequence number (should numbered correctly, incorrect numberig is a sigh of bad connection - lost or slow )  
`time` roundtrip time (from when packet leaves and until packet returns). `host unreachable` ICMP packet returned if can't reach the destinatino  
`64` bytes are `56` sent and `56+8` returned

## traceroute:
also `ICMP` based(Windoes) or UDP(others) based, sends 1 or bunch of packets in parallel to every `hop` on the way to target host

Could show Several IP addresses for 1 Hop -  if 2+ routers could server the path to next Hop.
```
 7  google.cr-2.g50.kiev.volia.net (82.144.192.137)  7.522 ms  7.389 ms  7.354 ms
 8  108.170.248.130 (108.170.248.130)  17.181 ms 108.170.248.146 (108.170.248.146)  7.683 ms 108.170.248.130 (108.170.248.130)  9.053 ms
```

Use Case Example:
```
# send only 1 packet - kinda single-threaded, with 0.5 second delay
# so less hops will ignore or filter out the packet
traceroute -N 1 -z 0.5
```

## tcptraceroute:
  traceroute to port 
   Example:
  sudo tcptraceroute teamcity.tideworks.com 443

## ip: 
  modern substitution for `ifconfig`, `route`, `arp` etc
  ### ip link:
  creates network interface with name and type:
	also could be used if 'ifconfig' is absent
	Example:
     ip link add br10 type bridge - will create network interface with 
	name br10 and of type bridge
   add - add interface
   set - sets interface, somehow
	ip link set br10 up - will enable network interface br10


  ### ip addr:
  manages ip addresses. 
	in case 'ifconfig' is not available this comes handy
   add - adds gateway ip and subnet mask for device with a name
	Example:
     addr add 10.10.100.1/24 dev br10 - adds gateway with addres ...100.1 
	and with subnet mask 255.255.255.0 to the device called br10
	which is a network interface

  Add persistant network interface:
	/etc/network/interfaces
	auto <interface name>
	iface <name> inet static
		address 10.10.100.1	#main address
		netmask 255.255.255.0	#class C, like 10.10.100.1/24
		bridge_ports dummy0	#some mock stuff
		bridge_stp off
		bridge fd 0 		#frame 0

  ### ip route:
    substitution for `route`
    shows Kernel IP routing table - where packet should go whether to Gateway or inside the subnet, or to other subnet


## Routing work:
when Kernel need to send traffic via Internet it goes into `Kernel Routing Table`  
It matches the Subnet  Mask(Genmask) to the IP address of recepient  
If IP Address matched by more than 1 Subnet Mask then the one with longer destination prefix - bigger CIDR, since CIDR is 0-32 bytes long, the bigger is the one matched
Example:
```
  0.0.0.0         _gateway        0.0.0.0         UG    100    0        0 enp5s0
  192.168.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
  192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp5s0
```
255.255.0.0 - is CIDR /16 and in binary it's sixteen bits `11111111 11111111`  
255.255.255.0 - is CIDR /24 and in binary it's twentyfour bits `11111111 11111111 11111111`  

### Default Gateway
If Kernel need to send package to 192.168.1.15 both networks would be matched, but 192.168.1.0/24 has longer prefix(24 ones) than 192.168.0.0/16 which is only 16. so Traffic will go to __enp5s0__ network interface, which will be caufchg by `__gateway`

>Default gateway is the address under `Gateway` column of `route -n`  output.
All traffic _not matched_ by other `Routing Rules` in the `Kernel Routing Table` will go to the `Default Gateway`

## route:
  show and manipulate routing table
   Example:
  ```
  $ route
  Kernel IP routing table
  Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
  default         _gateway        0.0.0.0         UG    100    0        0 enp5s0
  link-local      0.0.0.0         255.255.0.0     U     1000   0        0 enp5s0
  172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
  192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp5s0
  ```
  `U` in Flags means __UP__  
  `G` in Flags means __Gateway__  
  __Params:__
  - -n - show numerical instead of symbolic names
  ```
  Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
  0.0.0.0         192.168.1.1     0.0.0.0         UG    100    0        0 enp5s0
  169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 enp5s0
  ```


## Ports checks
 read more:
 https://www.cyberciti.biz/faq/how-do-i-find-out-what-ports-are-listeningopen-on-my-linuxfreebsd-server/
 https://www.maketecheasier.com/check-open-ports-linux/

## /etc/services:
local list of Well Known Ports on linux system  
Online registry and List of ports, as well as Standartization document is located at iana.org, RFC6335

## Ephemeral Ports
## Dynamic Ports
list of ports which are generated for 1 session and used by TCP protocol.  
__Internet Assigned Numbers Authority(IANA):__  
- 0-1023 - Well Known Ports (0 is reserved so from 1)
  > Ports well known for all, to start Applicatoin or service listening on Well Known port `sudo` is required  
- 1024-49151 - Registered Ports 
  > List of ports that are used by custom software to work on
- 49152-65535 - Ephemeral or Dynamic Ports
  > Ports used by TCP protocol to establish the connection - Adopted by BSD, Windows

__Linux Kernel:__
- 1-1023 - only Superuser can start application to listen on those ports
- 32768-60999 - Ephemeral or Dynamic ports
  > Linux uses different set of Dynamic ports than IANA standard

## netstat:
  find open ports  
> List of Well Known Ports: /etc/services  
> IANA list: https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml  
> RFC6335
  Find port on machine:  https://www.tecmint.com/find-open-ports-in-linux/
  Use Case Example:
  ```
  # show opened ports - On which someone listening
  netstat -tulpn
  ```
  Use Case Example:
  ```
  # Check ports 9211, 8685 opened on local machine for #tcp\udp for IPv4 and IPv6:
   netstat -lntu | egrep "9211|8685"
  ```
  __Params:__
  - -l / --listen - displays ports that are in LISTEN state (lots of)  
  ```
  # opened ports and established TCP connections 
  netstat -vatn
  ```
   - v - verbose
   - a - All - show all sockets
   - t - TCP connections
   - u - UDP connections
   - n - numbers instead of DNS hostnames. Does not try to resolve DNS
   - p - show related PID and Program Name
  Example:
  ```
   #display used porta and established connections
   netstat -vat 
   #will display all ports for local machine
   sudo netstat -tulw 
  ```
  

## ss:
  netstat could be outdated on some systems, use it instead
   Example:
  sudo ss -tulwn - display all opened ports on local machine

## netcat:
## ncat:
  check, send network packets, port monitor etc

  Listen on port Example:
  will listen 9211 TCP port so could check connectivity by `telnet` from other machine
  ```  
  ncat -l 9211
  ```

## fuser:
 identify processes using files or sockets
similar to lsof | grep /loced/file/path

Example:  
```
sudo fuser -v /var/cache/debconf/config.dat
```



## lsof:
  List Opened Files, lots of them.
  Use `sudo` or become 
  Columns:
  - __COMMAND__ - command name of for the process that holds the file
  - __FD__ - Purpose of the file or File Descriptor  
    >__File Descriptor__ - Number that Process uses together with the system libraries and kernel to identify and manipulate a file.
  - __TYPE__ - Type of file REGular file, directory, socket etc.
  - __DEVICE__ - Major and minor number of the device that holds the file
  - __NODE__ - file's inode number
  also checks ports, could display processes that are using
   port, and files used by the process  

  Example:  
  ```
  sudo lsof | grep /var/cache/debconf/config.dat
  ```

   Params:
  - /path/to/file - lists everyone who uses file or DIrectory
  - -p <PID> - list open files for PID
  - -i - displays processes and ports used, and connection
 	with friendly names
    - 4 - ipv4
    - tcp - display tcp (cant use 4 and tcp in the same time, it seems)

## telnet:
Secureless connection over the network  
Works only on TCP. For other protocols look at `netcat` 
  check ports open  
  telnet <hostname/ip> <port>  
   Example:  
  ```
  telnet lwpeartifabld.tideworks.com 5000
  ```

Example of HTTPconnecton:
```
telnet www.wikipedia.org 80
GET / HTTP/1.0
```

  
## nmap:
  Network Mapped, detects open ports on my system
   -s - Scan, there are lots of it
    -sT - scan TCP
    -sn - disable port scan, just 'ping'
    -stuff, see 'man nmap'
   -O - OS detection
   -T - TCP
   -U - UDP
    Example:
   sudo nmap -sT -O 192.168.42.74  - scan remote machine for opened ports
   sudo nmap -sT -O localhost - scan local machine
    Example:
   nmap artifactory-dev.tideworks.com -p 5000
     check port 5000 opened on 'artifactory-dev.tideworks.com' machine

  NOTE: 
   Read MAN - it has examples and lots of stuff:
    scanme.nmap.org/24 - will scan 256 IP in C network class of
	scanme network - CRAZY!!111

  Example:
   nmap -sT -O localhost    - [s]cans [T]CP on localhost
 	
## nslookup:
  looks for domain names by given IP
  uses /etc/resolv.conf  - where all the DNS servers are
   resolv.conf:
  domain twlab.int
  search customer.int usa.int tideworks.com
  nameserver 172.19.65.2
  nameserver 172.19.65.1
  nameserver 10.253.1.1
  nameserver 10.253.1.3
   Example:
  noslookup $(hostname -I)   - dns entry if only 1 if

## host:
could resolve `hostname` into `IP` and sometimes other way around  
IP>hostname resolution requires 'reverse lookup' set manually on DNS - wut da fock is that?

# Security

## pass:
password manager kind of
https://www.passwordstore.org/#download - see docs here
### Initialize simple store
 `pass init gpg-id@email.com`
### Initialize git store
 `pass git init`  
  All contents of simple store will be added as 'Initial commit'
### Sign the commits with the login gpg key
 `pass git config --bool --add pass.signcommits true`
 all commits will be signed using user.signingkey or the default git signing key
### List passwords
 `pass`
### Find existing passwords
 `pass find .com`  
 `pass search .com`  
 Find existing passwords that match .com
### Add password
 `pass insert Directory_name/name_for_password` name could be website or something else  
### Add multiline password
 `pass insert -m Directory_name/name_for_password` - multiline, for detailed stuff
### Generate password
 `pass generate Dir_name/pass_name <pass_length_int>`
### Show password
 `pass Directory_name/name_for_password`
### Copy password into clipboard
 `pass -c Directory_name/name_for_password`


## LSM: linux security module:
 https://www.kernel.org/doc/html/latest/admin-guide/LSM/index.html

Primary user of LSM is Mandatory Access Control(MAC) extensions

LSM loads a bit earlier than UserSpace, and controls user
iteraction with system through policies.
LSMs exist to apply policies to actions taken by user space, so 
as long as the LSM infrastructure is running by the time user 
space starts, everything is fine.
Lockdown, though, must act earlier: it needs to be able to 
block the action of certain types of command-line parameters and 
must be functional even before a security policy can be loaded. 
So the patch set starts by creating a new type of "early security 
module" that is initialized toward the beginning of the boot process. 
At this point, the module can't do much — even basic amenities like 
kmalloc() are not available — but it's enough to register its 
hooks and take control.



 Loaded modules - MAC Extensions:
 cat /sys/kernel/security/lsm
    <Ordered list of loaded modules>
   capability - loaded always
   then 'minor' modules
    yama
   last 'major' module, ONE:
    SELinux - OEL(RHEL?) OEL extension , might be custom
    AppArmor - Ubuntu(Debian?) MAC Extension default
     https://www.kernel.org/doc/html/latest/admin-guide/LSM/apparmor.html
    Smack
    Tomoyo
  Process attributes for MAJOR module location:
   /proc/.../attr
    Subfolder with Module Name could be there
    This is LEGACY for modules that provide subdirectories

AppArmor: apparmor:
centered around Tasks and Profiles for tasks
Profiles are attached to Tasks
Tasks w/o profile controlled only be regular permissions wrx (DAC)

If AppArmor is not the default security module it can be enabled by passing 
security=apparmor on the kernel’s command line.
If AppArmor is the default security module it can be disabled by passing 
apparmor=0, security=XXXX (where XXXX is valid security module), on the 
kernel’s command line.
For AppArmor to enforce any restrictions beyond standard Linux DAC 
permissions policy must be loaded into the kernel from user space 
(see the Documentation and tools links).

  Enable\disable:
  set CONFIG_SECURITY_APPARMOR=y


  Set default:
  CONFIG_DEFAULT_SECURITY="apparmor"
  CONFIG_SECURITY_APPARMOR_BOOTPARAM_VALUE=1

Wiki - http://wiki.apparmor.net
User space tools - https://gitlab.com/apparmor

## Lockdown:
  https://lwn.net/Articles/791863/
   Together with UEFI locks who can access kernel
   only Private Key owner with pair in UEFI
   or certificate?

  Lockdown is loaded much earlier than LSM and User Space, to prevent
tricky cmd params from doing anything
  Lockdown adds hooks at early-boot, throughh which operates 

There are two lockdown modes. Confidentiality mode prevents user-land processes from extracting confidential information from the kernel. The other mode, Integrity, allows the kernel to switch off features that would allow user-land processes to modify the running kernel. Both of these modes even prevent processes launched by the root user or anyone with sudo privileges from modifying the kernel.

# SELinux: selinux:

## Statuses:
1. Enforced : Actions contrary to the policy are blocked and a corresponding event is logged in the audit log.
2. Permissive : Permissive mode loads the SELinux software, but doesn’t enforce the rules, only logging is performed.
3. Disabled : The SELinux is disabled entirely.

## Read about status change and check:
 https://www.thegeekdiary.com/how-to-check-whether-selinux-is-enabled-or-disabled/

## getenforce:
  check status of selinux
   Example:
  # getenforce
  Permissive

## setenforce:
  change enforce TEMPORARY
   Example:
  # setenforce 0
  # getenforce
  Permissive

  change enforce PERMANENTLY
  cat /etc/selinux/config
  SELINUX=disabled  # to disable

## setstatus:
  more detailed current status output

 change to permissive and configuration info:
 https://www.thegeekdiary.com/how-to-disable-or-set-selinux-to-permissive-mode/


## dot in permisisons:
drwxr-xr-x.    < this dot(.) means file has Access Control List(ACL) with SELinux.
 this will not appear if SELinux is disabled

## setfacl:
 will or unset ACL for files\directories etc.  (the dot at the end of permisisons)
 https://stackoverflow.com/questions/30594871/what-does-the-dot-at-the-end-of-the-permissions-in-the-output-of-ls-lah-mean



# Virtualization general
Virtual box has several different network interfaces

NAT and NATNet networks would not be accessible from outside(and HOST)
so to SSH into virtual machine, gotta use Port Forwarding 
  and connecto to localhost:<forwarded_port>
 table:
+-----------+-------------+-------------+----------------+----------------+
|           | VM <-> Host | VM1 <-> VM2 | VM -> Internet | VM <- Internet |
+-----------+-------------+-------------+----------------+----------------+
| HostOnly  |     Yes     |     Yes     |      No        |       No       |
| Internal  |     No      |     Yes     |      No        |       No       |
| Bridged   |     Yes     |     Yes     |      Yes       |       Yes      |
| NAT       |     No      |     No      |      Yes       |  Port forward  |
| NATNet    |     No      |     Yes     |      Yes       |  Port forward  |
+-----------+-------------+-------------+----------------+----------------+
Bridged will allow your guest to appear as just another PC on your host's network. The host, all the other network PCs, the internet and the guest can all communicate. The guest would default to getting an IP address from your host network's router. The host needs to be connected to an active network to allow guests to use Bridged. And Bridged is not always compatible with Wi-Fi.

Internal makes a private network just for your guests. There is no host communication or internet. There is no DCHP server on an internal network by default, though you can put one on in Virtualbox. Otherwise configure static IP address for the guests within the guest OS's. Or you can make a router guest using a router OS like pfSense. You can make more than one internal network by naming the new networks differently.

Host-Only is a special form of internal network that also includes the host, via a virtual Host-Only network adapter made on your host, defaulting to IP address 192.168.56.1, and there is a default DHCP server handing out IP addresses to the guests starting at 192.168.56.101. You can change the IP address range or make multiple host-only networks in Virtualbox's Network settings in the main GUI. There is no internet in Host-Only.

Virtualbox has two kinds of NAT: regular NAT and a NAT network service. Regular NAT ("NAT" in the dropdown) connects each guest to the host's network connection and internet via an independent channel. The host and any other guests cannot connect to the NAT-connected guest, although ports can be opened, just like NAT in a router. (The NAT guest can, however, see any services and shared folders on the host's network.)

The NAT network service ("NAT network" in the dropdown) is like a home router, NAT to the outside world and multiple "LAN" guest connections. All the guests connected to this "NAT network" can see and communicate with each other. Internet is accessible. Ports can be opened just like a real router. You need to make a new NAT network in the main Virtualbox window, File Menu, Preferences, Network. Then attach the guests using the name of the NAT network you made in Preferences.


===GUI
GUI on Centos7
 https://unix.stackexchange.com/questions/181503/how-to-install-desktop-environments-on-centos-7

GNOME install on Centos7
yum group list
  check groups available
 "Server with GUI" - this for RHEL
 "GNOME Desktop" this is GNOME for Centos
 "KDE Plasma Workspaces"  - this is KDE
 "Graphical Administration Tools" - extra admin tools for GUI
yum groupinstall "GNOME Desktop" "Graphical Administration Tools"
  this will install two groups for Gnome and admin tools
  Read more:
  https://www.itzgeek.com/how-tos/linux/centos-how-tos/install-gnome-gui-on-centos-7-rhel-7.html

## gnome:
## gnome-control-center:
  calls GUI of main settings window
  gnome version could be viewed somewhere here
  also version:
  /usr/share/gnome/gnome-version.xml

## gnome-tweaks:
  calls GUI of advanced? settings window


# utilities:
## clamav:
## clamscan:
      ClamAV – A command line utility: It is a free, open source, and cross-platform antivirus toolkit for detecting many types of malicious software and viruses.
    ClamTK – A Graphical utility: ClamTk is a graphical front-end for the Clam Antivirus. It is designed to be an easy-to-use, lightweight, on-demand antivirus scanner for Linux systems.
   Install:
  sudo apt-get install clamav clamav-daemon
   Execute scan:
   Example:
  clamscan --invected --log=clamscan.log --recursive --copy=infected_files --detect-pua=yes /

## xxd:
  converts files to HEX
  Useful when 2 same files displayed as different
  Convert to HEX, find differences - those could be spaces, use dos2unix to fix
  

## dos2unix:
  line ending converter


# Kernel

## Out Of Memory handling
## Kernel OOM Killer
## Linux OOM Killer

https://www.kernel.org/doc/gorman/html/understand/understand016.html  
https://lwn.net/Articles/317814/

Flow:

1. When Heap expanding (`brk()`)or memory is reallocated (`mremap()`) first System verifies that there is enough Memory available - `vm_enough_memory()` is called.  
- `vm_enough_memory()` accepts param on number of memory Pages Required.  
- Free pages; Cached pages; Various Swap pages are summed up and considered Available pages - `vm_enough_memory()` returns `true`
- When Available pages are less than Pages Required function retuns `false` - so caller will know that there is no memory, and usually returns `-ENOMEM` which is [System Error](https://www-numi.fnal.gov/offline_software/srt_public_context/WebDocs/Errors/unix_system_errors.html)

2. System tries to cleanup. GC probably is launched and old pages are being reclaimed.  
If system is unable to free required Pages it calls `out_of_memory()`

3. When called `out_of_memory()` it ensures that there is in fact no memory available.  
It also makes various checks, all to ensure that memory potentially will not be available in near future.  
At last stand it kills a process.  
Following checks are made before look for a kill:  
    - Is there enough swap space left (nr_swap_pages > 0) ? If yes, not OOM
    - Has it been more than 5 seconds since the last failure? If yes, not OOM
    - Have we failed within the last second? If no, not OOM
    - If there hasn't been 10 failures at least in the last 5 seconds, we're not OOM
    - Has a process been killed within the last 5 seconds? If yes, not OOM 

4. `oom_kill()` is called to select process to kill  
it calls `select_bad_process()` which calculates `badness` with `badness()` function:
- Check amount of memory used (memory)
- Checks how many CPU time it uses - cpu time in seconds (cpu_time)
- Checks how long process lived - cpu time in minutes (life_lengh)  

Simplified example of `badness`:  
`badness = memory / (cpu_time * life_length)`
It will try  **to select a process that is using a large amount of memory but is not that long lived.**

>Processes which have been running a long time are unlikely to be the cause of memory shortage so this calculation is likely to select a process that uses a lot of memory but has not been running long. If the process is a root process or has CAP_SYS_ADMIN capabilities, the points are divided by four as it is assumed that root privilege processes are well behaved. Similarly, if it has CAP_SYS_RAWIO capabilities (access to raw devices) privileges, the points are further divided by 4 as it is undesirable to kill a process that has direct access to hardware.

5. Killing the process  
When task is selected(to be killed), list of processes is walked again to check who else shares same memory as selected process(task) - which means a thread(processes could be divided into threads): 
   - If process(thread) is using RAWIO it receives `SIGTERM`to try gracefully release all raw input\output.  
   - If process(thread) is not using RAWIO it receives `SIGKILL`.

### find which process killed by Kernel OOM Killer

https://stackoverflow.com/questions/624857/finding-which-process-was-killed-by-linux-oom-killer  
grep -i 'killed process' /var/log/messages

# cgroups
control groups  
limits and isolates accounts for resource usage (CPU, RAM, disk I/O, network etc.)

Also controls Swapping of memory, which is , seems like, essential for docker capability to limit memory and CPU cores for container usage.

## Add cgroups options:
to allow `docker` use `--cpus` and `--memory` CGROUPS SWAPPING should be enabled. Otherwise docker will generate error:
> docker run --cpus 1 -m 200m --rm -it busybox  
WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.

=====================
> NOTE: ALWAYS BACKUB `GRUB` file before editing  

=====================  
change `grub` configuration  
```
vim /etc/default/grub
# add follosing into the variable:
GRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1"

# update grub
sudo upgate-grub

# restart the Docker Host:
sudo reboot
```

now `docker` would work with CPU and Memory limitations. To check on host machine execute:
```
docker run --cpus 1 -m 200m --rm -it busybox   

# to check CPU used by a limited by CPU or Memory container
cat /sys/fs/cgroup/cpu/docker/534c...3a6/cpu.{shares,cfs_*}
>1024    # cpu shares (default value)
>100000  # cfs_period_us (100ms period length) - _us is Microseconds(aka ms)
>100000  # cfs_quota_us (total CPU time consumable per cfs_period) - THIS IS AMOUNT OF CPU from --cpus


# to check memory used
cat /sys/fs/cgroup/memory/docker/534c...3a6/memory.limit_in_bytes 
> 209715200

```

## LKM
Loadable Kernel Modules  
Could be FOSS or proprietary, 3rd party, or closed sources(pre-compiled) modules also exists.

Linux Kernel is monolithic, but with loadable modules it could be totally changed w/o performing the reboot.

Kernel Modules loads:  
- devices (like drivers and stuff)
- system calls (like API for stuff)
- filesystems (fs support could be loaded at runtime)

Live ISO CD disks could work because of the LKM - Kernel loads required stuff (drivers, syscalls and fs support) at runtime.

### LKM and GRUB(Boot Loaders)
`/boot/config` file contains list of modules to load or NOT load  
Example:  
`/boot/config-4.15.0-117-generic`

### modprobe:
loads and unloads Linux Kernel Modules

## Memory Management
Kernel manages memory, take care of it and keepin it clean

### Types of memory:  
- __Physical Memory__  
is the RAM

- __Swap__  
parts of Hard Drive which are used as the Memory by Kernel. There are many various Swap stuff, like /dev/shm partition used by Oracle DB , which sould be same or above than SGA (which is sum of two other types of ORacle memory. i hate Oracle.)

- __Virtual Memory__  
Abstration. Virtual representation of real memory (RAM and\or Swap) given to a Process so it sees it as a real memory.  
Similar to CPU units having some amount of own RAM to have Memory Table pointing to real memory to fool a Process that it owns whole Computer's RAM.


## Kernel Network Stack
it has 7 layers (not to be confused with 7 TCP\IP Network Layers):  
- 1th is in _User Space_: Application layer (i.e. Web Browser)
- 2-6th are in _Kernel Space_: System Call Interface
- 2-6th are in _Kernel Space_: Protocol Agnostic Interface 
- 2-6th are in _Kernel Space_: Network Protocol
- 2-6th are in _Kernel Space_: Device Agnostic Interface
- 2-6th are in _Kernel Space_: Device Driver
- 7th: Physical Hardware (i.e. Physical Network Device)

## Process management
There are two types of processes around:  
- Foreground Processes or Interactive Processes.  
  processes started by a User connected to the System, which did not started automatically as some Service
- Background Processes or Non-Interactive Processes.  
  processes started by the System as a Service or something similar.  
  such processes does not expect any User input.
  usually started by Init system (i.e. systemd), User can control such processes via Init system configuration or [command line] interfaces

Processes could have Parents and Children.  
All the processes are the children(or anscestors) of process with ID `1`. which is usually an `init` process (except Docker, it is complicated there).

### Process States
- __Running__ - running or ready to be run
- __Waiting__ - waiting for an Event or a Resource
- __Stopped__ - usually stopped by SIGnal 
- __Zombie__ - the process is Dead, but not completely (further research required)

### Threads
Processes could be divided to Threads.  
Threads scheduled and run similar to processes by the Kernel. Threads has own ID - TID - Thread ID  
ALl `Threads` inside of a single Process sharetheir system resources and memory
Due to shared resources and memory inside the Process the Threads could work faster, it also faster to spawn new Thread when some new input or output stream appears than to spaws new Process.

## Virtual FileSystem
Consists of:
- System Call Interface
  - Virtual File System (VFS)  
  abstract layer over the Physical device (i.e. Hard Drive)
    - various filesystems support (ext4, ntfs u name it)
  - Buffer Cache  
  filesystem agnostic cache storage for data before it is written\read to-from disk
- Device Drivers  
  which are the Interface for particular Physical Device (since those are all has it's own interface unknown to Kernel until Driver comes to rescue)

## System Call Interface
Well defined and safe implementation to give access for User Space Processes to perform variety of tasks - such as work with storage and networking devices, CPI, Memory and all th eother stuff around
Interface for POSIX Compliang and Linux Specific Applications to access Kernel functions

# User Space

## PAM
__Pluggable Authentication Modules__  
made by Sun Microsystems to extend Authentication and stuff  
Configuration: `/etc/pam.d/` and `/etc/pam.conf`  
Configurations consist of Rules (could `@include` other configs)  
Rules in config files __stack__

Example:
```
auth       required   pam_shells.so
```
- __Function Type__ - i.e. `auth` function that user app asks PAM to perform. _Global Goal_
- __Control Argument__ - i.e. `required` controls what PAM does _after_ success or failure of the `Action` for current line
- __Module__ - the module to run on the line

### Function Types
`Function Type` and `Module` forms the `Action`.
Is a _high-end_ gol - what user application wants PAM to do.  
Action is a specific step that PAM takes in order to reach that goal.  
Function is executed first, than Action is made by PAM.

PAM has 4 functions:
- __auth__ - Auth the user (see if the user is who they say they are)
- __account__ - check user acc status (i.e. is authorized to do something)
- __session__ - perform something for current user's session only
- __password__ - change password or other creds

Module behavior could differ with different actions.  
Example:  
_pam_unix.so_ checks for password with __auth__ Function Type  
_pam_unix.so_ sets password with __password__ Function Type  

## Control Arguments
Rules in configuration files stack, so all the rules in config file would run one-after-another.  
Control Arguments decide _how_ the Rules run.

### Simple Control Arguments

3 major simple Control Arguments:
- __sufficient__ - On __Success__ exits skipping other rules. On __Failure__ proceeds to other rules.
- __requisite__ - On __Success__ proceeds to other rules. On __Failure__ exits skipping other rules.
- __required__ - On __Success__ proceeds to other rules. On __Failure__ proceeds to other rules, but will return `FAIL` at the end anyway.

Example:
```
# cheks if current shell is in /etc/shells, if not - fail anyway
auth        required    pam_shells.so
# checks if user is root - if yes exit immediately with success
auth        sufficient  pam_rootok.so
# include other files from /etc/pam.d directory
@include common-auth
@include common-account
@include common-session
```

### Advanced Control Arguments
there are some.

## Modules
Module stores the code which is getting executed according to `Function Type` specified.  

List of known modules:
- _pam_unix.so_ - checks(__auth__) or sets(__password__) password
  - _nullok_ - return `success` if password is null
  - _obscure_ - check if password to set is obscure enough(not simple, no tlike old passwors, etc)
  - _sha512_ - algorythm to encrypt new password  
  Example:
    ```     
    password  sufficient  pam_unix.so obscure sha512
    ```
- _pam_rootok.so_ - checks if user is `root`
- _pam_shells.so_ - checks if current shell in `/etc/shells`
- _pam_deny.so_ - always return `failure`

Modules could take parameters.  
Example:
```
# do not Fail if user password is null
auth  sufficitnt  pam_unix.so nullok
```
