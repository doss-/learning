https://www.youtube.com/user/JeffreyMRichter/videos?view=0&sort=dd&shelf_id=1

# DNS

[Guide](https://dyn.com/blog/dns-why-its-important-how-it-works/)

Domain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.  

Also it is a directory of crucial information about domain names such as:  

- email servers (MX records)  
- sending verification (DKIM, SPF, DMARC)  
- TXT record verification of domain ownership  
- SSH fingerprints (SSHFP)

Intelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.

## Work principles

Computer sends requests every time it uses a domain address (example.com).  
This happens every time domain name is used.  
For any reason - web surfing, email, internet radio, API calls etc.

### Steps: In depth look for query

1. Local DNS cache. 

  Local computers(TODO: ensure), has [DNS cache](https://dyn.com/dyn-tech-everything-you-ever-wanted-to-know-about-ttls/), where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs [DNS query](https://dyn.com/blog/understanding-how-best-to-find-qps-metrics-for-your-managed-dns-account/).

2. Recursive DNS server. Resolver.

Usually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed.  
Resolvers also has their own cache, which is examined for query sent.  
If hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.

3. Root Name server. (.)

Resolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here.  
The Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution.  
The Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities.  
Root Name servers are managed by [13 companies](http://root-servers.org/) such as ICANN or NASA.  

4. Top Level Domain Name server. (.com. .us. .ua.)

Root server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers).  
TLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like __example__.com).  

5. Authoritative DNS servers (Name Servers)

Authoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.
A Record is what we need, this will be returned.

6. Retreive the record

recursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.

7. Receive the anwser

A record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.

# MAC addresses:

https://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/
http://www.linfo.org/mac_address.html

MAC is the uniq address of Network Interface Card.  

MAC is lowest level in networking, below IP protocil.  

MAC is used to send network packages, it is located in package header.  

NIC will accept only those packages whose header contains NIC's MAC(matches).  

Routing is done on IP level.

With MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.

##Steps:

WHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer.   
Answer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)

If IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.

Router receives packets for its own MAC, and for different(not router's) IP.  
Then checks whether he can directly reach the target IP, if not - passes to another router(hop)

# NAT  (Network Address Translation)

http://www.internet-computer-security.com/Firewall/NAT.html

There are only 32 bits of addresses or 4 billions in IPv4.  
To workaround it NAT is created, it introduces Private IP addresses.  

## Private IP addresses

there are 3 classes:

Class A: 10.0.0.0 - 10.255.255.255

Class B: 172.16.0.0 - 172.31.255.255

Class C: 192.168.0.0 - 192.168.255.255 (or CIDR block [192.168.0.0/16](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks) )

Within the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks.  
However as long as they stay private no conflicts appears.  

Routers are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique.  
Routers are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.

## NAT types:

### Static NAT

NAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.

### Dynamic NAT

same as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.

### Port Address Translation (PAT)

[video](https://www.youtube.com/watch?v=QBqPzHEDzvo)

Private address connects to a outer address with for instance:

    192.168.0.3:40213 connects to 40.30.20.10:80

Router substracts private ip and port and puts it in Routing Table  
and adds its own IP and another random port, changing the connection to:

    12.13.14.15:50098 connects to 40.30.20.10:80

Server(or another same NAT router) receives connection, and answers to the Router:

    40.30.20.10:80 answers to 12.13.14.15:50098

Then Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port

    40.30.20.10:80 answers to 192.168.0.3:40213

# Subnet Mask

https://www.iplocation.net/subnet-mask  
https://en.wikipedia.org/wiki/Subnetwork  

calculator http://jodies.de/ipcalc

# Proxy

https://www.youtube.com/watch?v=0OukrSld3sY

![image](cheatsheets/docs/img/forward_proxy_vs_reverse_proxy.jpg)

## Forward Proxy

Or just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers.  
See p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.

Usual use:
  - Content filtering:
> censoring some addresses or resources(prevents traffic to coming in)
> translation some content before shipping it to computers inside network  
  - Caching (see p4 proxy)
  - logging, monitoring  - what comes in, what comes out, etc.
  - anonimization  
> because not the computer bu proxy connects to server, it could ship different information

## Reverse Proxy

Processes Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.

Usual use:
  - __Stable client endpoint__ whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale
  - __Load balancing__: Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing
  - __Load balancing__: server selection like Green\Blue schamas or A\B testing
  - __SSL Termination__ - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http
  - __Caching__ - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)
  - __Authentication/validation__ - all the auth is done on the Proxy so no further auth is required.
  - __Tenant throttling/billing__ - deny connection\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\frequency of requests done to the servers.
  - __DDoS mitigation__ - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)

Reverse proxy examples:

Webservers (such as Nginx)  
Load Balancers  
API Gateways  

# Microservices

Monolith split into little parts which are independent one from each other.
Microservices have its own:  
 - Data storage, and exclusive access to it for read and write
 - Technology stack, so different services could be written in different languages and use different storages and stuff  
 
Why to split:
 - Different technology stacks could be used for different services  
 - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure  
 - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)
 - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes
 - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services


# Autoscaling instances

One of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\app) running at the same time.

Autoscaling is performed automatically by 'orchestrator' - some cloud tool  
There are couple of ways to autoscale:  
 - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.
 - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway).   
Load Balancer also could be used to make sure all the instances getting equialent load amount.
 - Scheduled: just some schedule related to day\night, weekdays\weekends\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available
