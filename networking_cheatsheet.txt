https://www.youtube.com/user/JeffreyMRichter/videos?view=0&sort=dd&shelf_id=1
https://www.youtube.com/watch?v=V2SpN-OePzc - network intro course on CBT Nuggets

# DNS

[Guide](https://dyn.com/blog/dns-why-its-important-how-it-works/)

Domain Name System - resolves human readable hostnames, such as www.example.com, into machine readable IP(Internet Protocol) addresses like 50.16.85.183.  

Also it is a directory of crucial information about domain names such as:  

- email servers (MX records)  
- sending verification (DKIM, SPF, DMARC)  
- TXT record verification of domain ownership  
- SSH fingerprints (SSHFP)

Intelligent DNS could even do something as Load Balancing - it could decide which IP(s) are returned after resolve.

## Work principles

Computer sends requests every time it uses a domain address (example.com).  
This happens every time domain name is used.  
For any reason - web surfing, email, internet radio, API calls etc.

### Simplex
One way communication from `Sender` to `Receiver`  

### Half-Duplex
Two way communication from `Sender` to `Receiver`  
But only __one at a time__

### Full-Duplex
Two way communication from `Sender` to `Receiver`  
__At the same time__

### Steps: In depth look for query

1. Local DNS cache. 

  Local computers(TODO: ensure), has [DNS cache](https://dyn.com/dyn-tech-everything-you-ever-wanted-to-know-about-ttls/), where request could find hostname-IP resolution and this will be enough. If there is no local cache computer performs [DNS query](https://dyn.com/blog/understanding-how-best-to-find-qps-metrics-for-your-managed-dns-account/).

2. Recursive DNS server. Resolver.

Usually resolvers are somewhere in ISP's(internet service provider) network, but there could be even local resolver installed.  
Resolvers also has their own cache, which is examined for query sent.  
If hostname-IP is resolved(username checks out), this will be enough and results is sent back to the computer.

3. Root Name server. (.)

Resolver queries other resolvers(for cache) until it comes to the Root Name Servers, which are not cached already, so cache story ends here.  
The Root Name servers forward query to the regular Name Servers, which know the address of the hostname - thus are able to do the resolution.  
The Root name servers are located all around the world, this is kind of hubs on the traffic roads, usually Bunches located in huge cities like Capitals or just megacities.  
Root Name servers are managed by [13 companies](http://root-servers.org/) such as ICANN or NASA.  

4. Top Level Domain Name server. (.com. .us. .ua.)

Root server reads address from right to left, and depending on TLD in the address directs query to that Top Level Domain name server (Root NS knows addresses of all TLD Name servers).  
TLD NS knows reads the address queried, and reads next part like example in example.com, and this TLD NS knows address of every such domain (like __example__.com).  

5. Authoritative DNS servers (Name Servers)

Authoritative name servers know everything about specific domain - it is stored in DNS records, such as A Record, CNAME and stuff.
A Record is what we need, this will be returned.

6. Retreive the record

recursive servers save request result from Name Server in its cache for amount of time set in TTL, after TTL expires recursive server will ask again to make sure info is up to date.

7. Receive the anwser

A record is returned back to the original asking computer by the Recursive server, answer is stored in local cache, reads IP address and passes it to browser. Browser opens the connection to receive the website.

# Physicall Addressing(MAC addresses):

https://www.howtogeek.com/169540/what-exactly-is-a-mac-address-used-for/
http://www.linfo.org/mac_address.html
https://www.youtube.com/watch?v=V2SpN-OePzc

MAC is the uniq address of Network Interface Card.  
- 48 bit long
- 12 hexadecimal characters long
- first 6 are Manufacturer's uniq numbers given by IEEE (institute of electricity and engineers and another E..)
- last 6 are NIC's uniq numbers
- uniq 6 + 6 is to decrease the chance of having two same MAC addresses

MAC is lowest level in networking, below IP protocil.  
MAC is used to send network packages, it is located in package header.  

NIC will accept only those packages whose header contains NIC's MAC(matches).  

Routing is done on IP level.

With MAC packages only could go inside the same network, if need to go through the Router the IP need to be used, wrapping package with MAC address.

## Steps:

WHen computer wants to send a packet he checks whether target IP is in the same network. seems like ARP is used here, allows to ask all the computers in network (via Broadcast MAC ff:ff:ff:ff...) who has the IP address, only the computer with the IP will answer.   
Answer will also contain the MAC address. This MAC will be written in the package and sent to that IP address (next hop). Info will be cached (as in DNS resolvers, seems like)

If IP is out of network, router receives the package with router's MAC in header and target IP address(not router's). Then Router check whether he could reach given target IP, if not, he sends package to next hot(router), and everything is done again.

Router receives packets for its own MAC, and for different(not router's) IP.  
Then checks whether he can directly reach the target IP, if not - passes to another router(hop)

# Network Protocol
Network protocol `Internet Protocol`, second layer of socalled 'TCP/IP' stack. Or one of(which?) OSI networking model  
Other Network protocols are:
- IPX - Deprecated. Used by Novell back in 80s up till ~93 in `Netware` to communicate to other novell's machines, moved to `IP`
- APPLETALK - Deprecated. Macintosh used this protocol to other macs until moved to `IP`
- NETBIOS - IBM's and adopted by Windows network protocol, which is used for file-sharing

## NAT  (Network Address Translation)

http://www.internet-computer-security.com/Firewall/NAT.html

There are only 32 bits of addresses or 4 billions in IPv4.  
To workaround it NAT is created, it introduces Private IP addresses.  

## IP Address
Network protocol which allows computers on the Network to address each other.  
Consists of 4 Octets - Oct == 8, which number of `bits` in each octet  
```
192.168.1.10 == 11000000.10101000.00000001.00001010
NOTE: use 'bc' and 'obase=2; 192' to convert decimal to binary
```
### Logical Address and Host Address
`IP Address` is paired with `Subnet Mask`.
- __Logical Address__ is the similar to all hosts in the __Subnet__
  - __Subnet mask__ determines the __Logical Address__
- __Host Address__ is unique to each host  
  - What is left after __Subnet Mask__ is the __Host Address__

__Ranges:__
0-255 but `0` is reserved for networks and `255` for broadcast; so:  
1-254, which is 254 addresses (since it already includes 1)

Example:  
```
IP: 192.168.1.10
Subnet Mask: 255.255.255.0    <-- only last Octet could be for Host Address
CIDR: 192.168.1.0/24         <-- CIDR Notation - 24 binary ones before the first 0
Network Address: 192.168.1.0  <-- CIDR Network Address always ends on 0 (i.e. 192.168.0.0)
[192.168.1] - Logical Address
[.10] - Host Address          <-- there could be only 254 hosts on this subnet

```

`ARP`(`Address Resolution Protocol`) protocol used to get MAC for given IP address  


## Private IP addresses

there are 3 classes of Network Masks: https://www.youtube.com/watch?v=WX5GaH5ootc&list=PLdFppKg4UodjGN8nAhojxWyncoEdwTfqf&index=8

- __Class A:__ Mask: `255.0.0.0`; CIDR Notation: `/8`; 10.0.0.0/8; Range up to - 10.255.255.255
  - First bit of First octet must be `0`. 
  - so the decimal range of first Octet is between `0 to 126`:
    - 0 is `00000000`, 126 is `01111110`; where as 127 is `01111111`... seemsdue to inability to use decimal `0` as first Octet?
- __Class B:__ 172.16.0.0 - 172.31.255.255
  - First bit of First octet must be `1`
  - so the decimal range of first octet is between `128 to 191`:
    - 128 is `10000000`, 191 is `10111111`
- __Class C:__ 192.168.0.0 - 192.168.255.255 (or CIDR block [192.168.0.0/16](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#IPv4_CIDR_blocks) )
  - First bit of First octet myst be `11`
  - so the decimal range of first octet is between `192 to 223`
    - 192 is `11000000`, 223 is `11011111`, whereas 224 is `11100000`
- __Class D:__  [Special] Multicast
  - First octet: 224-239
- __Class E:__  [Special] Experimental
  - Fist octet: 240-255
```
#Calculate binary from decimal
$ bc
bc 1.07.1
Copyright 1991-1994, 1997, 1998, 2000, 2004, 2006, 2008, 2012-2017 Free Software Foundation, Inc.
This is free software with ABSOLUTELY NO WARRANTY.
For details type `warranty'. 
obase=2; 192
11000000

```

Within the private network - behind the Router(hop) the IP addresses would be unique. In another private network those addresses also would be unique, but could be the same in two different private networks.  
However as long as they stay private no conflicts appears.  

Routers are the Gateways with real IP address purchased from the ISP (internet service provider). And this purchased address would be Public and unique.  
Routers are the 'real' sources for other services in the internet, and those services would address their packets only to Router, and Router will then translate (using routing tables) and substitute info in the package to send it to original sender inside the private network.

## NAT types:

### Static NAT

NAT device has a pool of Public ip addresses, and devices in network are assigned those addresses when accessing outside world. Static addresses will be given to the device permanently, so even when it goes offline the address still can not be taken by other device. It is useful for servers.

### Dynamic NAT

same as above - pool of IP addresses which are given to the devices going outside, but when devices goes offline IP address is released and could be given to another device. When all the addresses are taken - new device could not be given any IP from the pool hence cant go to the internet.

### Port Address Translation (PAT)

[video](https://www.youtube.com/watch?v=QBqPzHEDzvo)

Private address connects to a outer address with for instance:

    192.168.0.3:40213 connects to 40.30.20.10:80

Router substracts private ip and port and puts it in Routing Table  
and adds its own IP and another random port, changing the connection to:

    12.13.14.15:50098 connects to 40.30.20.10:80

Server(or another same NAT router) receives connection, and answers to the Router:

    40.30.20.10:80 answers to 12.13.14.15:50098

Then Router takes info from routing table, where matches router_address:random_port to the entry, and takes second pair with private_address:random_port

    40.30.20.10:80 answers to 192.168.0.3:40213

# Subnet Mask

https://www.iplocation.net/subnet-mask  
https://en.wikipedia.org/wiki/Subnetwork  

calculator http://jodies.de/ipcalc

# Proxy

https://www.youtube.com/watch?v=0OukrSld3sY

![image](cheatsheets/docs/img/forward_proxy_vs_reverse_proxy.jpg)

## Forward Proxy

Or just Proxy, processes Outgoing requests. Computers in the network connects to it, and it connects to the servers.  
See p4 proxy as example, with its own address, port - it accepts connections and does everything on its own - checks whether it needs to look furhter or not etc.

Usual use:
  - Content filtering:
> censoring some addresses or resources(prevents traffic to coming in)
> translation some content before shipping it to computers inside network  
  - Caching (see p4 proxy)
  - logging, monitoring  - what comes in, what comes out, etc.
  - anonimization  
> because not the computer bu proxy connects to server, it could ship different information

## Reverse Proxy

Processes Incoming requests. Computers from outside world connects to the Proxy and not to the server itself, which is more secure, more stable(only 1 static address for outer world), more managable.

Usual use:
  - __Stable client endpoint__ whereas servers behind id could change addresses, cease to exist, turn on and off, scale and descale
  - __Load balancing__: Level 4 (udp/tcp traffic) & Level 8 (http traffic,with headers and other info for better balancing
  - __Load balancing__: server selection like Green\Blue schamas or A\B testing
  - __SSL Termination__ - when encryption ends on Proxy and internal traffic to servers and between internal servers is unencrypted and over http
  - __Caching__ - frequent requests could be cached w/o going to internal servers at all (during TTL ofc)
  - __Authentication/validation__ - all the auth is done on the Proxy so no further auth is required.
  - __Tenant throttling/billing__ - deny connection\answer to computers that made too many requests per second. Or bill such customers, if we are charging by the amount\frequency of requests done to the servers.
  - __DDoS mitigation__ - deny connection if suspicious about DDoS attack for some amount of time (1 hour or 1 day etc)

Reverse proxy examples:

Webservers (such as Nginx)  
Load Balancers  
API Gateways  

# Microservices

Monolith split into little parts which are independent one from each other.
Microservices have its own:  
 - Data storage, and exclusive access to it for read and write
 - Technology stack, so different services could be written in different languages and use different storages and stuff  
 
Why to split:
 - Different technology stacks could be used for different services  
 - Scalability - some services could be scaled to more instances than the others. Which means that less resources is needed comparing to split of monolith service, where some parts are overscaled and no need at the moment, but still scaled bcs of monolith structure  
 - Different services(clients) could use and depend on one single service (see it as inheritance, when different classes could be inherited from single base class)
 - Versioning, some services could be upgraded with new featurs, but as long as they are backwards compatible other services could still use them w/o any additional changes
 - Conflicting version dependencies of shared libs in monolith services could be solved by splitting this monolith in microservices so different shared libs goes independent on different services


# Autoscaling instances

One of the holy grails Cloud apps is Autoscaling, which means changing amount of instances(of same service\app) running at the same time.

Autoscaling is performed automatically by 'orchestrator' - some cloud tool  
There are couple of ways to autoscale:  
 - Periodical Queue check: It has Queue which is in front of services, if amount of items in queue growth above some threshold for some period of time, the 'orchestrator' starts to scale instances up. When items in queue start to go down, 'orchestrator' will scale instances down.
 - Periodical Resource usage check: Instances could have some metrics being monitored by 'orchestrator' like CPU or RAM or HDD etc usage, and when those instances start to heat, some usage goes up, 'orchestrator' could decide to spin up another instance(scale up), and vice versa when resources goes low, 'orchestrator' could scale down some extra instances(keep n+1 amount anyway).   
Load Balancer also could be used to make sure all the instances getting equialent load amount.
 - Scheduled: just some schedule related to day\night, weekdays\weekends\holidays etc. As a downside - scheduled manner could not always be up to real world, and sometimes there could be too many or too less instances available


# Messaging

Messaging communication [reactivemanifesto.org](reactivemanifesto.org)

Benefits over standard HTTP network calls (like GET/PUT/POST etc) which are converted from Method calls

Request\Reply pattern Downsides:

1. Network calls could be sent to a services(instance of a server) by load balancer, but particular service could already be busy doing work, and LB could not know about it.  
But there could be other server instances that are not busy (and LB does not know about it)

2. Client could go down - crash or scale down, while waiting for message request.  

Messaging benefits:

1. Resource efficient - messages are taken from the queue by instance of a server which is done wokring, so all the instances have work and not overwhelmed with it. 

2. Client Services are not waiting longer than it could with Request-Reply pattern. So less chance to crash scale down before the answer.

3. Client and Server are both talking to Queue service, which is always there and is solid - it wont go down crashing or descaling (actially it could be reverse proxy endpoint hiding queue brokers like zookeper one)

4. Resilient - if message from queue is taken and service which took it crashes - message will be returned to queue (in some magical way lol), and will be worked by with another service instance. 

5. Queue should implies idempotency (or Server rather?), when Queue could push out _unordered_ messages multiple times, it is all need to be in Idempotent fashion.

6. Messages are not lost even when consumer is fully offline. Messages will be stored in Queue until consumer comes back. This could be useful if consumer had a but, and was taken offline for fix, so new and fixed consumer now could handle all the messages.

7. Elastic - Orchestrator could use queue length to determine whether we need to scale  number of instances up or down.

## Fault tolerant message processing

1. Service takes message from a queue.   
1.1 Message becomes invisible in the queue, but not deleted.  
1.2 Message becomes visible again in X seconds (amount of time expected to process the message)
1.3 MessageDequeue counter is incremented, counter shows how many times message was dequeued(taken from the queue for processing)

2. Service takes message again if it is visible, and again increments counter. 
2.1 If counter is greater than threshold, it means the message is poisonous and we need to log it and delete it w/o processing, because it could crash the service(the reason why it has dequeue number over threshold)
2.2 If message is processed fine the Service asks the Broker to delete the message.

This means that messages could be processed out of order(when retried), and could be even processed in parallel (if X seconds are not enough for processing at some point).

Which means that messages processing should be:  

- independent (one message processed should not affect others)  
- idempotent (2+ message processings should have no ill effect)

## Messages features

1. Multiple subscribers could receive one same message. So we need to send 1 message and 2+ subscribers could receive it.

2. Messages could have TTL and when TTL passes message got deleted from the queue. This could save costs in cloud when we are billed for storing messages, and unprocessed messages could pile up (e.g. noone can process the message at the moment for some period of time)

3. Invisibility timeout of message (see previous secion for X value).  
But it should be taken carefully because in case of short X message could be processed second time _in parallel_  
And in case of too long X it could be invisible long time after unsuccessful processing being completed long ago
Also Service could increase or decrease the X which is stored in Broker.

4. Update message in queue.  
In case message processing is really long, and one Service processed it partially, it could go to Broker and update the message, so in case of crash second Service could not do that part of work which was done by the Service number 1. To do not waste the resources on double work. _Which still should be idempotent_

### At-most-once pattern

There is cases when message should be processed 1 or 0 times, it applies to data which is actual now and does not matter as time passes.

This could be accomplished by using TTL of the message, so it will be deleted by the Broker after the TTL expires, and TTL is the amount of time the data in the message _matters_  
Additionally to add 1 ir 0 times processing, time of message invisibility should be more than TTL, so when service takes message into processing, it will stay invisible long enough for Broker to delete it



